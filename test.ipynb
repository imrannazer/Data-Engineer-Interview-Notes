{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78589e4e-c167-436a-9a52-7047f4fde11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b31b426-ea8d-4057-9a7f-95c47669d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[2]').appName('Local file').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9e8761-805d-4ff4-96e7-d59eafbe08fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://CODEDOC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Local file</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23dad9d1de0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb350c96-26f7-4744-b9a9-bba328a71999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5442c79e-d94a-49c0-8048-300c525e4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the input file and output file\n",
    "input_file_path = 'path/to/your/input/file.txt'\n",
    "output_file_path = 'path/to/your/output/output.txt'\n",
    "\n",
    "# Read data from the input file line by line and remove blank lines\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    lines = [line.strip() for line in input_file if line.strip()]\n",
    "\n",
    "# Write non-blank lines to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.write('\\n'.join(lines))\n",
    "\n",
    "print(\"Non-blank lines successfully written to '{}'.\".format(output_file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296684b-54c7-4285-b1e1-1fa8a718d47f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da664fb3-3e51-4553-8598-e888a53bfb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only\n",
      "one\n",
      "\n",
      "line\n"
     ]
    }
   ],
   "source": [
    "# Read data from the input file line by line\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        if len(line) != str(0):\n",
    "            print(f\"{line.strip()}\")  # Print or process each line as required\n",
    "            \n",
    "        else:\n",
    "            print(\"blank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "557eecf9-0e0b-41b2-a0f8-7309d533966b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**only**\n",
      "****\n",
      "**A.** \n",
      "**one**\n",
      "****\n",
      "**A.** \n",
      "****\n",
      "****\n",
      "**A.** \n",
      "**line**\n",
      "****\n",
      "**A.** \n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the input file\n",
    "input_file_path = 'data.txt'\n",
    "\n",
    "# Read data from the input file line by line\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        if len(line) != str(0):\n",
    "            print(f\"**{line.strip()}**\")  # Print or process each line as required\n",
    "            print(\"****\")\n",
    "            print(\"**A.** \")\n",
    "            \n",
    "        else:\n",
    "            print(\"blank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea03c2-29f6-4eb4-8d26-05c5a20612e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e20b7a-4340-4d90-a03f-3f4382e318a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      "\n",
      "\n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "\n",
      "\n",
      "l\n",
      "i\n",
      "n\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'data.txt'\n",
    "# output_file_path = 'path/to/your/output/output.txt'\n",
    "\n",
    "with open(input_file_path, 'r') as input_file:\n",
    "    data = input_file.read()\n",
    "    for i in data:\n",
    "        print(i)\n",
    "    # print(len(data))\n",
    "\n",
    "# with open(output_file_path, 'w') as output_file:\n",
    "#     output_file.write(data)\n",
    "\n",
    "# print(\"Data successfully read from '{}' and written to '{}'.\".format(input_file_path, output_file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60a7d9-7235-43dd-98ac-b72410a7152c",
   "metadata": {},
   "source": [
    "<div dir=\"ltr\" class=\"reader-article-content reader-article-content--content-blocks\">\r\n",
    "          \r\n",
    "        \r\n",
    "      \r\n",
    "    <p id=\"ember36\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        When dealing with big data, joining tables or data frames is one of the most common and crucial operations. In this article, we will delve into how Spark handles joins internally and the techniques employed by the Spark engine to make this process efficient. Understanding these internals will prove invaluable in mastering complex join operations.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember37\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        In Spark, you can expect to encounter five primary types of joins:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember38\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        <ul><li>Broadcast Hash Join</li><li>Shuffle Hash Join</li><li>Sort Merge Join</li><li>Cartesian Join</li><li>Broadcast Nested Loop Join</li></ul>\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember39\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        We'll explore each of these join types one by one, for a comprehensive understanding.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember40\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Note: Throughout this article, we'll be using Databricks sample tables to explore these joins. If you have a free/trial subscription with Databricks, you can use the same code to understand these joins.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <h3 id=\"ember41\" class=\"ember-view\">\r\n",
    "        Broadcast Hash Join\r\n",
    "\r\n",
    "<!---->    </h3>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember42\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        The Broadcast Hash Join is the speedster of Spark joins. As the name suggests, it occurs when one of the data frames or tables is broadcast to all the executor nodes. To make this happen, one side of the data must be small enough to fit entirely in memory. Initially, this table is stored on the Driver node and then broadcast to all the executor nodes.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember43\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        By default, Spark considers a table for broadcasting if it's under 10 MB in size. You can modify this configuration using the following command:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", [your threshold in Bytes])</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember44\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Spark automatically selects the Broadcast Hash Join when it encounters a smaller data side. To disable this automatic application, use the following configuration:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember45\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Note: It is highly recommended not to disable this configuration as it is the most efficient method for joining small and large tables.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember46\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Tips to make this join happen :\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember47\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        <ol><li>It should be Equi join with one side being smaller</li><li>Hint as broadcast join for one of the side.</li></ol>\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre># Way to mention hint for Broadcast Join\r\n",
    "\r\n",
    "largeDF.join(smallDF.hint(\"broadcast\"),largeDF.joinkey == smallDF.joinkey,\"inner\")</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember48\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Sometimes, Spark may fail to detect the smaller table. In such cases, you can use hints to trigger the Broadcast Hash Join. But broadcasting a large data will lead to OOM issue. The Join side with the hint will be broadcasted irrespective of autoBroadcastJoinThreshold, if a broadcast hint is specified on either side of the join. \r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember49\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Example to understand the broadcast join:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>orders = spark.read.table(\"samples.tpch.orders\") # Small Table\r\n",
    "\r\n",
    "lineitem = spark.read.table(\"samples.tpch.lineitem\") # Big Table size : 983MiB \r\n",
    "\r\n",
    "print(\"orders\",orders.count())\r\n",
    "print(\"lineitem\",lineitem.count())\r\n",
    "\r\n",
    "print(\"orders\",orders.rdd.getNumPartitions()) \r\n",
    "print(\"lineitem\",lineitem.rdd.getNumPartitions()) \r\n",
    "\r\n",
    "orders.hint(\"broadcast\").join(lineitem,orders.o_orderkey == lineitem.l_orderkey,\"inner\").explain()</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>== Physical Plan ==\r\n",
    "AdaptiveSparkPlan isFinalPlan=false\r\n",
    "+- BroadcastHashJoin [o_orderkey#1825L], [l_orderkey#1859L], Inner, BuildLeft, false\r\n",
    "   :- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=2561] ---&gt; All 3 intial partitions are meregd to single partition\r\n",
    "   :  +- Filter isnotnull(o_orderkey#1825L)\r\n",
    "   :     +- FileScan parquet samples.tpch.orders\r\n",
    "   +- Filter isnotnull(l_orderkey#1859L)  ---&gt; Containing 8 partitions intially, no shuflle happend\r\n",
    "      +- FileScan parquet samples.tpch.lineitem</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!----><!---->\r\n",
    "        \r\n",
    "    <div class=\"reader-image-block reader-image-block--resize\">\r\n",
    "      <figure class=\"reader-image-block__figure\">\r\n",
    "          \r\n",
    "    <div class=\"ivm-image-view-model   \">\r\n",
    "        \r\n",
    "    <div class=\"ivm-view-attr__img-wrapper display-flex\">\r\n",
    "<!---->          <img src=\"https://media.licdn.com/dms/image/D5612AQFzom9LDgaiQA/article-inline_image-shrink_1500_2232/0/1697724490460?e=1707350400&amp;v=beta&amp;t=UAIfMlcckWpaWVme_-Nr_KbgZ-38mTdQA4Qo1M5T_nI\" loading=\"lazy\" alt=\"\" id=\"ember50\" class=\"ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view\">\r\n",
    "    </div>\r\n",
    "  \r\n",
    "          </div>\r\n",
    "  \r\n",
    "\r\n",
    "          <figcaption class=\"display-block mt2 full-width text-body-small-open t-sans text-align-center t-black--light\">\r\n",
    "            <!---->Broadcast join stage<!---->\r\n",
    "          </figcaption>\r\n",
    "      </figure>\r\n",
    "    </div>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <h3 id=\"ember51\" class=\"ember-view\">\r\n",
    "        Shuffle Hash Joins\r\n",
    "\r\n",
    "<!---->    </h3>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember52\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        When both tables are large, getting the broadcast join may lead to out-of-memory issues. In that case, Shuffle Hash Join can be a better option. These are quite expensive compared to broadcast joins as they involve both shuffling of data and hashing.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember53\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        It happens in two phases:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember54\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        <ol><li>Repartitioning on both tables will happen based on the joining keys using hash partitioning, providing 200 partitions (as per the default shuffle partitions config; default value is 200).</li><li>A hash table is created for the lookup activity, and the join happens based on that.</li></ol>\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember55\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Currently, most of the Spark systems have made Sort Merge Join their default choice over Shuffle Hash Join because of its consistently better performance compared to Shuffle Hash Joins. For this example, if you want to test the Shuffle Hash Join, use the following command to turn off the automatic Sort Merge Joins.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>spark.conf.set(\"spark.sql.join.preferSortMergeJoin\",\"false\")</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember56\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        If you are aware that the data is distributed uniformly over partitions without any skewness, you can consider this join over Sort Merge Join.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember57\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Tips to make this join happen:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember58\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        <ol><li>It should be an Equi-join.</li><li>Hint it as a Shuffle join for one of the sides.</li></ol>\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre># Way to mention hint for Shuffle Join\r\n",
    "\r\n",
    "DF1.join(DF2.hint(\"shuffle\"),DF1.joinkey == DF2.joinkey,\"inner\")</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember59\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Example to understand the Shuffle Hash join:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>spark.conf.set(\"spark.sql.join.preferSortMergeJoin\",\"false\")\r\n",
    "\r\n",
    "orders = spark.read.table(\"samples.tpch.orders\") # Table Size : 272.4MiB \r\n",
    "\r\n",
    "lineitem = spark.read.table(\"samples.tpch.lineitem\") # Table size : 983MiB \r\n",
    "\r\n",
    "print(\"orders\",orders.count())\r\n",
    "print(\"lineitem\",lineitem.count())\r\n",
    "\r\n",
    "print(\"orders\",orders.rdd.getNumPartitions()) \r\n",
    "print(\"lineitem\",lineitem.rdd.getNumPartitions()) \r\n",
    "\r\n",
    "orders.join(lineitem,orders.o_orderkey == lineitem.l_orderkey,\"inner\").explain()</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>== Physical Plan ==\r\n",
    "AdaptiveSparkPlan isFinalPlan=false\r\n",
    "+- ShuffledHashJoin [o_orderkey#3177L], [l_orderkey#3211L], Inner, BuildLeft\r\n",
    "   :- Exchange hashpartitioning(o_orderkey#3177L, 200), ENSURE_REQUIREMENTS, [plan_id=3624]  --&gt; Raprtitioned based on Join Key\r\n",
    "   :  +- Filter isnotnull(o_orderkey#3177L)\r\n",
    "   :     +- FileScan parquet samples.tpch.orders\r\n",
    "   +- Exchange hashpartitioning(l_orderkey#3211L, 200), ENSURE_REQUIREMENTS, [plan_id=3625] --&gt; Raprtitioned based on Join Key\r\n",
    "      +- Filter isnotnull(l_orderkey#3211L)\r\n",
    "         +- FileScan parquet samples.tpch.lineitem</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!----><!---->\r\n",
    "        \r\n",
    "    <div class=\"reader-image-block reader-image-block--full-width\">\r\n",
    "      <figure class=\"reader-image-block__figure\">\r\n",
    "          \r\n",
    "    <div class=\"ivm-image-view-model   \">\r\n",
    "        \r\n",
    "    <div class=\"ivm-view-attr__img-wrapper display-flex\">\r\n",
    "<!---->          <img src=\"https://media.licdn.com/dms/image/D5612AQGK7_pbZSa2nw/article-inline_image-shrink_1500_2232/0/1697724735801?e=1707350400&amp;v=beta&amp;t=oUqv3AqoxG0VAx91VT88N2_t8AY9C0w3QMlB3LgR0KA\" loading=\"lazy\" alt=\"\" id=\"ember60\" class=\"ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view\">\r\n",
    "    </div>\r\n",
    "  \r\n",
    "          </div>\r\n",
    "  \r\n",
    "\r\n",
    "          <figcaption class=\"display-block mt2 full-width text-body-small-open t-sans text-align-center t-black--light\">\r\n",
    "            <!---->Shuffle Hash Join stage Implementation<!---->\r\n",
    "          </figcaption>\r\n",
    "      </figure>\r\n",
    "    </div>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <h3 id=\"ember61\" class=\"ember-view\">\r\n",
    "        Shuffle Sort Merge Join\r\n",
    "\r\n",
    "<!---->    </h3>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember62\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        This is more preferable over Shuffle Hash Join, considering its consistency over different cases. These joins happen in three stages:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember63\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        <ol><li>Repartitioning on both tables will happen based on the joining keys using hash partitioning, providing 200 partitions (as per the default shuffle partitions config; default value is 200).</li><li>Sorting of data happens on each partition individually.</li><li>As a last step, merging happens based on the joining key.</li></ol>\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember64\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Tips to make this join happen:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember65\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        <ol><li>It should be an Equi-join.</li><li>Hint it as a Merge join for one of the sides.</li></ol>\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember66\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Example to understand the Shuffle Sort Merge join:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>spark.conf.set(\"spark.sql.join.preferSortMergeJoin\",\"true\")\r\n",
    "\r\n",
    "orders = spark.read.table(\"samples.tpch.orders\") # Table Size : 272.4MiB \r\n",
    "\r\n",
    "lineitem = spark.read.table(\"samples.tpch.lineitem\") # Table size : 983MiB \r\n",
    "\r\n",
    "print(\"orders\",orders.count())\r\n",
    "print(\"lineitem\",lineitem.count())\r\n",
    "\r\n",
    "print(\"orders\",orders.rdd.getNumPartitions()) \r\n",
    "print(\"lineitem\",lineitem.rdd.getNumPartitions()) \r\n",
    "\r\n",
    "orders.join(lineitem,orders.o_orderkey == lineitem.l_orderkey,\"inner\").explain()</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>== Physical Plan ==\r\n",
    "AdaptiveSparkPlan isFinalPlan=false\r\n",
    "+- SortMergeJoin [o_orderkey#2385L], [l_orderkey#2419L], Inner\r\n",
    "   :- Sort [o_orderkey#2385L ASC NULLS FIRST], false, 0\r\n",
    "   :  +- Exchange hashpartitioning(o_orderkey#2385L, 200), ENSURE_REQUIREMENTS, [plan_id=3102]\r\n",
    "   :     +- Filter isnotnull(o_orderkey#2385L)\r\n",
    "   :        +- FileScan parquet samples.tpch.orders\r\n",
    "   +- Sort [l_orderkey#2419L ASC NULLS FIRST], false, 0\r\n",
    "      +- Exchange hashpartitioning(l_orderkey#2419L, 200), ENSURE_REQUIREMENTS, [plan_id=3103]\r\n",
    "         +- Filter isnotnull(l_orderkey#2419L)\r\n",
    "            +- FileScan parquet samples.tpch.lineitem</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!----><!---->\r\n",
    "        \r\n",
    "    <div class=\"reader-image-block reader-image-block--full-width\">\r\n",
    "      <figure class=\"reader-image-block__figure\">\r\n",
    "          \r\n",
    "    <div class=\"ivm-image-view-model   \">\r\n",
    "        \r\n",
    "    <div class=\"ivm-view-attr__img-wrapper display-flex\">\r\n",
    "<!---->          <img src=\"https://media.licdn.com/dms/image/D5612AQFgLHNBJaTGsA/article-inline_image-shrink_1000_1488/0/1697724924512?e=1707350400&amp;v=beta&amp;t=crNb-gF-cOUZ-MWGuyeWGJDaEPq08G9BnQ6ASw-FX1Q\" loading=\"lazy\" alt=\"\" id=\"ember67\" class=\"ivm-view-attr__img--centered  reader-image-block__img evi-image lazy-image ember-view\">\r\n",
    "    </div>\r\n",
    "  \r\n",
    "          </div>\r\n",
    "  \r\n",
    "\r\n",
    "          <figcaption class=\"display-block mt2 full-width text-body-small-open t-sans text-align-center t-black--light\">\r\n",
    "            <!---->Sort Merge Join stage Implementation<!---->\r\n",
    "          </figcaption>\r\n",
    "      </figure>\r\n",
    "    </div>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <h3 id=\"ember68\" class=\"ember-view\">\r\n",
    "        Cartesian Join\r\n",
    "\r\n",
    "<!---->    </h3>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember69\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        A Cartesian Join results in each record from one table being joined with every row in the other table. This is a costly join and can lead to out-of-memory (OOM) issues when there is insufficient space to execute the join.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember70\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Tips to make this join happen:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember71\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        <ol><li>It should be a Non-Equi join.</li><li>Joining condition is not necessary when you specify a cross join.</li></ol>\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember72\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Example to understand the Cartesian Join:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>orders = spark.read.table(\"samples.tpch.orders\")\r\n",
    "\r\n",
    "\r\n",
    "lineitem = spark.read.table(\"samples.tpch.lineitem\") \r\n",
    "\r\n",
    "join_cond = (orders.o_totalprice &lt; lineitem.l_extendedprice)\r\n",
    "\r\n",
    "orders.join(lineitem,join_cond,\"inner\").explain()</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>== Physical Plan ==\r\n",
    "CartesianProduct Inner, (o_totalprice#507 &lt; l_extendedprice#543)\r\n",
    ":- *(1) Filter isnotnull(o_totalprice#507)\r\n",
    ":  +- *(1) ColumnarToRow\r\n",
    ":     +- FileScan parquet samples.tpch.orders\r\n",
    "+- *(2) Filter isnotnull(l_extendedprice#543)\r\n",
    "   +- *(2) ColumnarToRow\r\n",
    "      +- FileScan parquet samples.tpch.lineitem</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <h3 id=\"ember73\" class=\"ember-view\">\r\n",
    "        Broadcast Nested Loop Join\r\n",
    "\r\n",
    "<!---->    </h3>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember74\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        In a Broadcast Nested Loop Join, we typically have one large table, often a fact table, and a smaller dimension table. Each dimension in the smaller table relates to all the fact records. This dimension data is broadcasted to all the executors handling portions of the fact data.\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember75\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Tips to make this join happen:\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember76\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        <ol><li>It should be a Non-Equi join.</li><li>Joining condition is not necessary when you specify a cross join.</li><li>The small table can be broadcastable.</li></ol>\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>orders = spark.read.table(\"samples.tpch.orders\").limit(750)\r\n",
    "\r\n",
    "lineitem = spark.read.table(\"samples.tpch.lineitem\") \r\n",
    "\r\n",
    "join_cond = (orders.o_totalprice &lt; lineitem.l_extendedprice)\r\n",
    "\r\n",
    "orders.join(lineitem,join_cond,\"full\").explain()</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "        <pre>== Physical Plan ==\r\n",
    "AdaptiveSparkPlan isFinalPlan=false\r\n",
    "+- BroadcastNestedLoopJoin BuildLeft, FullOuter, (o_totalprice#1745 &lt; l_extendedprice#1781)\r\n",
    "   :- BroadcastExchange IdentityBroadcastMode, [plan_id=1050]\r\n",
    "   :  +- GlobalLimit 750, 0\r\n",
    "   :     +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1047]\r\n",
    "   :        +- LocalLimit 750\r\n",
    "   :           +- FileScan parquet samples.tpch.orders\r\n",
    "   +- FileScan parquet samples.tpch.lineitem</pre>\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->        \r\n",
    "      \r\n",
    "    <p id=\"ember77\" class=\"ember-view reader-content-blocks__paragraph\">\r\n",
    "        Understanding these join types is very vital in understanding Spark workloads and making informed decisions when it comes to choosing the right join strategy if spark approach is not optimal. Being aware of the internals and knowing when to change the join type can significantly impact the performance and efficiency of your Spark applications. ,\r\n",
    "\r\n",
    "<!---->    </p>\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "<!---->\r\n",
    "<!---->\r\n",
    "<!---->  \r\n",
    "      </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1ace3-c5c1-41a7-b923-3981c7557ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a284b-8cb4-43ef-8f7d-84b2050b118c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
