{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ddadd6-e4c2-4988-84f9-7ece1083415b",
   "metadata": {},
   "source": [
    "**Q. What is the Apache Spark**\n",
    "****\n",
    "**A.** Apache spark is a **unified** computing engine and set of **libraries** for parallel data processing on a computer cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5580d2-869d-49fa-b4d7-40d873acf8bd",
   "metadata": {},
   "source": [
    "**Q. Why Apache Spark, what problem does it solve**\n",
    "****\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed6abb-ccb0-4292-b66d-5f6a79c665d3",
   "metadata": {},
   "source": [
    "**Q. What is unified**\n",
    "****\n",
    "**A.** Spark is designed to support wide range of tasks over the same computing engine example data scientists, data analytics, and data engineers all can use the same platform for their analysis, modeling, and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700125f-4a83-4a79-818a-bf40d22b8b88",
   "metadata": {},
   "source": [
    "**Q. What is computing engine**\n",
    "****\n",
    "**A.** Apache Spark is a limited to a computing engine(RAM), it does not store the data, Spark can connect with different data sources like Hdfs, Zdbc, Odbc, Azure, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af7966-de05-4af9-9af2-bbdd7d700e7b",
   "metadata": {},
   "source": [
    "**Q. What are libraries**\n",
    "****\n",
    "**A.** It is a set of code that can be used in project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266f249-18bb-4cf5-b765-555b564d979e",
   "metadata": {},
   "source": [
    "**Q. What is the computer cluster**\n",
    "****\n",
    "**A.** computer cluster is a group of nodes/machine/CPU, where Master slave architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bd357-edd0-4314-ad47-a1a8da9c4ded",
   "metadata": {},
   "source": [
    "**Q. What is parallel data processing?**\n",
    "****\n",
    "**A.** Parallel data processing is a technique that divides a complex problem into smaller parts, each handled by an individual processor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20337dc4-41e5-414c-96a3-74bebd8d752e",
   "metadata": {},
   "source": [
    "**Q. Why we need Apache Spark**\n",
    "****\n",
    "**A.** Apache spark processes data in-memory(RAM), Apache Spark schedules jobs in stages and tasks, uses catalyst optimizer to enhance execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d5f90-d1d9-45e5-865e-6f96e513c10f",
   "metadata": {},
   "source": [
    "**Q. Hadoop vs Apache Spark**\n",
    "****\n",
    "**A.**\n",
    "<table>\r\n",
    "  <thead>\r\n",
    "    <tr>\r\n",
    "      <th>Feature</th>\r\n",
    "      <th>Adaptive Spark</th>\r\n",
    "      <th>MapReduce</th>\r\n",
    "    </tr>\r\n",
    "  </thead>\r\n",
    "  <tbody>\r\n",
    "    <tr>\r\n",
    "      <td>Processing Paradigm</td>\r\n",
    "      <td>In-memory data processing</td>\r\n",
    "      <td>Batch processing using disk storage</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>Data Source and Formats</td>\r\n",
    "      <td>Support various data sources</td>\r\n",
    "      <td>Handles structured data using HDFS</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>Language Support</td>\r\n",
    "      <td>JAVA, Python, R, Scala</td>\r\n",
    "      <td>Primarily JAVA</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>Machine Learning</td>\r\n",
    "      <td>Provides MLLib library for machine learning</td>\r\n",
    "      <td>Not natively designed for machine learning</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>Ease of Use</td>\r\n",
    "      <td>Provides low-level (RDD), and High-level (Dataframe) APIs</td>\r\n",
    "      <td>Requires low-level programming in JAVA</td>\r\n",
    "    </tr>\r\n",
    "  </tbody>\r\n",
    "</table>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534447d1-6286-4e13-ab5c-7b69f544dca4",
   "metadata": {},
   "source": [
    "**Q. What is the Apache Spark ecosystem?**\n",
    "****\n",
    "**A.**              \n",
    "<div>\n",
    "    <div>\n",
    "        <img src=\"https://i.ibb.co/RpGZ4BC/009.png\" alt=\"009\" border=\"0\" width=\"500\" height=\"300\" align=\"right\">\n",
    "    \n",
    "   </div>\n",
    "        <div>\n",
    "       <strong>Spark Core:</strong> It is the best engine for large-scale parallel and distributed data processing.<br>\r\n",
    "<strong>Cluster Manager:</strong> Cluster manager is used to acquire resources for executing jobs.<br>\r\n",
    "<strong>RDD:</strong> Resilient Distributed Dataset (RDD) is a low-level API.\r\n",
    "\n",
    "   </div>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5287d899-624d-4556-ba4e-4e9b38feeec8",
   "metadata": {},
   "source": [
    "**Q. what is Apache Spark architecture?**\n",
    "****\n",
    "**A.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d7034-d04d-4d19-874c-2513aad8e69a",
   "metadata": {},
   "source": [
    "**Q. What is the transformation and how many types of transformation do we have?**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Transformation</th>\n",
    "      <th>Type of Transformation</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Transformation is a function applied to an RDD/DataFrame that creates a new RDD/DataFrame, defining a sequence of data processing operators without executing them until an action is called, there are two types of transformation <strong>narrow dependency</strong> and <strong>wide dependency</strong>. Narrow transformation is cheap and wide transformation is expensive. </br><strong> * Narrow Transformation </strong>one to one and </br><strong> * Wide Transformation </strong>one to N</td>\n",
    "      <td><img src=\"https://i.ibb.co/ynQ57mh/010.png\" alt=\"010\" border=\"0\"></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eee420-d6d0-4267-8a00-e9e40d8eb572",
   "metadata": {},
   "source": [
    "**Q. What is the narrow dependency transformation**\n",
    "****\n",
    "**A.** Narrow dependence transformation that does not require data movement between partitions, e.g. filter(), select(), Union(), map(), etc.  Narrow transformation is cheap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fef15f-ec64-4e4e-abda-3dd047fabde0",
   "metadata": {},
   "source": [
    "**Q. What is the wide dependency transformation**\n",
    "****\n",
    "**A.** Wide dependence transformation that does require data movement between partitions, e.g. groupBy(), join(), etc. Wide transformation is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0353c0d-bd62-4bc0-92e4-627cb64c75d3",
   "metadata": {},
   "source": [
    "**Q. What happens when we use groupBy or join transformation**\n",
    "****\n",
    "**A.** Both are groupBy and join are wide transformations for data will be shuffled between partitions, there for it is wide transformation and they are expensive transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81842a95-5de5-4eb0-b2a2-6edb926b1a8f",
   "metadata": {},
   "source": [
    "**Q. When jobs are created in Apache Spark**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "<center> <b>--------------------------------------------List of Narrow Transformation--------------------------------------------</b> </center>    \n",
    "\n",
    "| Narrow Transformation | Description                                           |\n",
    "|------------------------|-------------------------------------------------------|\n",
    "| `map`                  | Applies a function to each element of the RDD or DataFrame. |\n",
    "| `filter`               | Selects elements based on a given condition.          |\n",
    "| `flatMap`              | Similar to `map`, but each input item can produce zero or more output items. |\n",
    "| `union`                | Combines two RDDs or DataFrames without shuffling.    |\n",
    "| `distinct`             | Returns unique elements in the RDD or DataFrame.      |\n",
    "| `intersection`         | Returns common elements between two RDDs or DataFrames.|\n",
    "| `subtract`             | Returns elements in one RDD or DataFrame not present in another. |\n",
    "| `cartesian`            | Computes the Cartesian product of two RDDs.           |\n",
    "| `sortBy`               | Sorts elements based on a specified key.              |\n",
    "\n",
    "\n",
    "<center> <b>--------------------------------------------List of Wide Transformation--------------------------------------------</b> </center>\n",
    "\n",
    "| Wide Transformation    | Description                                           |\n",
    "|------------------------|-------------------------------------------------------|\n",
    "| `groupByKey`           | Groups the data by key, requiring a shuffle.          |\n",
    "| `reduceByKey`          | Aggregates values based on key, requiring a shuffle.  |\n",
    "| `aggregateByKey`       | Aggregates values with more control than `reduceByKey`, requiring a shuffle. |\n",
    "| `sortByKey`            | Sorts the RDD or DataFrame based on key, requiring a shuffle. |\n",
    "| `join`                 | Joins two RDDs or DataFrames based on a common key, requiring a shuffle. |\n",
    "| `cogroup`              | Groups the data from multiple RDDs or DataFrames based on a common key, requiring a shuffle. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5e69d-2edb-4350-bc0b-5d374ebdc2dc",
   "metadata": {},
   "source": [
    "**Q. What is the DAG in Apache Spark**\n",
    "****\n",
    "**A.**<div>\n",
    "    <img src=\"https://i.ibb.co/YtPyg6r/011.jpg\" width=\"200\" height=\"300\" align=\"right\" alt=\"011\" border=\"0\">\n",
    "    <p>\n",
    "        DAG is a scheduling layer of Apache Spark that implements stages-oriented scheduling and it is a transformation of a logical execution plan to a physical execution plan using stages.\n",
    "    </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774d3bd-76fd-49d2-a711-f0a40de22795",
   "metadata": {},
   "source": [
    "**Q. What is the lazy evolution?**\n",
    "****\n",
    "**A.** Lazy evolution is **planning everything but doing nothing**. In Apache Spark when we submit any type of transformation, Spark creates  only plan no more, therefore it is called <u>_lazy evolution_</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32bab2f-6f50-4f17-8714-d8d0c31b83db",
   "metadata": {},
   "source": [
    "**Q. What are the job, task, and stages in Apache Spark?**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "**Jobs**: Job is a sequence of stages triggered by an action such as count(), collect(), show() read(), etc \\         \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Every action inside Spark application trigger spark-job action **(action = SparkJob)**\n",
    "\n",
    "\n",
    "**Stages**: Stages in Apache Spark represent groups of tasks, that can be executed together(task and stage) to compute the same transformation on multiple machines, stages Run sequentially of parallel data processing to the executor. \n",
    "\n",
    "**Task**: A task is a unit of work that is sent to the executor, One task per partition. The same task is done over different partitions of the RDD parallely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a70df6-a04f-4992-986a-c374ebf456dc",
   "metadata": {},
   "source": [
    "**Important points**\n",
    "- Driver and executor are exclusive no other can use to Spark app\n",
    "- Multiple executors of the same application can run inside our worker machine\n",
    "- Multiple executors of different applications can run inside one worker machine\n",
    "- One executor handles multiple parallel tasks on the same stages and the same application\n",
    "- Tasks of the same stages can run in parallel stages are executed sequentially\n",
    "- A Spark application consists of multiple jobs and each job can be delivered into multiple stages\n",
    "- The result of each job is written to the driver by the executor\n",
    "- The new stage starts after a wild transformation\n",
    "- Stages run sequentially\n",
    "- Each stage has some tasks\n",
    "- The Task the smallest unit in the execution hierarchy\n",
    "- One task can not executed more than one executor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1e29601-8a27-4ac8-a478-0057410e1eef",
   "metadata": {},
   "source": [
    "**Q. What is the catalyst optimizer or Spark SQL engine?**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<di v>\n",
    "    <img src=\"https://i.ibb.co/znSZ5L9/013.jpg\" alt=\"013\" border=\"0\" width=\"500\" height=\"600\" align=\"left\"><br />\n",
    "</div>\n",
    "\n",
    "<div >\n",
    "    <p><strong>Catalyst</strong> optimizer is a query Optimisation and execution framework, which decides how the code should be executed and lays out a plan</p>\r\n",
    "\r\n",
    "<p>There are four paces</p>\r\n",
    " <ol>\r\n",
    "  <li>Code analysis (Un-resolve to resolve code)</li>\r\n",
    "  <li>Logical plan optimization</li>\r\n",
    "  <li>Physical plan optimization</li>\r\n",
    "  <li>Code generation</li></div>\r\n",
    "</ol>\r\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1190eaf-2389-4ec3-9730-c8f0962405f7",
   "metadata": {},
   "source": [
    "**Q. Why do we get AnalysisException error**\n",
    "****\n",
    "**A.** An AnalysisException in Apache Spark typically occurs during the analysis phase of query execution. This error is often related to issues such as:\n",
    "\n",
    "- Undefined or Unresolved Columns\n",
    "  - df.select(\"nonexistent_column\")\n",
    "- Ambiguous(same column name in two table in case of join) Column Reference\n",
    "  - df.select(\"ambiguous_column\")\n",
    "- Unsupported Operations\n",
    "  - df.write.saveAsTable(\"unsupported_operation\")\n",
    "- Issues with Table or View Resolution\n",
    "  - spark.sql(\"SELECT * FROM non_existent_table\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b5071-e88b-4e2d-9567-31b49a20f087",
   "metadata": {},
   "source": [
    "**Q. What is the catalog?**\n",
    "****\n",
    "**A.** The catalog is a part of the catalyst optimizer and metadata repository that organizes and manages information about tables, databases, and functions, facilitating structured data management and access in Spark SQL.                     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It is used to verify user code during the code conversion from un-resolve to resolve plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93be9cb-05de-4a82-a7e5-f968100a2b34",
   "metadata": {},
   "source": [
    "**Q. What is the physical planning or Spark plan**\n",
    "****\n",
    "**A.** It is a detailed strategy outlining how to execute a given DataFrame operation, specifying the series of stages and tasks that will be carried out on the distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c448e5-431d-4ebe-9a30-cb2a4659a275",
   "metadata": {},
   "source": [
    "**Q. Is Spark SQL engine a coompiler or not**\n",
    "****\n",
    "**A.** **Yes**. It is a compailer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffddd99-65a5-46e2-9d32-9918e8a38212",
   "metadata": {},
   "source": [
    "**Q. How many phases are involved in Spark SQL engine to convert a code into java bite code**\n",
    "****\n",
    "**A.** **Four Phases**\n",
    "<ol >\n",
    "  <li>Code analysis (Un-resolve to resolve code)</li>\n",
    "  <li>Logical plan optimization</li>\n",
    "  <li>Physical plan optimization</li>\n",
    "  <li>Code generation</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62732a1-6d85-45f9-aeab-9a46e6c546d8",
   "metadata": {},
   "source": [
    "**Q. What is the RDD**\n",
    "****\n",
    "**A.** RDD is standard for **R**esilient **D**istributed **D**ata-set.It is one of the fundamental schema-less data structure. That can handle both structured and semi-structured data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7b130-a40b-4846-b9af-8be657f7d9fb",
   "metadata": {},
   "source": [
    "**Q. When do you we need an RDD**\n",
    "****\n",
    "**A.** When user want to process data on low level then RDD can be used. RDD does not provide any optimization during the code execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df2990-4ffb-422b-a88b-cfb1deb97598",
   "metadata": {},
   "source": [
    "**Q. What is the Features of an RDD**\n",
    "****\n",
    "**A.**\n",
    "- In-memory competition\n",
    "- Lazy evolution\n",
    "- Fault-tolerant\n",
    "- Immutability\n",
    "- Partition\n",
    "- Low-level API\n",
    "- Code Optimisation by user\n",
    "- RDD uses user memory in memory management(Dataframe uses spark memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0fd6a-29e2-49ae-8b84-85fba55b6a67",
   "metadata": {},
   "source": [
    "**Q. What is the difference between dataFrame and data set**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a119c-bb8c-424e-90a5-7ed3f3cc04b0",
   "metadata": {},
   "source": [
    "**Q. Why we should not use an RDD**\n",
    "****\n",
    "**A.** RDD does not provide any Optimisation technique, therefore we don't use RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da1958-85ba-4dd6-b5ef-4e99e85a2eff",
   "metadata": {},
   "source": [
    "**Q. What is the difference between SparkSession and SparkContext**\n",
    "****\n",
    "**A.**\n",
    "<center><b>---------------------------------------SparkSession vs SparkContext---------------------------------------</b></center>\n",
    "\n",
    "| Aspect                | SparkSession                                           | SparkContext                                       |\n",
    "|-----------------------|--------------------------------------------------------|----------------------------------------------------|\n",
    "| **Purpose**           | Unified entry point for high-level Spark operations, including DataFrame and SQL functionality | Low-level interface for managing Spark jobs and RDD operations |\n",
    "| **Creation**          | Implicitly created in Spark applications               | Explicitly created using SparkConf in applications |\n",
    "| **Functionality**      | Higher-level API for Spark operations                 | Focused on low-level RDD operations and cluster configuration |\n",
    "| **Concurrency**        | Supports concurrent execution of multiple Spark jobs    | Manages the execution of a single Spark job at a time |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ab78ce1-d123-4856-9175-ad3bfadee331",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/tJ03ySk/012.png\" alt=\"012\" width=\"1200\" height=\"300\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b87d9b-2fcc-4309-b872-3cf535e33613",
   "metadata": {},
   "source": [
    "**Q. How many jobs will be created in the given question**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d8da0-74ec-4390-a834-0b55ad7c8695",
   "metadata": {},
   "source": [
    "**Q. How many stages will be created in the given question**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2534ced-fe54-45e7-b0c2-5c6c2490c9b3",
   "metadata": {},
   "source": [
    "**Q. How many tasks will be created in the given question**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8cf53-1abb-49f5-815e-fe47acdfecb4",
   "metadata": {},
   "source": [
    "**Q. How to convert Python variable into RDD**\n",
    "****\n",
    "**A.** By using parallelize() funciton                      \n",
    "`python_variable = [2,3,6,58,9,8,7,5,4]`                      \n",
    "`RDD = sc.parallelize(python_variable)`                             \n",
    "`type(RDD)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c63c6-4e2d-45ce-928d-582ac85826ea",
   "metadata": {},
   "source": [
    "**Q. How to create an RDD by using a file**\n",
    "****\n",
    "**A.** By using textFile() function                \n",
    "`fileRDD = sc.textFile(f\"file_path\")`              \n",
    "`print(fileRDD.collect())`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce23c3-cec0-4adb-9c52-d1cc9b12c7ce",
   "metadata": {},
   "source": [
    "**Q. How to convert an RDD to Dataframe**\n",
    "****\n",
    "**A.** By using toDF() funciton\n",
    "\n",
    "`df = RDD.toDF(schema = [column1, column2, ...])`                       \n",
    "`type(df)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cc2f6-10dc-466e-9d05-b06859d69959",
   "metadata": {},
   "source": [
    "**Q. How to get number of partitions in RDD**\n",
    "****\n",
    "**A.** By using getNumPartitions() function                 \n",
    "`RDD.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a9e87-5762-45a4-a982-240b3938eede",
   "metadata": {},
   "source": [
    "**Q. map( ) vs flatMap**\n",
    "****\n",
    "**A.**  \n",
    "**map()** is used to operate on the record level. It returns a new RDD by applying a function to each element of the RDD function, in map( ) must return only one item.                       \n",
    "**flatMap()** is similar to map(), it returns a new RDD by applying a function to each element of the RDD but output is flattend\n",
    "\n",
    "\n",
    "\n",
    "| **Aspect**           | `map` Transformation                              | `flatMap` Transformation                            |\n",
    "|-----------------------|----------------------------------------------------|-----------------------------------------------------|\n",
    "| **Functionality**     | Applies a function to each element of the RDD.    | Applies a function to each element and flattens the result. |\n",
    "| **Output**            | One output element for each input element.        | Can produce zero, one, or multiple output elements for each input element. |\n",
    "| **Output Structure**  | Maintains the structure of one-to-one mapping.     | Flattens the results into a single sequence.         |\n",
    "| **Example**           | `rdd.map(lambda x: (x, x * 2))`                   | `rdd.flatMap(lambda x: (x, x * 2))`                  |\n",
    "| **Input RDD**         | `[1, 2, 3, 4]`                                   | `[1, 2, 3, 4]`                                      |\n",
    "| **Output RDD**        | `[(1, 2), (2, 4), (3, 6), (4, 8)]`               | `[1, 2, 2, 4, 3, 6, 4, 8]`                           |\n",
    "|**Code**|sc.parallalize([3,4,5]).map(lambda x: [x**x]).collect()|sc.parallalize([3,4,5]).flatMap(lambda x: [x**x]).collect()|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08309bbd-a74d-4dc3-8882-7b9be4d20995",
   "metadata": {},
   "source": [
    "**Q. reduceByKey vs groupByKey**\n",
    "****\n",
    "**A.** <p><code>reduceByKey()</code> and <code>groupByKey()</code> both are wide dependency transformations, both perform transformation on key-value pair RDD</p>\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/92pkK1G/014.jpg\" width=\"500\" height=\"600\" align=\"right\">\n",
    "    \n",
    "\n",
    "<ul>\n",
    "  <li>reduceByKey is something like grouping + aggregation</br>\n",
    "      <code>rdd = sc.parallelize([(1, 2), (2, 4), (1, 6), (2, 8)])\n",
    "result = rdd.reduceByKey(lambda x, y: x + y)\n",
    "# Output: [(1, 8), (2, 12)]</code>\n",
    "  </li>\n",
    "    <li>groupByKey is just to group your dataset based on a key</br>\n",
    "    <code>rdd = sc.parallelize([(1, 2), (2, 4), (1, 6), (2, 8)])\n",
    "result = rdd.groupByKey()\n",
    "# Output: [(1, [2, 6]), (2, [4, 8])]\n",
    "</code>\n",
    "    </li>\n",
    "    <li>Too many unique keys go to <code>groupByKey()</code></li>\n",
    "    <li>Too many values (not unique) go to <code>reduceByKey()</code></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c870b-3cca-4810-b60c-904d4d97365d",
   "metadata": {},
   "source": [
    "**Q. Write word count program and explain it**\n",
    "****\n",
    "**A. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4a8de-817b-4b8b-b72d-e969c32265be",
   "metadata": {},
   "source": [
    "**Q. What is the over-parallelism and under-parallelism**\n",
    "***\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2086ce-c02f-4fe8-b27a-d442dd293e7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c79876a7-a1ac-4708-b8a9-d3a4b0f8716e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da5bd77-8bfa-46ea-9d09-4c5207ba1ff6",
   "metadata": {},
   "source": [
    "**Q. What is the repartitioning in Apache Spark**\n",
    "****\n",
    "**A.** Repartition is an operation that redistributed the data across the specified number of partitions to balance partition size and remove data skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9a2e3-73d0-45cc-b536-71bfb4c5e52b",
   "metadata": {},
   "source": [
    "**Q. What is the coalesce in Spark**\n",
    "****\n",
    "**A.** Coalesce is merging partitions and reducing number of partitions it is more efficient operation when you want to decrease the number of partitions without suffering data across the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e67dfe-c2c6-466f-a249-23ad6e150dcf",
   "metadata": {},
   "source": [
    "**Q. What is the difference between repartitioning and coalesce in Apache Spark**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "\n",
    "| **Aspect**          | `repartition` Operation                                        | `coalesce` Operation                                           |\n",
    "|----------------------|---------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Functionality**    | Increases or decreases the number of partitions in a DataFrame by reshuffling the data across the Spark cluster. | Decreases the number of partitions without a full shuffle, trying to minimize data movement. |\n",
    "| **Performance**      | More resource-intensive as it involves a full shuffle.         | More efficient when decreasing the number of partitions as it minimizes data movement. |\n",
    "| **Use Cases**        | Useful when you want to either increase or decrease the level of parallelism and distribute the data **more evenly**. | Useful when you want to decrease the number of partitions to reduce overhead or optimize performance. |\n",
    "|**Partition size**| More evenly| not confirm|\n",
    "|**Transformation**| Wide transformation|Narrow transformation|\n",
    "| **Example**          | `df.repartition(5)`                                             | `df.coalesce(3)`                                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf46e2-0a28-4223-bb3a-c81d0e1f739e",
   "metadata": {},
   "source": [
    "**Q. Which one will you choice and why (repartition or coalesce)**\n",
    "****\n",
    "**A.** \n",
    "|**`df.repartition()`**|**`df.coalesce()`**|\n",
    "|---------------------|--------------------|\n",
    "|When **more** data skewness|When **less** data skewness|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e129d80-d34c-47f3-8a60-ea1519eca9cc",
   "metadata": {},
   "source": [
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd182a3b-6f56-40cf-bb1c-93353cee3545",
   "metadata": {},
   "source": [
    "**Q. What are the join strategies in Apache Spark**\n",
    "****\n",
    "**A.** Join strategies in Apache Spark\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/mcXw72y/013.png\" width=\"700\" height=\"600\" align=\"right\">\n",
    "    \n",
    "<ol>\n",
    "  <li>Shuffle sort-merge join</li>\n",
    "  <li>Shuffle hash join</li>\n",
    "  <li>Broadcast hash join</li>\n",
    "  <li>Cartesian join</li>\n",
    "  <li>Broadcast nested loop join</li>\n",
    "</ol>\n",
    "</div>\n",
    "visit : <a>https://www.linkedin.com/pulse/spark-join-strategies-mastering-joins-apache-venkatesh-nandikolla-mk4qc/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0027e595-869b-41d4-9237-b2b84512b971",
   "metadata": {},
   "source": [
    "**Q. Why join is expensive or wide dependency transformation**\n",
    "****\n",
    "**A.** because every join perform data shuffling between partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd85b0-28f7-40c8-8e93-ce91e7941e35",
   "metadata": {},
   "source": [
    "**Q. Difference between shuffle-hash join and shuffle-short-marge join**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/YhDSJMz/017.png\" width=\"400\" height=\"45000\" align=\"right\">\n",
    "    <table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Aspect</th>\n",
    "      <th>Shuffle Sort-Merge Join</th>\n",
    "      <th>Shuffle Hash Join</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><strong>Process</strong></td>\n",
    "      <td>Sorting data before going to join</td>\n",
    "      <td>Create hash table by using smaller table, then generate hash number in the large table, and then join tables</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Resource Utilization</strong></td>\n",
    "      <td>Data processing in CPU</td>\n",
    "      <td>Data processing in-memory</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Time Complexity</strong></td>\n",
    "      <td>Sorting time complexity is O(nlogn)</td>\n",
    "      <td>Hashing time complexity is O(1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Error</strong></td>\n",
    "      <td><center>-------------------------</center></td>\n",
    "      <td>If you don't have enough memory, then you will get a memory out-of-exception error</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Spark Prefer</strong></td>\n",
    "      <td>By default, Spark prefers Shuffle Sort-Merge Join</td>\n",
    "      <td><center>-------------------------</center></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945bef9-bd49-44e0-843b-01b21a0e47ae",
   "metadata": {},
   "source": [
    "**Q. When do we need broadcast join**\n",
    "****\n",
    "**A.** Remove shuffling (table available on own executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0847c1-82c5-4133-b866-25f80ead4bb0",
   "metadata": {},
   "source": [
    "**Q. What is the accumulator in Apache Spark**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85348268-db2d-43d0-99ba-1c9bcfe55153",
   "metadata": {},
   "source": [
    "**Q. How does broadcast join works**\n",
    "****\n",
    "**A.** Driver sends data to executors, therefore it will save data shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238dea0-88a1-4f56-9b9a-468766a680b2",
   "metadata": {},
   "source": [
    "**Q. Difference between broadcast hash join and shuffle-hash join**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "\n",
    "|**broadCast Join**|**shuffle-hash join**|\n",
    "|-------------------|--------------------|\n",
    "|Join without data shuffling|Join with data shuffling|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd412ea0-49f9-413a-b798-294cfeb5e642",
   "metadata": {},
   "source": [
    "**Q. How can we change broadcast size of table**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "`spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")` Get by default small table size (10MB)                              \n",
    "`spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)` Set smaller table size in bite\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16d74f-5014-4830-a215-8d743e6b4d02",
   "metadata": {},
   "source": [
    "**Q. When broadcast table is not good or it will fail**\n",
    "****\n",
    "**A.** When broadcast table size is according to driver size, the Safe side size is 10MB, but you should configure scouting to your driver memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241dbee-ff84-4a8c-9cee-fdeac1115607",
   "metadata": {},
   "source": [
    "**Q. Why do we get driver OOM**\n",
    "****\n",
    "**A.** When the program tries to use more memory than is available.           \n",
    "- collect() : It will show OOM error, try to read whold data form all executor\n",
    "- show() : only show data single executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2ce31-e9e3-47d3-89ba-0eeb4222b0c6",
   "metadata": {},
   "source": [
    "**Q. What is the OOM(out of memory) in Apache Spark**\n",
    "****\n",
    "**A.** When the program tries to use more memory than is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad40d84-e05b-417a-a9c1-e01372af802f",
   "metadata": {},
   "source": [
    "**Q. What is the driver overhead memory**\n",
    "****\n",
    "**A.** Driver Overhead Memory is a non-JVM process and container hold this space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a42958-9c84-46a3-8d9c-d10260212aa3",
   "metadata": {},
   "source": [
    "**Q. Coomon reason to get a driver OOM**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "- collect() method is used\n",
    "- Performe broadCast join\n",
    "- More objects is used in the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd1f36-cf86-411c-b78e-c7496b0c2814",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bde26ba-d89b-4103-ada6-450a4d4f6bb0",
   "metadata": {},
   "source": [
    "**Q. How to handle OOM**\n",
    "****\n",
    "**A.**\n",
    "- Don't use collect() if not required\n",
    "- Check broadCast join table size\n",
    "- Don't use unnecessary objects.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e0246-b458-4639-a579-cc7746db24fd",
   "metadata": {},
   "source": [
    "**Q. Why do we get OOM when data can be spill to the disc**\n",
    "****\n",
    "**A.** A spark has two types of memory 1 is execution memory and second is storage memory. Execution memory is used for transformation and storage memory is used for storing cache, dataframe, broadcast variable, etc. If storage memory is full by using cache or other things and execution memory needs more space(RAM), then try to get space data from storage memory(storage memory is already full by storing chche, dataframe, rdd, ect) and data is skewed data so in this case we will get out of memory exception and data cannot be spill to the disc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d295e-883d-4391-8544-1e60fb92c4d8",
   "metadata": {},
   "source": [
    "**Q. How Spark manage storage inside executor intelligently**\n",
    "****\n",
    "**A.**  Spark has two types of memory, execution memory, and storage memory, and both have 50%. This distribution is unified (dynamically) after 1.6.0 version, so when cache, RDD, and dataframe need to store data of more than 50% space then get space from execution memory, and in another case when execution memory needs space of more than 50% it goes to storage in memory and gets space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561b382-1356-44e0-aed4-88606ffd7682",
   "metadata": {},
   "source": [
    "**Q. How task is spill in executor**\n",
    "****\n",
    "**A.** 1.6.0 + task is spill in executor dynamically(from execution memory to storage memory and storage to execution memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2524636-ea0f-4db9-b74d-14cf3a290131",
   "metadata": {},
   "source": [
    "**Q. Why do we need overhead memory**\n",
    "****\n",
    "**A.** Overhead Memory hold non-JVM process and container required spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479a29f-141a-4e5f-98fa-594dac4ed34a",
   "metadata": {},
   "source": [
    "**Q. How many type of Spark Memory Manager**\n",
    "****\n",
    "**A.** Two types of Spark Memory Manager\n",
    "1. Static Memory Manager\n",
    "2. Dynamic Memory Manager\n",
    "<img src=\"https://i.ibb.co/bgRcFwx/018.png\" width=\"600\" height=\"100\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd9abe-d3bc-4732-b405-d7e30bb20dfc",
   "metadata": {},
   "source": [
    "**Q. When do we get executor OOM**\n",
    "****\n",
    "**A.** When a worker runs out of space to perform tasks. This can be due to tasks needing too much memory, not enough memory allocated to the worker, or **unevenly distributed data**. To solve it, consider giving more memory to workers, improving code efficiency, and making sure data is spread evenly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524cb53-30f8-4215-ac99-08f52a4a4945",
   "metadata": {},
   "source": [
    "**Q. What is the Spark-submit**\n",
    "****\n",
    "**A.** hello world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ad0e1-0714-4b76-8820-a85339374bce",
   "metadata": {},
   "source": [
    "**Q. How do you run your job on Spark cluster**\n",
    "****\n",
    "**A.**           \n",
    "\n",
    "\n",
    "`/bin/ Spark-submit\\`                         \n",
    "`     -- master local [s] \\ ` YARN,                                \n",
    "`    -- deploy-mode Cluster \\`                            \n",
    "`    -- class main-class.scala \\`                             \n",
    "`    -- jars C:\\ my-sql-jar \\ my-sql-connector.jar \\`                \n",
    "`    -- conf Spark.dynamicAllocation.enabled = true \\`                 \n",
    "`    -- conf spark.dynamicAllocation.minExecutors = 1 \\`                \n",
    "`    -- conf spark.dynamicAllocation.maxExecutors = 10\\`                     \n",
    "`    -- Conf spark.sql.broadcastTimeout = 3600\\`                          \n",
    "`    -- Conf spark.sql.autoBroadcastJoinThreshold = 100000 \\`                       \n",
    "`    -- driver-memory 1g\\`                            \n",
    "`    -- executor-memory 2G\\`                 \n",
    "`    -- executor-cores 2\\`                                  \n",
    "`    -- py-files spark-submission.py, spark-log.py` use absolute path for all files                                \n",
    "`    -- files config.py, readme.ini`                          \n",
    "`    -- C:\\\\spark-project\\DE-project\\main.py testing-project` it will pass in list                                     \n",
    "\n",
    "visit : https://spark.apache.org/docs/latest/configuration.html               \n",
    "visit : https://spark.apache.org/docs/latest/submitting-applications.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0efc7-9041-4249-a993-569ac492ec8d",
   "metadata": {},
   "source": [
    "\n",
    "**Q. Where is your Spark cluster**\n",
    "****\n",
    "**A.** On YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2970b-fee7-45b2-8813-c620a8fdac9e",
   "metadata": {},
   "source": [
    "**Q. How do we provide memory configuration and why do you use this much memory**\n",
    "****\n",
    "**A.** Try to run application with required memory, it will not get out-of-memory issues. \n",
    "\n",
    "`    -- driver-memory 1g\\`                            \n",
    "`    -- executor-memory 2G\\`                 \n",
    "`    -- executor-cores 2\\` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9587fd-14de-45ac-98cd-f858ffa7f8df",
   "metadata": {},
   "source": [
    "**Q. How to update configuration like broadcast threshold timeout dynamic memory allocation**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "`    -- Conf spark.sql.broadcastTimeout = 3600\\`                          \n",
    "`    -- Conf spark.sql.autoBroadcastJoinThreshold = 100000 \\`       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2a81956-c7bd-40ba-81ff-ebee2326d997",
   "metadata": {},
   "source": [
    ".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d60ee-300c-41b5-98a3-b0a0ce0baf02",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is the deploy mode in Apache Spark submit**\n",
    "****\n",
    "**A.** There is two types of deployment mode in spark                   \n",
    "**1. Client Mode**: In client mode, the Spark driver program runs on the machine where the application is submitted, coordinating and overseeing the Spark job.                            \n",
    "\n",
    "**2. Cluster Mode**: In cluster mode, the Spark driver program is submitted to run on one of the worker nodes within the Spark cluster(YARN), operating independently after submission.\n",
    "<div>\n",
    "<img src=\"https://i.ibb.co/qmX1jKZ/022.png\" width=\"1200px\" height=\"300px\" align=\"right\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08058cd-8bd3-472b-9956-df3ff243be1f",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is Master in Spark- submit**\n",
    "****\n",
    "**A.** ` spark-submit ` is a command-line tool in Apache Spark used to submit and launch Spark applications on a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae56c2-784b-40dd-aa19-8585f6f7ea70",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is the edge node**\n",
    "****\n",
    "**A.** An edge node is a computer that acts as an end-user portal for communication with other nodes in cluster computing. Edge nodes are also sometimes called gateway nodes or edge communication nodes.\n",
    "In cluster, there are three types of nodes\n",
    "1. Master Node\n",
    "2. Worker/Data Node\n",
    "3. Edge Node\n",
    "\n",
    "- Edge nodes facilitate communications from end users to master and worker nodes.\n",
    "- Edge node isn’t used to store data or perform computation.\n",
    "- Edge node allows end users to contact worker nodes when necessary\n",
    "- Edge node authenticates user by using karbores\n",
    "- Edge node authorized user, have read-write access or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2a13d-cd93-4832-b43e-522cef6c86cd",
   "metadata": {},
   "source": [
    "**Q. Why do we need client and cluster modes**\n",
    "****\n",
    "**A.** Watch all execution processes in case of client deployment mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf56f7-c489-4eca-af35-aa00a814c6eb",
   "metadata": {},
   "source": [
    "**Q. What will happen if I close my adge node**\n",
    "****\n",
    "**A.** When we shut down edge node in client mode all processes will be stopped(executors kill in the absence of driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d739393-1b99-4430-9047-d87295f3927e",
   "metadata": {},
   "source": [
    "**Q. what is the Client mode vs Cluster mode deployment**\n",
    "***\n",
    "\n",
    "**A.** \n",
    "\n",
    "\n",
    "| Aspect                              | Client Mode Deployment                                         | Cluster Mode Deployment                                       |\r\n",
    "|-------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|\r\n",
    "| **Logs Location**                   | Logs are generated on the client machine, making it easy to debug. | Logs are generated in STDOUT or STDERR files, suitable for production workloads. |\r\n",
    "| **Network Latency**                 | Network latency is higher due to communication with the client.  | Network latency is less, as the driver communicates directly with the cluster. |\r\n",
    "| **Driver Memory Issues**            | Driver out of memory can occur.                                   | Driver can go into out of memory (OOM), but chances are less. Even if the edge server is closed, the process still runs on the cluster. |\r\n",
    "| **Driver Persistence**              | The driver goes away once the edge node server is disconnected or closed. | The process continues to run on the cluster even if the edge server is closed. |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b9bcc-6ab5-4c32-abdb-47c26ed34cc3",
   "metadata": {},
   "source": [
    "**Q. What is the adaptive query execution (AQE) in Apache Spark**\n",
    "***                  \n",
    "**A.** Adaptive Query Execution (AQE) provides facilities to control application processes on runtime, update and enhance performance dynamically it is enrolled 3.0 and above\n",
    "</br>**<center>---------------------------------------or---------------------------------------</center>**</br>\n",
    "Adaptive Query Execution (AQE) in Apache Spark dynamically adjusts and optimizes the execution plan of Spark SQL queries based on runtime statistics, enhancing performance by adapting to data characteristics and cluster conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7162c-9268-4b1c-b34f-a542bc0d248a",
   "metadata": {},
   "source": [
    "**Q. What is the features of adaptive query execution?**\n",
    "***\n",
    "**A.** Features of AQE \n",
    "  <div>\n",
    "      <div>\n",
    "          <img src=\"https://i.ibb.co/yPcNwRD/023.png\" width=\"900\" height=\"600\" align=\"right\">\n",
    "      </div>\n",
    "      <div>\n",
    "          <ul>\n",
    "              <li>Dynamically coalescing/choosing the number of shuffle partitions</li>\n",
    "              <li>Dynamically switching join strategies</li>\n",
    "              <li>Dynamically optimizing</li>\n",
    "          </ul>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a2f29-8d28-4251-b382-432193462a80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2729ad3e-7e09-4149-8c16-00ea05593b5c",
   "metadata": {},
   "source": [
    "**Q. What is the Over Parallelism and Under Parallelism**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "1. **Over Parallelism** :\n",
    "Small amount of data divided into large number of partitions\n",
    "\n",
    "2. **Under Parallelism** :\n",
    "Large amount of data divided into small number of partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360412a2-2ba1-4cd6-ab69-8c0b325ab7dc",
   "metadata": {},
   "source": [
    "**Q. Why do we need adaptive query execution**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<table>\r\n",
    "  <thead>\r\n",
    "    <tr>\r\n",
    "      <th><strong>Challenges</strong></th>\r\n",
    "      <th><strong>AQE Solutions</strong></th>\r\n",
    "      <th><strong>Example</strong></th>\r\n",
    "    </tr>\r\n",
    "  </thead>\r\n",
    "  <tbody>\r\n",
    "    <tr>\r\n",
    "      <td><strong>Skewed Data Distributions</strong></td>\r\n",
    "      <td>1. Dynamically identifies and handles skewed data during runtime.</td>\r\n",
    "      <td>1. Suppose a join operation is affected by skewed data in a key column. AQE might dynamically adjust the execution plan to use a broadcast or dynamic redistribution strategy to mitigate the skewness.</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td></td>\r\n",
    "      <td>2. Adjusts join strategies to handle skewed joins more effectively.</td>\r\n",
    "      <td></td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td></td>\r\n",
    "      <td>3. Facilitates dynamic redistribution of data to mitigate skewness.</td>\r\n",
    "      <td></td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td><strong>Static Partitioning Decisions</strong></td>\r\n",
    "      <td>4. Allows for dynamic repartitioning decisions based on runtime statistics.</td>\r\n",
    "      <td>4. If the size of data for a specific partition grows unexpectedly, AQE may dynamically repartition the data during execution, optimizing the partitioning strategy based on the actual data distribution.</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td></td>\r\n",
    "      <td>5. Adapts partitioning strategy during query execution for better performance.</td>\r\n",
    "      <td></td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td><strong>Changing Data Characteristics</strong></td>\r\n",
    "      <td>6. Monitors runtime statistics and adjusts the execution plan in response to changes.</td>\r\n",
    "      <td>6. If the characteristics of the data change during query execution (e.g., due to a filter reducing data size), AQE can dynamically adapt the execution plan to optimize for the new data state.</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td></td>\r\n",
    "      <td>7. Ensures adaptability to evolving conditions, optimizing for current data state.</td>\r\n",
    "      <td></td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td><strong>Optimal Join Strategies</strong></td>\r\n",
    "      <td>8. Dynamically selects the most efficient join strategy based on runtime information.</td>\r\n",
    "      <td>8. In a join operation, AQE may analyze the size of the tables being joined and dynamically choose between a broadcast join or a shuffle join to optimize performance based on the actual data distribution.</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td></td>\r\n",
    "      <td>9. Chooses between broadcast and shuffle joins depending on data size and distribution.</td>\r\n",
    "      <td></td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td><strong>Resource Utilization</strong></td>\r\n",
    "      <td>10. Adjusts resource allocation dynamically to optimize cluster resource usage.</td>\r\n",
    "      <td>10. If the cluster load changes during query execution, AQE may dynamically allocate or deallocate resources to ensure efficient utilization based on the changing demands of the Spark job.</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td></td>\r\n",
    "      <td>11. Ensures efficient utilization of available resources in a dynamic environment.</td>\r\n",
    "      <td></td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td><strong>Query Performance Improvement</strong></td>\r\n",
    "      <td>12. Results in more efficient query execution, leading to improved overall performance.</td>\r\n",
    "      <td>12. AQE might dynamically adjust the execution plan to optimize for filter conditions, aggregations, or other operations, resulting in reduced query processing times and improved Spark SQL query efficiency.</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td></td>\r\n",
    "      <td>13. Reduces query processing times and enhances Spark SQL query efficiency.</td>\r\n",
    "      <td></td>\r\n",
    "    </tr>\r\n",
    "  </tbody>\r\n",
    "</table>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b61d9d-2500-453b-82db-5673ac84ea4e",
   "metadata": {},
   "source": [
    "**Visit for AQE** : https://spark.apache.org/docs/latest/sql-performance-tuning.html                                        \n",
    "**Visit for AQE** : https://kyuubi.readthedocs.io/en/master/deployment/spark/aqe.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34306765-aaf6-4ed3-92de-3975cb852dc5",
   "metadata": {},
   "source": [
    "**Q. What is the caching and persist**\n",
    "****\n",
    "**A.** Caching and persist both in Spark is an optimization technique to store the intermediate result of an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, Dataframe, and Dataset computation results). \n",
    "- cache() method default saves it to memory (MEMORY_ONLY) or spark storage pole\n",
    "- persist() method is used to store it to the user-defined storage level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87cd63-c2c0-4589-90a8-654547ea234e",
   "metadata": {},
   "source": [
    "**Q. Best choice to select storage label in prestige**\n",
    "***\n",
    "**A.** ` df.persist(StorageLevel(useDisk, useMemory, UseOffHeap, deserialize, Replication = 1)) `              \n",
    "` df.persist(StorageLevel(False, True, False, False, Replication = 1)) `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78247586-72d6-4063-bac4-43c581c8d881",
   "metadata": {},
   "source": [
    "**Q. RAM and Hard-Disk in which format store data**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "1. You can have serialized and deserialized format format while caching to **MEMORY**, default is deserialized(speed processing)                \n",
    "2. You can have only serialized format while caching to **DISK** and **Off_HEAP**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b3623-7688-43c9-a869-034345028f11",
   "metadata": {},
   "source": [
    "**Q. Why we use caching in a spark**\n",
    "***\n",
    "**A.**  \n",
    "\n",
    "When we create a dataframe, this dataframe is store in short-live memory, short-live memory is alive in a short time when we do not use this dataframe, this data frame will be deleted form short-live memory, If I again try to use this dataframe, again create is by using **DAG**, and this is a time-consuming process, In this case, we are creating a cache for this data frame to store in spark-memory it will save some processing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a3e18-5028-4963-a09a-eebe76554d5b",
   "metadata": {},
   "source": [
    "\n",
    "**Q. Why do we need caching or persistence**\n",
    "****\n",
    "**A.** Reduce processing time(to create dataframe form DAG), both store data in spark-storeage-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aea0c-83ea-4eb7-9435-f1421d7a8c14",
   "metadata": {},
   "source": [
    "**Q. How many storage level in Spark perseste()**\n",
    "***\n",
    "**A.** \n",
    "- RAM: Store data in De-Serialized\n",
    "- Disk: Store data in Serialized\n",
    "\n",
    "\n",
    "| Storage Level        | Description                                                        | Example                                           | Prone | Causes |\n",
    "|----------------------|--------------------------------------------------------------------|---------------------------------------------------|-------|--------|\n",
    "| MEMORY_ONLY          | Data stored as deserialized Java objects in JVM heap.              | `df.persist(storageLevel=\"MEMORY_ONLY\")`           | High  | Memory pressure |\n",
    "| MEMORY_ONLY_SER      | Data stored as serialized Java objects in JVM heap.                | `df.persist(storageLevel=\"MEMORY_ONLY_SER\")`       | Moderate  | Serialization overhead |\n",
    "| MEMORY_ONLY_2        | MEMORY_ONLY with 2 replicas for fault tolerance.                   | `df.persist(storageLevel=\"MEMORY_ONLY_2\")`         | High  | Memory pressure, Fault tolerance |\n",
    "| DISK_ONLY            | Data stored on disk in serialized format, read into memory on demand. | `df.persist(storageLevel=\"DISK_ONLY\")`           | Low  | Disk I/O |\n",
    "| MEMORY_AND_DISK      | Data stored as deserialized Java objects in JVM heap, with overflow to disk. | `df.persist(storageLevel=\"MEMORY_AND_DISK\")`  | High  | Memory pressure, Disk I/O |\n",
    "| MEMORY_AND_DISK_SER  | Data stored as serialized Java objects in JVM heap, with overflow to disk. | `df.persist(storageLevel=\"MEMORY_AND_DISK_SER\")`  | Moderate  | Serialization overhead, Disk I/O |\n",
    "| MEMORY_AND_DISK_2    | MEMORY_AND_DISK with 2 replicas for fault tolerance.               | `df.persist(storageLevel=\"MEMORY_AND_DISK_2\")`   | High  | Memory pressure, Disk I/O, Fault tolerance |\n",
    "| OFF_HEAP             | Data stored off-heap using Spark's external memory management.     | `df.persist(storageLevel=\"OFF_HEAP\")`            | High  | Memory pressure, External memory management overhead |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427572cc-f455-4765-98ae-d84a7d8e90e4",
   "metadata": {},
   "source": [
    "**Q. caching Vs persistence**\n",
    "***\n",
    "**A.**\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Criteria</th>\n",
    "      <th>Caching using <code>persist()</code></th>\n",
    "      <th>Caching using <code>cache()</code></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Lazy Operation</td>\n",
    "      <td>Caching is a lazy operation; it occurs on executing an action.</td>\n",
    "      <td>Similar lazy operation as <code>persist()</code>.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Storage Fraction</td>\n",
    "      <td>Does not store a fraction of a partition; either stores the complete partition or none.</td>\n",
    "      <td>Similar behavior, complete partition or none.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Caching Complete DataFrame</td>\n",
    "      <td>Use an action like <code>count()</code> to cache the complete DataFrame.</td>\n",
    "      <td>Similar approach using actions like <code>count()</code>.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Default Storage Level</td>\n",
    "      <td>MEMORY_AND_DISK_SER with 1x replication.</td>\n",
    "      <td>MEMORY_AND_DISK_SER with 1x replication.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Removing from Cache</td>\n",
    "      <td>Use <code>unpersist()</code> method.</td>\n",
    "      <td>Use <code>unpersist()</code> method.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Different Storage Levels</td>\n",
    "      <td>Offers flexibility to use different storage levels.</td>\n",
    "      <td>Similar flexibility in using various storage levels.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005a928-af1f-4f47-b8b7-db79c1943be8",
   "metadata": {},
   "source": [
    "**Q. \n",
    "What happens in case of caching partition not filling on memory**\n",
    "***\n",
    "**A.**  Data will spill on the Disk or recalculate process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0681559-dd63-4ef7-b881-302bc5897924",
   "metadata": {},
   "source": [
    "**Q. When should we avoid cashing**\n",
    "****\n",
    "**A.** When we have a small size dataframe or no need for enough time to recalculate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98524b-2f90-4630-ab0e-885eb43ce5c6",
   "metadata": {},
   "source": [
    "**Q. How do we recalculate, When we lose partition during the read or write process?**\n",
    "***\n",
    "**A.** Partition again recalculated by using DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac86af6-7988-4b80-adca-cf201581cd64",
   "metadata": {},
   "source": [
    "**Q. How to uncache the data**\n",
    "****\n",
    "**A.** By using unpersist() function                                    \n",
    "`df.unpersist()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f8188-fadf-46e1-8373-ed6c4a332162",
   "metadata": {},
   "source": [
    "**Q. Difference between cache and persist**\n",
    "****\n",
    "**A.** cache() nothing but just like persist() only has a fix argument on storage level `MEMORY_AND_DISK`\n",
    "\n",
    "\n",
    "\n",
    "| Operation | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| `cache`   | Quick method equivalent to `persist` with default storage level (`MEMORY_ONLY`). | `df.cache()` |\n",
    "| `persist` | More flexible operation allowing customization of storage levels and options. | `df.persist(storageLevel=\"MEMORY_ONLY_SER_2\")` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a8293-4dda-4f4a-b0a2-60898c67b1dd",
   "metadata": {},
   "source": [
    "**Q. Which storage level to choice**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "<img src=\"https://i.ibb.co/m43pzGg/024.png\" width=\"1200\" height=\"600\" alt=\"024\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329ac9b-0614-4c94-ba88-62d08218d37e",
   "metadata": {},
   "source": [
    "**Q. What is the dynamic resource allocation in Spark**\n",
    "****\n",
    "**A.** Dynamic Resource Allocation in Apache Spark is a feature that allows a Spark application to dynamically acquire and release executor resources based on the workload. It aims to optimize resource utilization and improve the overall performance of Spark applications by adjusting the number of executors based on the workload's demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36863de3-f020-4368-b6c3-755b59b64404",
   "metadata": {},
   "source": [
    "`spark-submit \\`                           \n",
    "`  --conf spark.dynamicAllocation.enabled=true \\`                                \n",
    "`  --conf spark.dynamicAllocation.minExecutors=5 \\`                       \n",
    "`  --conf spark.dynamicAllocation.maxExecutors=20 \\`                       \n",
    "`  --class YourSparkApp \\`                             \n",
    "`  your-spark-app.jar`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9876c-1d5c-418f-856f-8c82a7017b5b",
   "metadata": {},
   "source": [
    "**Q. How resource manager provides the resources if dynamic resource allocation**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "`  --conf spark.dynamicAllocation.enabled=true \\`                                \n",
    "`  --conf spark.dynamicAllocation.minExecutors=5 \\`                       \n",
    "`  --conf spark.dynamicAllocation.maxExecutors=20 \\`                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348db01-c723-48c5-92d8-1e12a4d3a479",
   "metadata": {},
   "source": [
    "**Q. What are the resource allocation techniques we have in Apache Spark**\n",
    "****\n",
    "**A.** Two types of technique\n",
    "- Static Resource Allocation\n",
    "- Dynamic Resource Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127ac2e-6402-43dd-9d1b-7889c293edeb",
   "metadata": {},
   "source": [
    "**Q. What are the challenges involved with dynamic resource allocation**\n",
    "****\n",
    "**A.** Be careful about release time and demand time\n",
    "\n",
    "- relase but have minimum executor\n",
    "- set a perfect time to demand executor\n",
    "- Care on the ideal time to de-allocate resources from spark jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9bc024-dcfa-44b4-8a5f-1e12913cfd05",
   "metadata": {},
   "source": [
    "**Q. When to avoid dynamic resource allocation?**\n",
    "***\n",
    "**A.** When I have a run on productuon level code and critical spark job, I do not want to delay any jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ae408-805e-40fd-bedc-bd235dcc1198",
   "metadata": {},
   "source": [
    "**Q. What is the ideal time to release resources**\n",
    "***\n",
    "**A.** 60s by default and configurable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b2d6a-8d3f-42f9-a7fa-60353456c65c",
   "metadata": {},
   "source": [
    "**Q. How spark will remove or add resources**\n",
    "***\n",
    "\n",
    "<b>A.</b> If resource not use 60s spark will remove resource, and add by uisng <b>two fold</b> technique\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/DfKTn6L/025.png\" alt=\"025\" width=\"400\" height=\"50\" border=\"0\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb651e8-3c0a-46ee-bb27-e24ebc789cbe",
   "metadata": {},
   "source": [
    "**What configuration needed for dynamic resource allocation**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "`spark-submit \\`                             \n",
    "`    --name Your AppName\\ --master yarn \\ `                          \n",
    "`    --deploy-mode cluster \\`                                 \n",
    "`    --conf spark.dynamicAllocation.enabled=true \\ `                    \n",
    "`    --conf spark.dynamicAllocation.minExecutors=5 \\ `                          \n",
    "`    --conf spark.dynamicAllocation.maxExecutors=49 \\`                  \n",
    "`    --conf spark.shuffleTracking.enabled=true\\`                 \n",
    "`    --conf spark.dynamicAllocation.executorIdle Timeout=45s \\`                     \n",
    "`    --conf spark.scheduler.backlogTimeout=2s \\`                   \n",
    "`    --conf spark.executor.memory=20g \\`                          \n",
    "`    --conf spark.executor.cores=4 \\`                           \n",
    "`    --conf spark.driver.memory=20g \\`                    \n",
    "`    --py-files python_dependencies.zip \\`                       \n",
    "`    main.py`                                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea8478-3052-4095-9022-285fef591e9c",
   "metadata": {},
   "source": [
    "**Q. What is spark.shuffleTracking.enabled=true configration**\n",
    "***\n",
    "**A.** When data shuffle then write it to output exchange, and use it when executor de-allocates, this configuration protects cache in this executor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0bfec3-5119-479e-aa9c-cf7c4ee73d73",
   "metadata": {},
   "source": [
    "**Q. What is the dynamic partition pruning**\n",
    "****\n",
    "**A.**  It allows Spark to dynamically skip unnecessary partitions when executing queries. This feature is handy when dealing with large datasets and partitioned tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45879871-a435-417e-81d6-d24564ea599b",
   "metadata": {},
   "source": [
    "**Q. Why do we need dynamic partition pruning (DPP)**\n",
    "****\n",
    "**A.** Dynamic Partition Pruning optimizes query performance by intelligently skipping unnecessary partitions based on filter conditions, reducing data scanning and improving resource efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa06c0-57f6-4e2f-805b-3bd3c2fed228",
   "metadata": {},
   "source": [
    "**Q. When dynamic partition pruning will not work**\n",
    "****\n",
    "**A.**\n",
    "- When data not partitioned\n",
    "- 2nd table cant be broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc219f7c-c134-4e6a-9b82-14d789dc2ce2",
   "metadata": {},
   "source": [
    "**Q. What is the data skewness problem**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed29bb-27ef-4d73-b58b-fddb74f6ca03",
   "metadata": {},
   "source": [
    "**Q. What are the ways to remove skewness**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a4c156-5760-4dab-8cff-d3e27996f6a1",
   "metadata": {},
   "source": [
    "**Q. What is salting**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a1a7f-a32d-44d6-ac84-bfa56c4b5ca5",
   "metadata": {},
   "source": [
    "**Q. How can we implement salting**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd48972-0c45-4a24-b265-802ad8527ad5",
   "metadata": {},
   "source": [
    "**Q. Which cluster manager have you used in project**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63834c-715b-4bf4-a88e-b272336d0f13",
   "metadata": {},
   "source": [
    "\r\n",
    "**Q. What is the size of the cluster?**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c4e24-e26b-4e32-aa53-f2deae2219e1",
   "metadata": {},
   "source": [
    "**Q. Data warehouse Vs Data Lake**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "| DATAWAREHOUSE | Vs | DATALAKE|\n",
    "|----------|---------------|-----------------|\n",
    "|THINK FIRST, LOAD LATER |**Philosophy** |LOAD FIRST, THINK LATER|\n",
    "|STRUCTURED DATA| **Processing** | STRUCTURED, SEMI-STRUCTURED, UNSTRUCTURED DATA\n",
    "|EXPENSIVE FOR LARGE DATA STORAGE|**Storage**|BUILT FOR LOW COST STORAGE|\n",
    "|LESS AGILE (RIGID)|**Agility**|HIGHLY AGILE (FLEXIBLE)|\n",
    "|OPERATIONAL REPORTING(SUITS BUSINESS USERS)|**Usage**|ADVANCED ANALYTICS (SUITS DATA SCIENTISTS)|\n",
    "|MATURED|**Security**|STILL MATURING|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641b6cb-b851-4b87-951f-3b7dc463aefa",
   "metadata": {},
   "source": [
    "## Spark Execcutor Memory Management System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e0199-c68d-49b1-9ec5-60909cded176",
   "metadata": {},
   "source": [
    "<div style=\"float: left; width: 70%;\">\n",
    "    <p>Lets Assume ...</p>\n",
    "    <blockquote>\n",
    "      <ul>\n",
    "        <li>1 worker node</li>\n",
    "        <li>1 executor within worker</li>\n",
    "        <li>Worker memory -&gt; 16 GB</li>\n",
    "        <li>Executor memory -&gt; 10 GB</li>\n",
    "      </ul>\n",
    "    </blockquote>\n",
    "    <p>So remaining 6 GB controlled by OS which is called off-heap memory</p>\n",
    "    <blockquote>\n",
    "      <ul>\n",
    "        <li>Executor memory controlled by JVM process</li>\n",
    "      </ul>\n",
    "    </blockquote>\n",
    "</div>\n",
    "\n",
    "<div style=\"float: right; width: 30%;\">\n",
    "    <img src=\"images/030.png\" width=\"300\" height=\"600\" alt=\"024\" border=\"0\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557d02b-9d30-468f-9861-87238eb831da",
   "metadata": {},
   "source": [
    "> - Executor memory controlled by JVM process\n",
    "> - off-heap memory controlled by OS(operating system) \n",
    "> - on-heap memory controlled by Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c927e5a-6588-495e-84e9-43de8f3dec29",
   "metadata": {},
   "source": [
    "1. **Reserved Memory**: Reserved by Spark for internal purposes               \n",
    "2. **User Memory**: For storing the data- structures created and managed by the user's code            \n",
    "3. **Execution Memory**: JVM heap space used by data-structures during shuffle operations (join and aggregations)         \n",
    "4. **Storage Memory**: JVM heap space reserved for cached data                 \n",
    "UMM = Execution memory (50% UMM) + Storage memory (50% of UMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdf8fe-d758-4b95-b7db-bf81a9a51d8f",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <div style=\"width: 65%;\">\n",
    "<blockquote>\n",
    "  <ul>\n",
    "    <li>Each executor within the worker node has access to Off-heap memory</li>\n",
    "    <li>Off-Heap memory can be used by Spark explicitly for storing its data</li>\n",
    "    <li>The amount of off-heap memory used by Spark to store actual data frames is governed by <code>spark.memory.offHeap.size</code></li>\n",
    "    <li>To enable off-heap memory, <code>set spark.memory.offHeap.use=true</code></li>\n",
    "    <li>Accessing off-heap is slightly slower than accessing on-heap storage but still faster than reading/writing from a disk. GC (Garbage Collector) Scan can be avoided by using off-heap memory</li>\n",
    "  </ul>\n",
    "</blockquote>\n",
    "    </div>\n",
    "\n",
    "  <div style=\"width: 35%;\">\n",
    "    <img src=\"images/031.png\" width=\"200\" height=\"600\" alt=\"024\" border=\"0\">\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d35b8-9be0-4bdd-b26b-19fe8a66f3fe",
   "metadata": {},
   "source": [
    "| **On-Heap**      | **Off-Heap**              |\n",
    "|----------|-------|\n",
    "| Better performance than Off-heap because object allocation and deallocation happens automatically | Slower than On-heap but still better than disc performance. Manual memory management                                                                  |\n",
    "| Managed and controlled by Garbage collector within JVM process so adding overhead of GC scans     | Directly managed by Operating system so avoiding the overhead of GC                                                                                   |\n",
    "| Data stored in the format of Java bytes (deserialized) which Java can process efficiently         | Data stored in the format of array of bytes(serialized). So adding overhead of serializing/ deserializing when java program needs to process the data |\n",
    "| While processing smaller sets of data that can fit into heap memory, this option is suitable      | When need to store bigger dataset that can not fit into heap memory, can make advantage of off-heap memory to store the data outside JVM process      |\n",
    "|                                                                                                   |                                                                                                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6829e8-15cc-4d77-ad06-945332186f7e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
