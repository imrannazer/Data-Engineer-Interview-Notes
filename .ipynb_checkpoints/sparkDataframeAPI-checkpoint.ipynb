{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c7b37a-f329-4c36-afda-f7c551beb204",
   "metadata": {},
   "source": [
    "**Q. How to explain PySpark code**\n",
    "****\n",
    "**A.** explain( ) function, it shows cost based plan from catalyst optimizer End of the code ` df.select(\"*\").filter(col(\"age\")>20)  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1755d3f-ae9e-423a-a0ae-c3ee20e51eaa",
   "metadata": {},
   "source": [
    "**Q. What is the difference between show( ) and collect( ) methods**\n",
    "***\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4c3ff-a456-40ff-b055-a4a1ba75aa4d",
   "metadata": {},
   "source": [
    "**Q. What is the difference between take( ) and show( ) methods**\n",
    "***\n",
    "**A.** \n",
    "\n",
    "| Feature          | `show()`                                     | `take(n)`                              |\n",
    "| ---------------- | -------------------------------------------- | --------------------------------------|\n",
    "| **Purpose**      | Display DataFrame in a tabular format        | Retrieve first n rows programmatically|\n",
    "| **Usage**        | `df.show()`                                  | `rows = df.take(n)`                    |\n",
    "| **Output**       | Tabular display in the console               | display in List of `Row` objects                 |\n",
    "| **Default Rows** | Shows first 20 rows by default               | Retrieves the first n rows specified |\n",
    "| **Display Limit**| Customizable display limit with `show(n)`    | N/A                                   |\n",
    "| **Example**      | `df.show()`                                 | `rows = df.take(5)`                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8aa71-cb0b-4a13-8792-00a2fa4de64d",
   "metadata": {},
   "source": [
    "**Q. How to create Schema in PySpark**\n",
    "****\n",
    "**A.** There is three way to create\n",
    "- String\n",
    "  - `schema = \"id int, name string, salary float, date_of_joining date\"`\n",
    "- StructType([StructField(col(\"id\"), integerType(), null=True)])                  \n",
    "  - `StructType([`                                          \n",
    "   `    StructField(\"id\", IntegerType(), True),`                         \n",
    "    `    StructField(\"name\", StringType(), True),`                   \n",
    "    `    StructField(\"salary\", FloatType(), True),`                \n",
    "    `    StructField(\"id\", StringType(), True),`             \n",
    "   `    StructField(\"date_of_joining\", DateType(), True)])`             \n",
    "- StructType().add()\n",
    "  - ` schema2 = StructType().add(\"name\", StringType(), nullable = True).add(\"age\", StringType(), nullable = True) `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23965413-1211-48f0-b53c-5ecc17659dc1",
   "metadata": {},
   "source": [
    "**Q. What is the StructType and StructField in schema**\n",
    "****\n",
    "**A.**`StructType()`: Define structure of Dataframe          \n",
    "`StructType()`: Define metadata of the Dataframe columns\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718f074-dfcb-4a5c-a949-c170b091fce1",
   "metadata": {},
   "source": [
    "**Q. What if I have a header in my DataFrame**\n",
    "****\n",
    "**A.** Use `option(\"header\", True)` or skip this header `option(\"skipRows\", 4)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bff556-c65e-4ee0-884a-ec6bd2f1ee59",
   "metadata": {},
   "source": [
    "**Q. How to create schema for struct and array**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "**Create schema for Struct**\n",
    "\n",
    "`schema_with_struct = StructType([`                   \n",
    "`    StructField(\"id\", StringType(), True),`             \n",
    "`    StructField(\"details\", StructType([`                     \n",
    "`        StructField(\"name\", StringType(), True),`                \n",
    "`        StructField(\"age\", StringType(), True)`                   \n",
    "`    ]), True),`                                         \n",
    "`    StructField(\"score\", StringType(), True)`                             \n",
    "`])`                                          \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Create schema for Array**\n",
    "\n",
    "`schema_with_array = StructType([`                 \n",
    "`    StructField(\"id\", StringType(), True),`                 \n",
    "`    StructField(\"names\", ArrayType(StringType()), True),`                        \n",
    "`    StructField(\"score\", StringType(), True)`                     \n",
    "`])`                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb29ee-93a8-449d-a9bd-23ad8a470249",
   "metadata": {},
   "source": [
    "**Q. How to create data-frame in pySpark**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "- Career data-frame by using a file                          \n",
    "` df = spark.read\\`                   \n",
    "`    .option('header', True)`                \n",
    "`    .option('inferschema', True)\\`                 \n",
    "`    .format('csv')\\`                        \n",
    "`    .load(r'\\C:\\data-path\\csv_data.csv')`\n",
    "\n",
    "- Career data frame by using verbal                    \n",
    "` df = spark.createDataFrame(data = data, schema = my_schema)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f53e6-caa5-44cf-8329-c2642b6d785a",
   "metadata": {},
   "source": [
    "**Q. More option( ) values use in spark dataframe**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "`df = spark.read.format(\"csv\")\\` # by default parquet format                                      \n",
    "`    .option(\"header\", True)\\`                        \n",
    "`    .option(\"inferschema\", True)\\`                     \n",
    "`    .option(\"sep\", ',')\\`                              \n",
    "`    .option(\"comment\", \"This is comment messages\")\\`                  \n",
    "`    .option(\"nullValue\", \"null_data\")\\`                  \n",
    "`   .option(\"lineSep\", \"\\n\")\\`                   \n",
    "`    .option(\"delimiter\", \";\")\\`                 \n",
    "`    .option(\"mode\", \"PERMISSIVE\")\\`                            \n",
    "`    .option(\"skipRows\", 1)` # skip single upper row                          \n",
    "`    .load(\"file_path\")`              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d214aa6-0c5e-481e-82ae-47170fff5291",
   "metadata": {},
   "source": [
    "**Q. How to skip a range of rows in dataframe**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<img src=\"https://i.ibb.co/xz5WgCN/015.jpg\" style=\"width:100%\"/>\n",
    "\n",
    "` range_ = full_df.head(5)[1:4] ` # select rows form dataframe                    \n",
    "` range_rdd = sc.parallelize(head) ` # convert list to rdd                     \n",
    "` range_df = df_w.toDF() ` # convert rdd to dataframe                             \n",
    "` modify_df = full_df.subtract(df_w) ` # subtract range_rdd from full_df                  \n",
    "` modify_df.show() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e4cf4-f59d-41e9-bbc0-71abc3fea1fc",
   "metadata": {},
   "source": [
    "**Q. How to print a range of dataframe**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` display(df.head()) ` # display first row as a list                      \n",
    "` display(df.head(3)) ` # display first 3 rows                       \n",
    "` display(df.first()) ` # display first row as a list                     \n",
    "` display(df.head(5)[2]) ` # getting first five rows but display only third row                  \n",
    "` display(full_df.head(5)[2:4]) ` # getting first 10 rows but display only third and fourth row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0ffe0-4133-49e5-a7b0-89de3b77fdd9",
   "metadata": {},
   "source": [
    "**Q. How to create Spark SQL table by using pySpark data-frame**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df.createOrReplaceTempView('sql_tab')  ` # Convert PySpark data-frame into MySQL table               \n",
    "` spark.sql(\"\"\"\r\n",
    "select * from sql_tab\r\n",
    "where DEST_COUNTRY_NAME = 'United States'\r\n",
    "limit 3\r\n",
    "\"\"\").show()  `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32afb8-6cce-4a85-943d-1bba4fbffedd",
   "metadata": {},
   "source": [
    "**Q. What is the difference between createOrReplaceTempView( ) and createOrReplaceGlobalTempView( )**\n",
    "***\n",
    "**A.** View Constructs a virtual table that has no physical data.              \n",
    "` CreateOrReplaceTempView `: It is session based. It is saved in defualt database                \n",
    "      <code> df.CreateOrReplaceTempView(\"sql_table\")                      \n",
    "      ` %sql `                                                          \n",
    "      ` SELECT * FROM sql_table `</code>                 \n",
    "\n",
    "      \n",
    "` CreateOrReplaceGlobalTempView `:It is not session based. It is saved in global_temp database            \n",
    "      <code> df.CreateOrReplaceGlobalTempView(\"sql_table_global\")                      \n",
    "      ` %sql `                                                          \n",
    "      ` SELECT * FROM global_temp.sql_table_global `</code>                                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbcc4da-924b-45cd-a57f-a38b1b3f74f1",
   "metadata": {},
   "source": [
    "**Q. Have you worked with corrupted records**\n",
    "****\n",
    "**A.** Yes!                         \n",
    "visit:https://medium.com/@sasidharan-r/how-to-handle-corrupt-or-bad-record-in-apache-spark-custom-logic-pyspark-aws-430ddec9bb41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215122e-6e62-4302-b6cd-f17c0d11cb09",
   "metadata": {},
   "source": [
    "**Q. When do you say that records are corrupted**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "\n",
    "- In JSON file              \n",
    "  - Missing {               \n",
    "\n",
    "- In CSV file             \n",
    "  - More or Less value according to columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ca889-b0f2-4392-9d32-fd2604384ad2",
   "metadata": {},
   "source": [
    "**Q. What happens when we encounter corrupted records in different read modes**\r\n",
    "****\r\n",
    "**A.**`option(\"mode\", \"PERMISSIVE\")`: Set null value to all corrupted fields              \n",
    "`option(\"mode\", \"DROPMALFORMED\")`: Drop the corrupted record/row              \n",
    "`option(\"mode\", \"FAILFAST\")`: Fail execution if malformed record in dataset             \n",
    "\n",
    "By default `option(\"mode\", \"PERMISSIVE\")` \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d918ed-92e1-41c1-aa5f-36ad54eb92b2",
   "metadata": {},
   "source": [
    "**Q. How can we read and write bad records**\n",
    "****\n",
    "**A.** \n",
    "1. Create a dataframe schema (we can not use read mode)                        \n",
    "    `schema = StructType([`               \n",
    "    `StructField(\"id\", IntegerType(), True),`               \n",
    "    `StructField(\"name\", StringType(), True),`                 \n",
    "    `StructField(\"salary\", FloatType(), True),`                     \n",
    "    `StructField(\"_corrupt_record\", StringType(), nullable = True)` # create this column for corrupt records          \n",
    "    `])`\n",
    " 2. Create datafame with this schema                         \n",
    "    `df = spark.read.\\ `              \n",
    "    `format(\"csv\")\\ `                 \n",
    "    `.schema(schema)\\ `                \n",
    "    `.load(r\"/FileStore/tables/Algerian_forest_fires_cleaned_dataset.csv\")`\n",
    "<img src=\"image/004.webp\" />\n",
    "3. Write bad/corrupet records            \n",
    "   ` df = spark.read.\\`               \n",
    "    `format(\"csv\")\\`               \n",
    "    `.schema(schema)\\`                                                          \n",
    "    `.option(\"badRecordsPath\", \"/FilesStore/tables/bad_redods\")\\`               \n",
    "    `.load(r\"/FileStore/tables/Algerian_forest_fires_cleaned_dataset.csv\")`             \n",
    "Note: bad_records save in JSON file format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4abc4-0867-40c5-bf92-888f2ba3b573",
   "metadata": {},
   "source": [
    "**Q. Where do you store corrupted records and how can we access them later**\n",
    "****\n",
    "**A.** Assign a path to store bad record `option(\"badRecordsPath\",\"/file/store/data/\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec6a6b-5983-46aa-b200-d34204d022ff",
   "metadata": {},
   "source": [
    "**Q. How to print all dataframe columns name**\n",
    "***\n",
    "**A.** ` df.columns  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d55802-7717-451e-b7f6-ad3c717a4833",
   "metadata": {},
   "source": [
    "**Q. How to print data from schema with StrutType( ) and StructField( )**\n",
    "***\n",
    "**A.** ` df.schema `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559c0a2-7975-480d-915c-f8b125831bbf",
   "metadata": {},
   "source": [
    "**Q. How to display dataframe columns name and data type**\n",
    "***\n",
    "**A.** ` df.dtypes `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f0f04-c6a1-4f0e-aff5-88d0f3225fd6",
   "metadata": {},
   "source": [
    "**Q. List of Spark Data Types**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<table>\r\n",
    "  <tbody>\r\n",
    "    <tr>\r\n",
    "      <td>StringType</td>\r\n",
    "      <td>ShortType</td>\r\n",
    "      <td>ArrayType</td>\r\n",
    "      <td>IntegerType</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>MapType</td>\r\n",
    "      <td>LongType</td>\r\n",
    "      <td>StructType</td>\r\n",
    "      <td>FloatType</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>DateType</td>\r\n",
    "      <td>DoubleType</td>\r\n",
    "      <td>TimestampType</td>\r\n",
    "      <td>DecimalType</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>BooleanType</td>\r\n",
    "      <td>ByteType</td>\r\n",
    "      <td>CalendarIntervalType</td>\r\n",
    "      <td>HiveStringType</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>BinaryType</td>\r\n",
    "      <td>ObjectType</td>\r\n",
    "      <td>NumericType</td>\r\n",
    "      <td>NullType</td>\r\n",
    "    </tr>\r\n",
    "  </tbody>\r\n",
    "</table>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43191a2c-500e-465e-9b6e-37d971a216d6",
   "metadata": {},
   "source": [
    "**Q. What is JSON data and how to read it in Apache PySpark**\n",
    "****\n",
    "**A.** JSON standard for **<u>JavaScript Object Notation</u>** is a semi-structured data, store data in key:value pair, use ` format(\"json\") ` \n",
    "\n",
    "- JSON is a semi-structured data type\n",
    "- JSON is a key value pair data format file\n",
    "- Every record enclosed in Curly braces\n",
    "- Struck Fallatan in columns by using *.* ` df.select(col(\"Address.*\")).show() `\n",
    "- Array a Fallatan in rows/records by using ` df.select(explode(col(\"company_name\")).alias(\"new_col_name\")).show() `\n",
    "                                 \n",
    "`js_df = spark.read.option(\"header\", True)\\`                     \n",
    "`    .option(\"multiline\", True)\\`                           \n",
    "`    .option(\"inferschema\", True)\\`                      \n",
    "`    .format(\"json\")\\`                        \n",
    "`    .load(\"/FileStore/tables/data/resturant_json_data.json\")`                  \n",
    "`js_df.show()`                                                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfaf9f-32c0-4146-968c-bd793d1b88b4",
   "metadata": {},
   "source": [
    "**Q. What if I have 3 keys in all lines and 1 key in one line in the JSON file**\n",
    "****\n",
    "**A.** Create 4 columns in dataframe and assign 4<sup>th</sup> column null if value is not persent \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb39868d-85c8-4fbd-8ea4-90e2fda124e8",
   "metadata": {},
   "source": [
    "**Q. What is multi-line and line-delimited JSON**\n",
    "****\n",
    "**A.**                        \n",
    "**1. Multi-line** : Where JSON single record in more than one line                \n",
    "`          {`                     \n",
    "`            \"name\":\"Nazer\",`                  \n",
    "`            \"email\":\"naziri1920@gmail.com\"`              \n",
    "`            \"mobile\": 5847896542`                    \n",
    "`          }`                   \n",
    "\n",
    "**2. line-delimited**: Single line JSON                  \n",
    "`          {\"name\":\"Nazer\",\"email\":\"naziri1920@gmail.com\",\"mobile\":123456790}`\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb7da15-88f2-433d-af82-4a6d08241dfb",
   "metadata": {},
   "source": [
    "**Q. Which one works faster multi-Line or Line-delimited in JSON in file format**\n",
    "****\n",
    "**A.** line-delimited work fister bucause by derault spark consider JSON line-delimited\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb448d78-ef39-4580-bf9a-bea5e103c4ca",
   "metadata": {},
   "source": [
    "**Q. How to read nested JSON into PySpark DataFrame**\n",
    "****\n",
    "**A.** Use ` option(\"multiline\", True) ` and ` format(\"json\") `\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be005bb-210b-499f-ad59-227e1192b799",
   "metadata": {},
   "source": [
    "**Q. What will happen if I have a corrupted JSON record and corrupted file**\n",
    "****\n",
    "**A.**  In case of corrupted record, this record is saved in \"_curropt_record column\". In case of corrupted file return error. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20669c01-6287-437a-bd49-6f6554c7aeef",
   "metadata": {},
   "source": [
    "**Q. How to flatten Array and Struct in data frame JSON file format**\n",
    "***\n",
    "**A.** by using ` explode( ) ` and **.** function\n",
    "\n",
    "`js_df.select(explode(\"restaurants\").alias(\"new_restaurant\"))\\`                         \n",
    "`    .select(col(\"new_restaurant.*\"))\\`                                    \n",
    "`    .select(col(\"restaurant.*\"))\\`                         \n",
    "`    .select(`                         \n",
    "`            col(\"R.*\"),` # flatten all Struct                    \n",
    "`            col(\"location.*\"),`                       \n",
    "`            explode(\"offers\").alias(\"new_offer\"),`  # Flatten Array                          \n",
    "`            col(\"user_rating.*\")`                      \n",
    "`            ).printSchema()`                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa38f1d-fefb-4b5b-8734-425e124b34c3",
   "metadata": {},
   "source": [
    "**Q. How to flatten a struct in dataframe JSON file format**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "`flattened_df = df.select(`                               \n",
    "`    col(\"id\"),`                                  \n",
    "`    col(\"details.name\").alias(\"details_name\"),`                   \n",
    "`    col(\"details.age\").alias(\"details_age\"),`                    \n",
    "`    col(\"details.address.city\").alias(\"details_address_city\"),`                   \n",
    "`    col(\"details.address.zip\").alias(\"details_address_zip\"),`                             \n",
    "`    explode(\"details.hobbies\").alias(\"hobby\"),`                                                            \n",
    "`    col(\"salary\")`                                    \n",
    "`)`                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07602c-4669-4ee4-8f80-4a8ce4867d0f",
   "metadata": {},
   "source": [
    "**Q. What is Parquet**\n",
    "****\n",
    "**A.** Parquet is a default file format in Spark and this columner file format. There is not required any format to define during file read \n",
    "visit: https://medium.com/analytics-vidhya/whats-the-buzz-about-parquet-file-format-8a1fe4f65de\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab19eb-0a98-4e46-b311-b68800a97a9d",
   "metadata": {},
   "source": [
    "**Q. What is the parquet hybrid mode in writing file**\n",
    "***\n",
    "**A.** Here we can see that the hybrid is a combination of row and columnar storage.\n",
    "<img src=\"https://i.ibb.co/BgkqYRw/005.webp\" style=\"width:100%;\"/>\n",
    "<center>Hybrid storage model</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d2e28-7696-4d7a-85a0-da7023cb038d",
   "metadata": {},
   "source": [
    "**Q. Why do we need Parquet**\n",
    "****\n",
    "**A.** \n",
    "- Parquet is a columnar file format, and columnar file format is easy to read and process in case of big-data, low storage required,\n",
    "- parquet saved in hybrid form(data divided into column and rows),\n",
    "- Parquet is a structured file format\n",
    "- Parquet is a binary form\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923274f-b8ff-49c2-948d-6b25168d8228",
   "metadata": {},
   "source": [
    "**Q. Where should we use columnar file format or row file format**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "| Concept                  | OLAP (Online Analytical Processing)                       | OLTP (Online Transactional Processing)                           |\n",
    "|--------------------------|---------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Use Case**             | most of the time read data     | most of the time write data, transactional data, loan data, banking data |\n",
    "| **Characteristics**      | Supports complex queries, aggregations, reporting.     | Optimized for fast, real-time transactional operations.       |\n",
    "| **Example in PySpark**   | Performing complex aggregations using DataFrame API. | Basic CRUD operations on a DataFrame, dealing with individual records. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d86450-f073-4040-9471-c721a6e29626",
   "metadata": {},
   "source": [
    "**Q. How to read a Parquet file**\n",
    "****\n",
    "**A.** \n",
    "`spark.read.option(\"header\", True).load(\"file path\")` There is not necessary to provide format information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52035b74-c5a4-4665-af21-6d7c9c68d710",
   "metadata": {},
   "source": [
    "**Q. How to read parquet file in windows CMD**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "Install these libraries                                        \n",
    "> pip install pyarrow                        \n",
    "> pip install parquet-tools\n",
    "\n",
    "Open python terminal and run this code\n",
    "> parquet_file = pq.ParquetFile(r'D:\\Big-Data-2023\\git_repo\\Data-Engineer-Interview-Notes\\git_ignore\\data\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
    "> parquet_file.metadata                                   \n",
    "> parquet_file.metadata.row_group(0)                        \n",
    "> parquet_file.metadata.row_group(0).column(0)                          \n",
    "> parquet_file.metadata.row_group(0).column(0).statistics\n",
    "\n",
    "Run the below command in cmd/terminal\n",
    ">parquet-tools show  D:\\Big-Data-2023\\git_repo\\Data-Engineer-Interview-Notes\\git_ignore\\data\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet                          \n",
    ">parquet-tools inspect  D:\\Big-Data-2023\\git_repo\\Data-Engineer-Interview-Notes\\git_ignore\\data\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8efa1-0670-46a3-9293-6eb4a0557429",
   "metadata": {},
   "source": [
    "**Q. How to data organize in parquet**\n",
    "***\n",
    "**A.**\n",
    "Date organizaton in parquet\n",
    "- File \n",
    "  - Row Group (we have metadate(min, max, count, etc) at group level also)\n",
    "    - Column\n",
    "      - Pages\n",
    "        - Metadata\n",
    "          - Min\n",
    "          - Max\n",
    "          - Count\n",
    "         \n",
    "<img src=\"https://i.ibb.co/hYTYXXP/036.png\" style=\"width:100%;height:400px;\" alt=\"Date organizaton in parquet\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d1124-24df-4f9c-8d02-33845ad3a9ff",
   "metadata": {},
   "source": [
    "**Q. What makes Parquet the default choice**\n",
    "***\n",
    "**A.** Parquet follows RLE (Run Length Encoding) Technique.\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea596f2-0849-4ece-92b9-472c784f2f46",
   "metadata": {},
   "source": [
    "**Q. What encoding is done on Parquet data**\r\n",
    "****\r\n",
    "**A.** \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8f6b2-5748-4205-9048-786407f59d1f",
   "metadata": {},
   "source": [
    "**Q. What comparison technique is used in the Parquet file format**\n",
    "****\n",
    "**A.** ` gzip ` comparison technique\n",
    "` df.write.parquet(\"/path/to/your/output\", compression=\"gzip\") ` ,\r\n",
    "parquet by derault use snappy compression codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfab5a4-8dc9-4098-adc5-4d502f913266",
   "metadata": {},
   "source": [
    "**Q. How to optimize the Parquet file**\n",
    "****\n",
    "**A.** By using three technique\n",
    "1. comparison technique ` gzip ` or ` Snappy `\n",
    "2. Parquet has matadate on row group lelvel\n",
    "3. Parquet follows RLE (Run Length Encoding) Technique (store repeated consecutive values efficiently by representing them as a base value and the number of consecutive occurrences)\n",
    "   \n",
    "   <img src=\"https://i.ibb.co/gFq8BPT/039.png\" alt=\"RLE\" width=\"200\" height=\"300\">\n",
    "5. Bit Packing (reducing storage space by optimizing available bits for representing integers.)\n",
    "6. Predicate Pushdown Technique\n",
    "<img src=\"https://i.ibb.co/C61bGjv/037.png\" alt=\"optimization in parquet\" width=\"100%\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d570b2-64aa-46cc-bf7c-bb1ddc018e3e",
   "metadata": {},
   "source": [
    "**Q.How to write data frame to disk in spark**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df.write.format('csv').option('header', True)\\  `                 \n",
    "`     .option('path', '/FileStore/tables/csv_write/')\\  `                   \n",
    "`         .save()  `                \n",
    "\n",
    "- File name create by pySpark in databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175e2f6-fe53-40cf-96ed-d86ec5632952",
   "metadata": {},
   "source": [
    "**Q. How to write data in partition**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df.write.format('csv').option('header', True)\\`                             \n",
    "`    .option('mode','overwrite')\\`                                              \n",
    "`    .option('path', '/FileStore/tables/csv_write_repartition__/')\\`                      \n",
    "`    .partitionBy('ORIGIN_COUNTRY_NAME')\\`                        \n",
    "`    .save()`           \n",
    "\n",
    "> Partition on multiple columns\n",
    "` .partitionBy('col1', 'col2', 'col3')\\` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12682716-a824-4a20-8ef3-391517596a51",
   "metadata": {},
   "source": [
    "**Q. How to write data in bucket**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "\n",
    "` df.write.format('csv').option('header', True)\\ `                                 \n",
    "`        .option('mode','overwrite')\\`                                            \n",
    "`        .option('path', '/FileStore/tables/csv_write_repartition_bucket/')\\`                           \n",
    "`        .bucketBy(3,'ORIGIN_COUNTRY_NAME')\\` #3 is number of buncket         \n",
    "`        .format(\"csv\") `                        \n",
    "`        .saveAsTable('bucket_name')`               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ad535-1fed-4bbe-adef-7876e42faa4b",
   "metadata": {},
   "source": [
    "**Q. What is the best way to write data in bucket**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "Most of the time when we go to bucket data, then 200 partitions interrupt in this bucketing, so the best way to write in bucket use repartitioning and then bucket data\n",
    "\n",
    "\n",
    "` df.repartition(3)\\`                       \n",
    "`        .write.format('csv')\\`            \n",
    "`        .option('header', True)\\ `                                 \n",
    "`        .option('mode','overwrite')\\`                                            \n",
    "`        .option('path', '/FileStore/tables/csv_write_repartition_bucket/')\\`                           \n",
    "`        .bucketBy(3,'ORIGIN_COUNTRY_NAME')\\` #3 is number of buncket                           \n",
    "`        .saveAsTable('bucket_name')`     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ee920-ca5a-4901-b27b-f829adc3248d",
   "metadata": {},
   "source": [
    "**Q. What is the write default mode**\n",
    "***\n",
    "**A.** ` mode(\"error\") ` is default write mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea56e8-b83f-4af4-ad5c-ea5490c4f239",
   "metadata": {},
   "source": [
    "**Q. What are the modes available in DataFrame write**\n",
    "****\n",
    "**A.**                                                                    \n",
    "` mode(\"append\") `: Appends the data to the existing data if it exists.                          \n",
    "` mode(\"overwrite\") `: Overwrites the existing data if it exists.                                  \n",
    "` mode(\"ignore\") `: Ignores the operation if the data already exists.                                  \n",
    "` mode(\"error\") `: Raises an error if the data already exists.                                  \n",
    "` mode(\"errorifexist\") `: Raises an error if the data already exists.                   \n",
    "\n",
    "Note: Spark default write mode is ` mode(\"error\") `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3fc73-62da-4133-84a0-664701aa7ff2",
   "metadata": {},
   "source": [
    "**Q. How to write data into multiple partitions**\n",
    "****\n",
    "**A.**                             \n",
    "\n",
    "` df.repartition(3).write.format('csv').option('header', True)\\  `                 \n",
    "`     .option('path', '/FileStore/tables/csv_write/')\\  `                   \n",
    "`     .save()  `                 \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06891d-0d4a-46d3-9b39-6e24004750e9",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is schema**\n",
    "****\n",
    "**A.** A schema is a combination of **column-name** and **column-data-type**                  \n",
    "\n",
    "- Print schema in data frame ` df.printSchema() `\n",
    "- Print only columns name ` df.columns  `\n",
    "- print data-frame StructType and StructField ` df.schema  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf1e99-402d-41e0-906d-f810cf0cd6c6",
   "metadata": {},
   "source": [
    "**Q. What is DataFrame**\n",
    "****\n",
    "**A.** A DataFrame in PySpark is a                    \n",
    "- distributed,                      \n",
    "- immutable, and                    \n",
    "- lazily evaluated data structure                              \n",
    "that represents structured data and enables scalable data processing across a cluster of machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaeecca-1248-4ef8-b1c0-c62bd7d36480",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q. What is the column**\n",
    "***\n",
    "**A.**  A column represents a named and typed collection of data.                          \n",
    "Columns are expressions, and an expression is a set of transformations on one or more than one value in a record                  \n",
    "` df.select(col('age')+5)  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cc906-e5ee-4d41-8313-08d8f7aa0128",
   "metadata": {},
   "source": [
    "**Q. What is the row**\n",
    "***\n",
    "**A.** Row is an object, define by                                        \n",
    "` from pyspark.sql import Row  `                       \n",
    "` row = Row(1, 'Khan', 2563) `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e77ce-c2e1-4564-985f-b3aa9469fc6f",
   "metadata": {},
   "source": [
    "**Q. How many ways to select columns**\n",
    "***\n",
    "\n",
    "**A.** \n",
    "1.  ` df.select('*').show()  ` # select all columns\n",
    "2.  ` df.select(col('age'),col('salary')).show()  `                     \n",
    "3.  ` df.select(\"age\",\"salary\").show()  `                       \n",
    "4.  ` df.select(df[\"age\"]).show() `  # handy option in case of join                       \n",
    "5.  ` df.select(df.ORIGIN_COUNTRY_NAME).show()  `  # handy option in case of join              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d2e88-7fb2-4f78-84cb-a293be7e7689",
   "metadata": {},
   "source": [
    "**Q. What is the expression**\n",
    "***\n",
    "**A.** expr() used for assigning MySQL queries                         \n",
    "` df.select(expr(\"ORIGIN_COUNTRY_NAME AS new_name\")).show()  `               \n",
    "` df.select(expr(\"CONCAT (fname,' ',lname) AS full_name\")).show()  `               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf3211-d9cc-4ea9-b85c-23d24ee30b5a",
   "metadata": {},
   "source": [
    "**Q. What is aliasing**\n",
    "****\n",
    "**A.** alias( ) Function use to change column name\n",
    "` df.select(col(\"ORIGIN_COUNTRY_NAME\").alias(\"new_name\")).show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07a9b1-beec-47eb-8c21-e2d56a8e11e0",
   "metadata": {},
   "source": [
    "**Q. What is the difference between filter and where in Apache PySpark**\n",
    "****\n",
    "**A.** There is no difference, both are used for filtering result                         \n",
    "\n",
    "` df.select(\"*\").filter(col(\"ORIGIN_COUNTRY_NAME\")=='Romania').show()  `\n",
    "` df.select(\"*\").where(col(\"ORIGIN_COUNTRY_NAME\")=='Romania').show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d09489-e080-49d2-bf22-d21176723d61",
   "metadata": {},
   "source": [
    "**Q. What is the literal function**\n",
    "****\n",
    "**A.** Assign a static value in data frame column\n",
    "` df.withColumn(\"lit_col\", lit(\"123\")).show() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74490a2-f961-445e-8223-fbdc37622b0b",
   "metadata": {},
   "source": [
    "**Q. How to add a new column in DataFrame**\n",
    "****\n",
    "**A.*withColumn() used to add a new column or modify an existing column in data-frame*                              \n",
    "` df.withColumn(\"TOTAL_DEST_COUNTRY\", col(\"count\")+20).show()  `                              \n",
    "` df.withColumn(\"TOTAL_DEST_COUNTRY\", col(\"count\")+20).show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c785ef-b294-4581-8393-b063f1e992e4",
   "metadata": {},
   "source": [
    "**Q. How to rename a column in DataFrame**\n",
    "****\n",
    "**A.**  ` df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME_S\").show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cdcc1-b913-4851-ab7c-382df66a6b87",
   "metadata": {},
   "source": [
    "\n",
    "**Q. How to cast data types**\n",
    "****\n",
    "**A.** \n",
    "` df.withColumnRenamed(\"count\", \"count_traivel\").select(col(\"count_traivel\").cast(IntegerType())).printSchema()  ` #New col whth cast()        \n",
    "` df.withColumn('count', col('count').cast(IntegerType())).printSchema() ` # change schema in defined col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b71718-0bca-4b04-975c-3e5aeff54f93",
   "metadata": {},
   "source": [
    "**Q. How to remove a column in DataFrame**\n",
    "****\n",
    "**A.** drop( ) used to remove an existing column from data-frame                  \n",
    "` df.drop(col(\"DEST_COUNTRY_NAME\")).select(\"*\").show() `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfebf9d-ccee-4bb3-8cd8-d19756df40b0",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is the difference between Union and union all**\n",
    "****\n",
    "**A.** It is used to combine two dataframe vertically, including all rows from both dataframe, even if there are duplicates records.\n",
    "\n",
    "- union() and unionAll() don't remove any duplicate records from dataframe.\n",
    "- union() will remove duplicate records in MySQL table\n",
    "- unionAll() doesn't remove any duplicate records from MySQL table.\n",
    "- Number of columns in both tables must be same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bc1c5-c802-4703-a554-a0ab3e8a11d3",
   "metadata": {},
   "source": [
    "**Q. What will happen if I change the number of columns while Union in the data**\n",
    "****\n",
    "**A.** Return an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2b2e0-7540-4394-9ace-cffa579d5e75",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What if the column name is different**\n",
    "****\n",
    "**A.** Dataframe will be union but fetch header from first table, And column data type may be mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9b2bd-539f-4deb-9686-2cead8aa6d5f",
   "metadata": {},
   "source": [
    "**Q. What is UnionByName**\n",
    "****\n",
    "**A.** UnioByName() tries to find out same column name in both dataframe.\n",
    "\n",
    "- Column order can be mismatched\n",
    "- Column name must be same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13791bd0-964b-4e6e-b348-35b0016f082d",
   "metadata": {},
   "source": [
    "**Q. What is the case when in Spark SQL**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "` df.createOrReplaceTempView(\"sql_table\") `                           \n",
    "` spark.sql( `                                     \n",
    "`    \"\"\" `                    \n",
    "`        select *, `                      \n",
    "`        CASE `                        \n",
    "`            WHEN count>200 and DEST_COUNTRY_NAME = 'United States' THEN 'most_busy' `             \n",
    "`            WHEN count>100 THEN 'busy' `                                            \n",
    "`            ELSE 'okay' `                     \n",
    "`        END AS check_pointing `               \n",
    "`        FROM sql_table `                    \n",
    "`    \"\"\" `                 \n",
    "`).show() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d9af6-44a3-4831-86c6-4e467ffa1587",
   "metadata": {},
   "source": [
    "**Q. What is the when otherwise in Spark**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "`df.withColumn(\"check_point\", when(col(\"count\") > 200, \"most_busy\")\\`                      \n",
    "`    .when(col(\"count\")>100, \"busy\")\\`                    \n",
    "`    .otherwise(\"clear\"))\\`              \n",
    "`    .show()`                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185be32-7530-4dcf-ab40-794fdefd8cb6",
   "metadata": {},
   "source": [
    "**Q. How to deal with Null value in DataFrame**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "`df.withColumn(\"check_point\", when(col(\"count\").isNull(), lit(0))\\` #handel null values                         \n",
    "`    .when(col(\"count\") > 200, \"most_busy\")\\`                       \n",
    "`    .when(col(\"count\")>100, \"busy\")\\`                \n",
    "`    .otherwise(\"clear\"))\\`                  \n",
    "`    .show()`                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26123896-2fa3-49be-92e2-6f36ed932fbf",
   "metadata": {},
   "source": [
    "**Q. How to use case when or when otherwise with multiple AND, OR conditions**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "` df.withColumn('check_pointing', `                             \n",
    "`            when((col(\"count\")>200) & (col(\"DEST_COUNTRY_NAME\")=='United States'),\"Profitable\")\\`                     \n",
    "`            .otherwise(\"okay\")).show()`                                                          \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27c6d2-3420-4b52-b0eb-d33418fcd309",
   "metadata": {},
   "source": [
    "**Q. How to find unique rows**\n",
    "****\n",
    "**A.** distinct( ) Function used to get unique record from data frame                 \n",
    "\n",
    "` df.select(\"*\").distinct().count() ` # get unique rows                              \n",
    "` df.select(col(\"DEST_COUNTRY_NAME\")).distinct().count() ` #get unique country name                          \n",
    "` df.distinct().count() ` #all record based distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a404635-97d0-483d-8e03-fbad77ada045",
   "metadata": {},
   "source": [
    "**Q. How to drop duplicate rows**\n",
    "****\n",
    "**A.** *dropDuplicates( )* and *drop_duplicates( )*\n",
    "\n",
    "\n",
    "` df.dropDuplicates([\"DEST_COUNTRY_NAME\"]).count() ` #drop records on *DEST_COUNTRY_NAME* based output(125)                  \n",
    "`df.dropDuplicates([\"DEST_COUNTRY_NAME\", \"count\"]).count()` #drop records on *DEST_COUNTRY_NAME* and *COUNT*  based output(213)   \n",
    "` df.drop_duplicates([\"DEST_COUNTRY_NAME\", \"count\"]).count() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd2eb84-ed97-4dc1-8197-a46b3615118d",
   "metadata": {},
   "source": [
    "**Q. How to sort the data in ascending and descending order**\r\n",
    "****\r\n",
    "**A.**\n",
    "` df.sort(col(\"count\")).show()` # Dataframe sorting on single column                    \n",
    "` df.sort(col(\"count\"), col(\"ORIGIN_COUNTRY_NAME\")).show() ` # Dataframe sorting on multiple columns             \n",
    "` df.sort(col(\"count\").desc()).show() ` # Dataframe sorting on single column in descending order      \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae64b6d-44ad-4f94-9515-36fc3a0a82b6",
   "metadata": {},
   "source": [
    "**Q. What is the aggregation in PySpark**\n",
    "****\n",
    "**A.** Collecting something together \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d41363-694d-42ee-98b3-4d2b16cc879f",
   "metadata": {},
   "source": [
    "**Q. How to use aggregation in PySpark**\n",
    "****\n",
    "**A.** You can use multiple aggregation functions in agg( ) function                         \n",
    "`df.select(col(\"dept\"),col(\"salary\"))\\`                 \n",
    "`    .groupBy(col(\"dept\"))\\ `                                  \n",
    "`    .agg(sum(col(\"salary\")),avg(col(\"salary\"))).show()`                      \n",
    "-- -------\n",
    "` df.groupBy(col(\"ORIGIN_COUNTRY_NAME\")).agg(round(avg(col(\"count\"))).alias(\"agg_col\")).show() ` # AVG                 \n",
    "` df.agg(count(col(\"count\")).alias(\"Count_col\")).show() ` # number of records                         \n",
    "` df.agg(min(col(\"count\")).alias(\"min_fun\")).show() ` # min                          \n",
    "` df.agg(max(col(\"count\")).alias(\"max_fun\")).show() ` # max                       \n",
    "` df.agg(sum(col(\"count\")).alias(\"sum_fun\")).show() ` #add all values                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd7cd8-33a7-4354-83ca-f852afac0bd7",
   "metadata": {},
   "source": [
    "**Q, Explain count( ) function**\n",
    "***\n",
    "**A.** It is sometime action(when return a result or calls a job) and sometime transformation(when does not call a job)\n",
    "\n",
    "- count( ) skips null value, If I use single column\n",
    "- count( ) skips null value, If all column has null values on same position\n",
    "\n",
    "1. Transformation ` df.select(count(col(\"name\"))) `\n",
    "2. Action ` df.select(\"*\").count() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854c556-4163-4080-954f-5e8c3ea7b77e",
   "metadata": {},
   "source": [
    "**Q. How groupBy works**\n",
    "****\n",
    "**A.** \n",
    "1. GroupBy operation on single column                        \n",
    "   ` df.groupBy(\"dept\").agg(`                       \n",
    "   ` sum(col(\"salary\")).alias(\"tatal_salary\") `                \n",
    "   ` ).show() `\n",
    "2. GroupBy operation on Multiple Columns                          \n",
    "   ` df.groupBy([\"country\",\"dept\"]).agg( `                  \n",
    "   ` sum(col(\"salary\")).alias(\"total_salary\") `                     \n",
    "   ` ).orderBy(col(\"total_salary\")).show() `\n",
    "3. GroupBy operation on Multiple Aggregations           \n",
    "   ` df.groupBy(col(\"dept\")).agg( `     \n",
    "   ` sum(col(\"salary\")).alias(\"total_salary\"), `       \n",
    "   ` round(avg(col(\"salary\")),2).alias(\"avg_salary\") `      \n",
    "   ` ).show() `\n",
    "4. Filter Aggregated data using where condition                \n",
    "   ` df.groupBy(col(\"dept\")).agg( `                   \n",
    "   ` sum(col(\"salary\")).alias(\"total_salary\") `            \n",
    "   ` ).where(col(\"total_salary\")>50000).show() `             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416e7bf-189c-49ec-96fa-789d7f3f56ec",
   "metadata": {},
   "source": [
    "**Q. How to implement groupBy in PySpark**\n",
    "***\n",
    "**A.** select_columns > groupBy() > some aggregration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf7c0e-00aa-4295-a5b6-ea5fa43663f3",
   "metadata": {},
   "source": [
    "**Q.Create this table and answer flowing questions**\n",
    "***\n",
    "**A.**\n",
    "<code>\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"dept\", StringType(), True)\n",
    "])\n",
    "data = [\n",
    "    (\"manish\", 50000, \"IT\"),\n",
    "    (\"vikash\", 60000, \"sales\"),\n",
    "    (\"raushan\", 70000, \"marketing\"),\n",
    "    (\"mukesh\", 80000, \"IT\"),\n",
    "    (\"pritam\", 90000, \"sales\"),\n",
    "    (\"nikita\", 45000, \"marketing\"),\n",
    "    (\"ragini\", 55000, \"marketing\"),\n",
    "    (\"rakesh\", 100000, \"IT\"),\n",
    "    (\"aditya\", 65000, \"IT\"),\n",
    "    (\"rahul\", 50000, \"marketing\")\n",
    "]\n",
    "df_test = spark.createDataFrame(data, schema=schema)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fae244-8e80-4126-bd10-5b698786cbab",
   "metadata": {},
   "source": [
    "**Q. What is the total salary given to its employee**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df_test.select(sum(col(\"salary\")).alias(\"total_salary\")).show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afec579-34d3-4b81-85a4-41c31fbcf99a",
   "metadata": {},
   "source": [
    "**Q. What is the total salary per department wise**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df_test.select(col(\"dept\"), col(\"salary\")).groupBy(col(\"dept\")).agg(sum(col(\"salary\")).alias(\"dept_salary\")).show() `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a0482-7160-4b62-b3ff-31f8f138bc99",
   "metadata": {},
   "source": [
    "**Q. I want all column name with one extra column where total salary of each department is maintained**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` from pyspark.sql.window impoer Window `  # import Window function                               \n",
    "` window = Window.partitionBy(col(\"department\")) ` # create new window              \n",
    "` df.withColumn(\"total_dep_salary\", sum(col(\"salary\")).over(window)).show() `                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cd431-8dfe-4473-9e1a-65d0f46fd91a",
   "metadata": {},
   "source": [
    "**Q. How join works**\n",
    "****\n",
    "**A.** Joins used to combine data from different tables based on a common key. \n",
    "<code>result = df1.join(df2, on=\"id\", how=\"inner\")</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b216d-c81c-4b0a-87e3-48c9eb2f3482",
   "metadata": {},
   "source": [
    "**Q. Why do we need JOIN**\n",
    "****\n",
    "**A.** If we don't have enough information in single table then we need another table that fulfills our requirement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364aaed2-fb9b-4be7-9e19-edfc18dbcc6b",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What to do after joining two tables**\n",
    "****\n",
    "**A.** It depends on you after joining you can use distinct and you can skip this part(using distinct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8f550-72b7-4e00-a69c-ddac45a869ba",
   "metadata": {},
   "source": [
    "**Q. What if tables have the same column name**\n",
    "****\n",
    "**A.** If you don't define column name with data frame it will return an **ambiguous error**               \n",
    "` df = df.join(df2, df[\"id\"] == df2[\"id\"], \"inner\") `                     \n",
    "` df.show() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028dc5e-b809-4601-9a34-f918bbfafcdb",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/qgyN2Vq/joins.png\" style=\"width:100%;height:300px;\" alt=\"joins\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1404a-43c4-4a83-81c0-0e573af82624",
   "metadata": {},
   "source": [
    "**Q. How to join on two more columns**\n",
    "****\n",
    "**A.**                         \n",
    "<code>df.join(df1, \n",
    "        (df[\"id\"]==df2[\"emp_id\"]) & (df2[\"emp_id\"]==df3[\"employee_id\"]),\n",
    "        \"inner\"\n",
    "        ).select(\"*\")show()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609621b-0f94-4c66-bef0-8776178fd3e0",
   "metadata": {},
   "source": [
    "**Q. How to join multiple tables**\n",
    "***\n",
    "**A.**                        \n",
    "<code>df.join(df1, \n",
    "        (df[\"id\"]==df2[\"emp_id\"]) & (df2[\"emp_id\"]==df3[\"employee_id\"]),\n",
    "        \"inner\"\n",
    "        ).select(\"*\")show()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7985667-93a2-472e-9638-d7d3846b4dd5",
   "metadata": {},
   "source": [
    "**Q. How many types of join**\n",
    "****\n",
    "**A.** \n",
    "1. **Inner Join**: An inner join returns rows from both dataframes that have matching keys.                           \n",
    "   <code>result = df1.join(df2, on=\"id\", how=\"inner\")</code> \n",
    "2. **Outer (Full) Join**: An outer join, also known as a full join, returns all rows from both dataframes. If a key is present in one dataframe but not in the other, the missing values are filled with nulls.                \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"outer\")</code> \n",
    "3. **Left Join**: A left join returns all rows from the left dataframe and the matched rows from the right dataframe. If no match is found for a key in the right dataframe, the result will contain null values.                          \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"left\")</code>                    \n",
    "4. **Right Join**: A right join returns all rows from the right dataframe and the matched rows from the left dataframe. If no match is found for a key in the left dataframe, the result will contain null values.                       \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"right\")</code>                \n",
    "5. **Left Semi Join**: A left semi join returns only the columns from the left dataframe for the rows with matching keys in both dataframes. It is similar to an inner join but only returns the columns from the left dataframe.              \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"left_semi\")</code>                 \n",
    "6. **Left Anti Join**: A left anti join returns the rows from the left dataframe that do not have matching keys in the right dataframe. It is the opposite of a left semi join.                         \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"left_anti\")</code>              \n",
    "7. **Cross Join**: A cross join, also known as a cartesian join, returns the cartesian product of both dataframes. It combines each row from the left dataframe with each row from the right dataframe.                     \n",
    "<code>result = df1.crossJoin(df2)</code> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadfb1e-7fff-44a6-8b4f-98cef2e001d6",
   "metadata": {},
   "source": [
    "**Q. Define left_semi and left_anti join**\n",
    "***\n",
    "**A.**                           \n",
    "**left_semi**: All records form left table(A), that common in both table(A,B), join(inner-left)                 \n",
    "**left_anti**: All records form left table(A), that not common in table(B), join(full-right-inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f88b6-16bb-4180-9c50-b4fbd61932c9",
   "metadata": {},
   "source": [
    "**Q. What is the window function**\n",
    "***\n",
    "**A.** Window function creates a window of rows, where we can perform our transformation and action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714ab55-59de-4795-81ab-3f5aa8b9b986",
   "metadata": {},
   "source": [
    "**Q. How to create a window**\n",
    "***\n",
    "**A.**  ` window = Window.partitionBy(col(\"col_name\")).orderBy(col(\"col_name\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5633eb2-3963-4b54-b6b4-dbb19a38389c",
   "metadata": {},
   "source": [
    "**Q. What is the window function**\n",
    "****\n",
    "**A.** Window function used to create rows of the same category into groups, allowing for calculation or aggregation to be applied within these groups \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/imrannazer-/\">\n",
    "<img src=\"https://i.ibb.co/7jPTLzV/035.png\" style=\"width:100%;\" alt=\"035\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e21708-89d0-4c77-82a9-7013ab308c63",
   "metadata": {},
   "source": [
    "**Q. What is the row number rank and dense_rank in PySpark**\n",
    "***\n",
    "**A.**            \n",
    " \n",
    "**row_number():** Returns a sequential number starting from 1 within a window partition         \n",
    "**rank()** Returns the rank of rows within a window partition, with gaps.         \n",
    "**dense_rank():** Returns the rank of rows within a window partition without any gaps. Where as Rank() returns rank with gaps.\n",
    "\n",
    "```python\n",
    "emp_df.withColumn(\"row_num\", row_number().over(window))\\\n",
    "    .withColumn(\"rank_num\", rank().over(window))\\\n",
    "    .withColumn(\"dense_rank_num\", dense_rank().over(window))\\\n",
    "    .show()\n",
    "\n",
    "\n",
    "| emp_id | emp_name | salary | department | gender | row_num | rank_num | dense_rank_num |\n",
    "|--------|----------|--------|------------|--------|---------|----------|----------------|\n",
    "|   1    | imran    | 50000  | IT         | m      |   1     |   1      |        1       |\n",
    "|  11    | Khan     | 50000  | IT         | f      |   2     |   1      |        1       |\n",
    "|   9    | Glaxy    | 65000  | IT         | m      |   3     |   3      |        2       |\n",
    "|   4    | Nazer    | 80000  | IT         | m      |   4     |   4      |        3       |\n",
    "|   8    | roshan   |100000  | IT         | f      |   5     |   5      |        4       |\n",
    "|   6    | Hasib    | 45000  | marketing  | f      |   1     |   1      |        1       |\n",
    "|  10    | abraham  | 50000  | marketing  | m      |   2     |   2      |        2       |\n",
    "|   7    | Baidan   | 55000  | marketing  | f      |   3     |   3      |        3       |\n",
    "|   3    | Nitin    | 70000  | marketing  | m      |   4     |   4      |        4       |\n",
    "|   2    | Mufti    | 60000  | sales      | m      |   1     |   1      |        1       |\n",
    "|   5    | Hasina   | 90000  | sales      | f      |   2     |   2      |        2       |\n",
    "|  12    | Rajab    | 90000  | sales      | m      |   3     |   2      |        2       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9b238-de90-42bf-a056-ef55861883e6",
   "metadata": {},
   "source": [
    "**Q. How to use windows function**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<code>from pyspark.sql.window import Window\n",
    "window = Window.partitionBy(col(\"department\")).orderBy(col(\"salary\"))\n",
    "emp_df.withColumn(\"row_num\", row_number().over(window)).show()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d2e3b-cedc-4641-938d-3979bc090566",
   "metadata": {},
   "source": [
    "**Q. How to calculate the top two salary holders from each department**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "<code>window = Window.partitionBy(col(\"department\"),col(\"gender\")).orderBy(desc(col(\"salary\")))\n",
    "emp_df.withColumn(\"row_num\", row_number().over(window)).filter(col(\"row_num\")<3)\\\n",
    "    .show()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3b2e6-d80d-48db-b4d8-8dadff98a016",
   "metadata": {},
   "source": [
    "**Q. What is LEAD and LAG in PySpark**\n",
    "****\n",
    "**A.** \n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div style=\"float: left; width: 70%; padding: 20px; box-sizing: border-box;\">\n",
    "    \n",
    "1. **lag(** columnName: String, offset: Int, defaultValue: Any **)** returns the value that is `offset` rows before the current row, and `null` if there is less than `offset` rows before the current row.                                      \n",
    "   ` window = Window.partitionBy(col(\"Temperature\")).orderBy(col(\"Rain\"))`         \n",
    "   ` new_df = df.withColumn(\"lead_new\", lag(col(\"Temperature\"),1).over(window))`                            \n",
    "   ` new_df.show()`                                                             \n",
    "\n",
    "3. **lead(** columnName: String, offset: Int, defaultValue: Any **)** :returns the value that is `offset` rows after the current row, and `null` if there is less than `offset` rows after the current row.                                 \n",
    "   ` window = Window.partitionBy(col(\"Temperature\")).orderBy(col(\"Rain\"))`         \n",
    "   ` new_df = df.withColumn(\"lead_new\", lead(col(\"Temperature\"),1).over(window))`                            \n",
    "   ` new_df.show()`                                                             \n",
    "\n",
    "    </div>\n",
    "    <div style=\"float: right; width: 30%; padding: 20px; box-sizing: border-box;\">\n",
    "        <img src=\"https://i.ibb.co/W5Yq1DP/048.png\" style=\"width:250px;height:240px;\" align=\"right\"/>\n",
    "    </div>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4354459-145d-44ff-a168-a26dfe962149",
   "metadata": {},
   "source": [
    "**Q. Where required partitionBy and orderBy in window function and where not required both**\n",
    "***\n",
    "**A.** \n",
    "1. Ranking (partition by + order by)\n",
    "    - row_number,rank,dense_rank,percent_rank,ntile\n",
    "2. Analytical (partition by + order by)\n",
    "    - cume_dist,lag,lead\n",
    "3. Aggregation (Partition by is compulsory)(order by is not required)\n",
    "    - min,max,avg,sum\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7db20833-d4df-4e8d-9b6d-ddd8e413f3d9",
   "metadata": {},
   "source": [
    "1. Ranking Functions (partitionBy + ordeBy)\n",
    "| Function        | Description                                 | PartitionBy | OrderBy | Example                                                |\n",
    "|-----------------|---------------------------------------------|-------------|---------|--------------------------------------------------------|\n",
    "| `row_number()`  | Assigns a unique number to each row within a partition based on the specified order | Yes         | Yes     | `df.withColumn(\"row_num\", F.row_number().over(Window.partitionBy(\"col1\").orderBy(\"col2\")))` |\n",
    "| `rank()`         | Assigns a unique rank to each distinct row within a partition based on the specified order, with ties receiving the same rank | Yes         | Yes     | `df.withColumn(\"rank_col\", F.rank().over(Window.partitionBy(\"col1\").orderBy(\"col2\")))`        |\n",
    "| `dense_rank()`   | Similar to `rank()` but without gaps between ranks for tied values | Yes         | Yes     | `df.withColumn(\"dense_rank_col\", F.dense_rank().over(Window.partitionBy(\"col1\").orderBy(\"col2\")))` |\n",
    "| `percent_rank()` | Calculates the relative rank of each distinct row within a partition | Yes         | Yes     | `df.withColumn(\"percent_rank_col\", F.percent_rank().over(Window.partitionBy(\"col1\").orderBy(\"col2\")))` |\n",
    "| `ntile()`        | Divides the result set into a specified number of buckets and assigns a bucket number to each row | Yes         | Yes     | `df.withColumn(\"ntile_col\", F.ntile(4).over(Window.partitionBy(\"col1\").orderBy(\"col2\")))`           |\n",
    "\n",
    "2. Analytical Functions (partitionBy + orderBy) \n",
    "| Function        | Description                                 | PartitionBy | OrderBy | Example                                                |\n",
    "|-----------------|---------------------------------------------|-------------|---------|--------------------------------------------------------|\n",
    "| `cume_dist()`    | Calculates the cumulative distribution of a value within a partition | Yes         | Yes     | `df.withColumn(\"cume_dist_col\", F.cume_dist().over(Window.partitionBy(\"col1\").orderBy(\"col2\")))`    |\n",
    "| `lag()`          | Accesses data from a previous row within the partition based on the specified order | Yes         | Yes     | `df.withColumn(\"lag_col\", F.lag(\"col1\").over(Window.partitionBy(\"col2\").orderBy(\"col3\")))`           |\n",
    "| `lead()`         | Accesses data from a subsequent row within the partition based on the specified order | Yes         | Yes     | `df.withColumn(\"lead_col\", F.lead(\"col1\").over(Window.partitionBy(\"col2\").orderBy(\"col3\")))`         |\n",
    "\n",
    "3.Aggregation Functions (Partition by is compulsory)(order by is not required)\n",
    "| Function        | Description                                 | PartitionBy | OrderBy | Example                                                |\n",
    "|-----------------|---------------------------------------------|-------------|---------|--------------------------------------------------------|\n",
    "| `min()`          | Calculates the minimum value within a partition | Yes         | No      | `df.withColumn(\"min_col\", F.min(\"col1\").over(Window.partitionBy(\"col2\")))`                           |\n",
    "| `max()`          | Calculates the maximum value within a partition | Yes         | No      | `df.withColumn(\"max_col\", F.max(\"col1\").over(Window.partitionBy(\"col2\")))`                           |\n",
    "| `avg()`          | Calculates the average value within a partition | Yes         | No      | `df.withColumn(\"avg_col\", F.avg(\"col1\").over(Window.partitionBy(\"col2\")))`                           |\n",
    "| `sum()`          | Calculates the sum of values within a partition | Yes         | No      | `df.withColumn(\"sum_col\", F.sum(\"col1\").over(Window.partitionBy(\"col2\")))`                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e34af-ef7b-4478-8061-bd66134a6595",
   "metadata": {},
   "source": [
    "**Q. what is the percentage of loss or gain based on previous month sales**\n",
    "***\n",
    "**A.**\n",
    "```python\n",
    "prev_month_sales = prd_df.withColumn(\"prev_month_sales\", lead(col(\"p_sales\"),1).over(window))\\\n",
    "    .withColumn(\"profit\", col(\"p_sales\")-col(\"prev_month_sales\"))\\\n",
    "    .withColumn(\"p_or_l\", round((col(\"profit\")/col(\"p_sales\"))*100, 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817dc65-2c04-4bc2-84d6-57d5d277fdf9",
   "metadata": {},
   "source": [
    "**Q. What is the percentage of sales each month based on last 6 month saless**\n",
    "***\n",
    "**A.**\n",
    "```python\n",
    "window = Window.partitionBy(\"p_id\")\r\n",
    "last_six_month_df = prd_df.withColumn(\"previous_six_month_total_sales\", sum(\"p_sales\").over(window))\\\r\n",
    "    .withColumn(\"perc_sales_each_month\", round((col(\"p_sales\")/col(\"previous_six_month_total_sales\")) * 100, 2) )\r\n",
    "last_six_month_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5406a-4c62-4510-9f18-5ee65413d4a2",
   "metadata": {},
   "source": [
    "**Q. What is the Window.unboundedPreceding and Window.unboundedFollowing**\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div style=\"float: left; width: 70%;padding: 20px; box-sizing: border-box;\">\n",
    "        \n",
    "        A.  The purpose of the rows clause is to specify the window frame in relation to the current row\n",
    "</div>\n",
    "    <div style=\"float: right; width: 30%; padding: 20px; box-sizing: border-box;\">\n",
    "        <img src=\"https://i.ibb.co/fdL9tdL/049.png\" />\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9653b1-c45e-4078-888d-cc38fb0a0493",
   "metadata": {},
   "source": [
    "**Q. Find out the difference in sales of each product from their first month sells to latest sales**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<code>window = Window.partitionBy(col(\"p_id\")).orderBy(col(\"p_date\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "prd_df.withColumn(\"first_sale\",first(col(\"p_sales\")).over(window))\\\n",
    "    .withColumn(\"last_sale\", last(col(\"p_sales\")).over(window))\\\n",
    "    .withColumn(\"difference\", col(\"first_sale\")-col(\"last_sale\")).show() </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9fba6c-a23a-4076-ba66-76c6a5414e23",
   "metadata": {},
   "source": [
    "**Q. Send a mail who has not completed duty time(8 hours)**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<code>new_df= emp_df.withColumn(\"login\",first(\"timestamp\") .over (window) ) \n",
    "            .withColumn(\"logout\", last(\"timestamp\") .over (window) ) \n",
    "            .withColumn(\"login\", to_timastamp(\"login\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"logout\", to_timestamp(\"logout\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"total_time\",col(\"logout\")-col(\"login\") ) . show() </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e78c26-6854-462b-a57e-2a2f2352cdbf",
   "metadata": {},
   "source": [
    "**Q. Analyze launched three month sales**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<code>window = Window.partitionBy(col(\"p_name\")).orderBy(col(\"p_date\"))\n",
    "prd_df.withColumn(\"row_number\", row_number().over(window))\n",
    "    .filter(col(\"row_number\")>=3)\n",
    "    .withColumn(\"total_sum\", sum(col(\"p_sales\")).over(window)).show()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ccf57e-1a8d-4c37-88be-91dee433c565",
   "metadata": {},
   "source": [
    "**Q. What is SCD2(slowly changing dimension)**\r\n",
    "****\r\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37beb4cf-5918-4568-8778-c66bf0e7a108",
   "metadata": {},
   "source": [
    "**Q. What is default date format**\n",
    "****\n",
    "**A.** *` YYYY-MM-DD `* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d9507-975e-4516-aec9-f1821a676a8a",
   "metadata": {},
   "source": [
    "**Q.**\r\n",
    "****\r\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c3197-d522-4363-b7b4-f8c62d017813",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "139b975c-cc10-4550-a0b3-7f80bd8a1ac6",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div style=\"float: left; width: 50%; background-color: lightblue; padding: 20px; box-sizing: border-box;\">\n",
    "        <h2>Left Div</h2>\n",
    "        <p>This is the content of the left div.</p>\n",
    "    </div>\n",
    "    <div style=\"float: right; width: 50%; background-color: lightcoral; padding: 20px; box-sizing: border-box;\">\n",
    "        <h2>Right Div</h2>\n",
    "        <p>This is the content of the right div.</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
