{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bff910-f149-4d54-81f2-98fb41a085e2",
   "metadata": {},
   "source": [
    "## Hive Important Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e3012-dc89-416b-89cf-17a11c93159f",
   "metadata": {},
   "source": [
    "- Hive Build by Facebook\n",
    "- Hive just like interface, not a database\n",
    "- Structured data has data in row and column format and also has schema\n",
    "- Hive can handle structured and semi-structured data, not familiar with unstructured data\n",
    "- Hive run time schema validation on read\n",
    "- Hive schema validation is done when we go to read hive_table\n",
    "  - The amount of data to be loaded in Hive table is Huge\n",
    "  - WORM : (Write Once and Read Many)\n",
    "- Hive is not recommended for Row-level updates(insert, update, delete)\n",
    "- Hive by default use \"Derby\" to store metadata\n",
    "- Hive default database location /user/hive/warehouse/default/, it is configurable\n",
    "- Hive user define database location /user/hive/warehouse/dbname/table_name/\n",
    "- Hive can be used as a file type converter \n",
    "- Hive store metadata on metastore(RDBMS/derby) and data on HDFS\n",
    "- Hive preferred file format is ORC(Optimized Row Columnar)\n",
    "- **TRUNCATE** operation will remove data only, nor metadata from internal/managed table\n",
    "- **DROP** operation will remove data and metadata both.\n",
    "- **gparted** used to extend linux hard disk size\n",
    "- TRUNCATE operation\n",
    "  - Supported by internal/managed table\n",
    "  - Not supported by external table\n",
    "- DROP operation\n",
    "  - remove metadata and data from internal/managed table\n",
    "  - remove metadata only from external table, data will be saved\n",
    "- You can delete external table by Using HDFS command ` hdfs dfs -rmr /user/hive/warehouse/7jan.db/tx_ext  `, Convert external table to Internal and truncate it `   `\n",
    "- Multi-level partitioning on multiple columns\n",
    "- Hive partition create directory for all partitions\n",
    "- Hive Design label optimization\n",
    "     - Partitioning\n",
    "       - Max partition size is 200 ` set hive.exec.max.dynamic.partitions=500 `\n",
    "       - In partitioning NULL value store in *` column = _HIVE_DEFAULT_PARTITION_ `*\n",
    "       - Partitioning for query optimization and data management\n",
    "       - Static Partitioning\n",
    "         - Define partition column name with value\n",
    "       - Dynamic Partitioning\n",
    "         - Define partition column name only\n",
    "     - Bucketing\n",
    "       - Bucketing on multiple column consider a single string(all columns)\n",
    "       - Bucketing for data sampling and join optimization\n",
    "       - Hive bucket devide data based on hash() function\n",
    "         - `f(x) = x % number of buckets`: for int data type\n",
    "         - `f(x) = 'column'.hashCode() => f(x) = x % number of buckets `: for string data type\n",
    "- Hive Storage Level optimization\n",
    "  - File formats\n",
    "    - Text-based (read normal text reader)\n",
    "    - Binary (couldn't read normal text reader)\n",
    "  - Compression\n",
    "    - Cold data: Not frequently used data : *GZ*\n",
    "    - Hot data: frequently used data : *Snappy*\n",
    "\n",
    "- Hive JOIN optimizaiton\n",
    "  -  Common join\n",
    "  - Map side join\n",
    "  - Bucket map join\n",
    "  - Sort merge bucket join (SMB join)\n",
    "  - Sort merge bucket Map join (SMBM join)\n",
    "- Vectorization allows Hive to process a batch of rows together instead of processing one row at a time(1024 rows)\n",
    "  - Vectorization allows ORC file format\n",
    "  - Vectorization not allows on complex data type\n",
    "  - data casting not supported\n",
    "- The **CBO** optimizes and calculates the cost of various plans for a query and selects the cheapest plane.\n",
    "- The **RBO** optimizes to reuse resources and RAM.\n",
    "- Logical plan can be visualized by using ` explain ` keyword before every hive commands\n",
    "- Create VIEW in Hive ` create view tx_view as select cname, sum(revenue) from tx_orc group by cname ; `\n",
    "- **VIEW** stores Hive query, that is used to create this VIEW table\n",
    "- **Materialized Views** stores actual data, that is output by Hive query\n",
    "- - Materialized Views table should be internal and transaction\n",
    "  - Materialized Views table file format should by ORC\n",
    "- **VIEW** stores Hive query but **Materialized Views** stores Hive query output\n",
    "- Create Materialized Views ` create materialized view <name> as query ` , table can be update, drop, etc.\n",
    "\n",
    "\n",
    "\n",
    "- Data move from HDFS to Hive table, when we run LOAD command\n",
    "- Data copy from Local to Hive, when we run LOAD LOCAL command\n",
    "- Serializer is used to convert data in row format to row() object\n",
    "- deserializer is used to convert data in row() object to row format\n",
    "- **SODI** Serializer in output/read and deserialize in input/write\n",
    "\n",
    "- UDF (User define function) Can use in Hive, Function performed on row-based like upper-case, lower-case, etc (one to one)\n",
    "- - ` add jar /home/jar_dir/jar_file.jar  `\n",
    "  - ` CREATE TEMPRORY FUNCTION fun_name AS 'jar_function_name' ;  `\n",
    "  - ` SELECT fun_name(col_name), col2 FROM hivedb.hive_table;  `\n",
    "- UDAF (User define aggregate function) Function performed on column based like max(), min(), sum(), etc (many to one)\n",
    "- UDTF User Define Table Generating Function, Function performed pruning on table-based (one to many)\n",
    "- **Window functions**\n",
    "  - Ranking (partition by + order by)\n",
    "    - Row_number, rank, dense_rank, percent_rank, ntile\n",
    "  - Analytical (partition by + order by)\n",
    "    - Cume_dist, lag, lead\n",
    "  - Aggregation (Partition by is compulsory)(order by is not required)\n",
    "    - Min, max, avg, sum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "263ab5c7-05a4-42dd-81f3-32ff18b3845e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57701177-9245-4e50-9345-39b7b5107c8d",
   "metadata": {},
   "source": [
    "ppy\");\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212e654-6647-46d0-af03-afd2e292d61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
