{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff7fe18-4d14-4384-8fdc-9dc535cd8a6a",
   "metadata": {},
   "source": [
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7b37a-f329-4c36-afda-f7c551beb204",
   "metadata": {},
   "source": [
    "**Q. How to explain PYspark code**\n",
    "****\n",
    "**A.** explain( ) function, End of the code ` df.select(\"*\").filter(col(\"age\")>20)  `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8aa71-cb0b-4a13-8792-00a2fa4de64d",
   "metadata": {},
   "source": [
    "**Q. How to create Schema in PySpark**\n",
    "****\n",
    "**A.** There is three way to create\n",
    "- String\n",
    "  - `schema = \"id int, name string, salary float, date_of_joining date\"`\n",
    "- structType([structField(col(\"id\"), integerType(), null=True)])                  \n",
    "  - `structType([`                                          \n",
    "   `    structField(col(\"id\"), IntegerType(), null=True)`                         \n",
    "    `    structField(col(\"name\"), StringType(), null= True)`                   \n",
    "    `    structField(col(\"salary\"), FloatType(), null= True)`                \n",
    "    `    structField(col(\"id\"), StringType(), null= True)`             \n",
    "   `    structField(col(\"date_of_joining\"), DateType(), null= True)])`             \n",
    "- structType().add()\n",
    "  - `structType().add(\"name\", StringType(), nullable = True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c89643-05cc-4a14-ab3a-d3e721f425dc",
   "metadata": {},
   "source": [
    "**Q. What is the structType and structField in schema**\r\n",
    "****\r\n",
    "**A.**`structType()`: Define structure of Dataframe          \n",
    "`fieldType()`: Define metadata of the Dataframe columns\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718f074-dfcb-4a5c-a949-c170b091fce1",
   "metadata": {},
   "source": [
    "**Q. What if I have a header in my DataFrame**\n",
    "****\n",
    "**A.** Use `option(\"header\", True)` or skip this header `option(\"skipRows\", 4)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bff556-c65e-4ee0-884a-ec6bd2f1ee59",
   "metadata": {},
   "source": [
    "**Q. How to create schema for struct and array**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "**Create schema for Struct**\n",
    "\n",
    "`schema_with_struct = StructType([`                   \n",
    "`    StructField(\"id\", StringType(), True),`             \n",
    "`    StructField(\"details\", StructType([`                     \n",
    "`        StructField(\"name\", StringType(), True),`                \n",
    "`        StructField(\"age\", StringType(), True)`                   \n",
    "`    ]), True),`                                         \n",
    "`    StructField(\"score\", StringType(), True)`                             \n",
    "`])`                                          \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Create schema for Array**\n",
    "\n",
    "`schema_with_array = StructType([`                 \n",
    "`    StructField(\"id\", StringType(), True),`                 \n",
    "`    StructField(\"names\", ArrayType(StringType()), True),`                        \n",
    "`    StructField(\"score\", StringType(), True)`                     \n",
    "`])`                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb29ee-93a8-449d-a9bd-23ad8a470249",
   "metadata": {},
   "source": [
    "**Q. How to create data-frame in pySpark**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "- Career data-frame by using a file                          \n",
    "` df = spark.read\\`                   \n",
    "`    .option('header', True)`                \n",
    "`    .option('inferschema', True)\\`                 \n",
    "`    .format('csv')\\`                        \n",
    "`    .load(r'\\C:\\data-path\\csv_data.csv')`\n",
    "\n",
    "- Career data frame by using verbal                    \n",
    "` df = spark.createDataFrame(data = data, schema = my_schema)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0ffe0-4133-49e5-a7b0-89de3b77fdd9",
   "metadata": {},
   "source": [
    "**Q. How to create Spark SQL table by using pySpark data-frame**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df.createOrReplaceTempView('sql_tab')  ` # Convert PySpark data-frame into MySQL table               \n",
    "` spark.sql(\"\"\"\r\n",
    "select * from sql_tab\r\n",
    "where DEST_COUNTRY_NAME = 'United States'\r\n",
    "limit 3\r\n",
    "\"\"\").show()  `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32afb8-6cce-4a85-943d-1bba4fbffedd",
   "metadata": {},
   "source": [
    "**Q. What is the difference between createOrReplaceTempView( ) and createOrReplaceGlobalTempView( )**\n",
    "***\n",
    "**A.** View Constructs a virtual table that has no physical data.              \n",
    "` CreateOrReplaceTempView `: It is session based. It is saved in defualt database                \n",
    "      <code> df.CreateOrReplaceTempView(\"sql_table\")                      \n",
    "      ` %sql `                                                          \n",
    "      ` SELECT * FROM sql_table `</code>                                 \n",
    "` CreateOrReplaceGlobalTempView `:It is not session based. It is saved in global_temp database            \n",
    "      <code> df.CreateOrReplaceGlobalTempView(\"sql_table_global\")                      \n",
    "      ` %sql `                                                          \n",
    "      ` SELECT * FROM global_temp.sql_table_global `</code>                                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbcc4da-924b-45cd-a57f-a38b1b3f74f1",
   "metadata": {},
   "source": [
    "**Q. Have you worked with corrupted records**\n",
    "****\n",
    "**A.** Yes! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215122e-6e62-4302-b6cd-f17c0d11cb09",
   "metadata": {},
   "source": [
    "**Q. When do you say that records are corrupted**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "\n",
    "In JSON file              \n",
    "- Missing {               \n",
    "\n",
    "In CSV file             \n",
    "- More or Less value according to columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ca889-b0f2-4392-9d32-fd2604384ad2",
   "metadata": {},
   "source": [
    "**Q. What happens when we encounter corrupted records in different read modes**\r\n",
    "****\r\n",
    "**A.**`option(\"mode\", \"PERMISSIVE\")`: Set null value to all corrupted fields              \n",
    "`option(\"mode\", \"DROPMALFORMED\")`: Drop the corrupted record/row              \n",
    "`option(\"mode\", \"FAILFAST\")`: Fail execution if malformed record in dataset             \n",
    "\n",
    "By default `option(\"mode\", \"PERMISSIVE\")` \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33707391-dbb5-4090-85e0-7b15115fdc88",
   "metadata": {},
   "source": [
    "**Q. How can we print bad records**\n",
    "****\n",
    "**A.** Create a dataframe schema ans this column `StructType([StructField(\"_corrupt_record\", StringType(), nullable = True)])`\r\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f0f04-c6a1-4f0e-aff5-88d0f3225fd6",
   "metadata": {},
   "source": [
    "**Q. List of Spark Data Types**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<table>\r\n",
    "  <tbody>\r\n",
    "    <tr>\r\n",
    "      <td>StringType</td>\r\n",
    "      <td>ShortType</td>\r\n",
    "      <td>ArrayType</td>\r\n",
    "      <td>IntegerType</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>MapType</td>\r\n",
    "      <td>LongType</td>\r\n",
    "      <td>StructType</td>\r\n",
    "      <td>FloatType</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>DateType</td>\r\n",
    "      <td>DoubleType</td>\r\n",
    "      <td>TimestampType</td>\r\n",
    "      <td>DecimalType</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>BooleanType</td>\r\n",
    "      <td>ByteType</td>\r\n",
    "      <td>CalendarIntervalType</td>\r\n",
    "      <td>HiveStringType</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "      <td>BinaryType</td>\r\n",
    "      <td>ObjectType</td>\r\n",
    "      <td>NumericType</td>\r\n",
    "      <td>NullType</td>\r\n",
    "    </tr>\r\n",
    "  </tbody>\r\n",
    "</table>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8ce47-c646-47fc-bb9a-6593ff2e393c",
   "metadata": {},
   "source": [
    "**Q. Where do you store corrupted records and how can we access them later**\n",
    "****\n",
    "**A.** Assign a path to store bad record `option(\"badRecordsPath\",\"/file/store/data/\")`\n",
    "\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43191a2c-500e-465e-9b6e-37d971a216d6",
   "metadata": {},
   "source": [
    "**Q. What is JSON data and how to read it in Apache PySpark**\n",
    "****\n",
    "**A.** JSON standard for **JavaScript Object Notation** is a semi-structured data, store data in key:value pair, use ` format(\"json\") ` \n",
    "\n",
    "- JSON is a semi-structured data type\n",
    "- JSON is a key value pair data format file\n",
    "- Every record enclosed in Curly braces\n",
    "- Struck Fallatan in columns by using *.* ` df.select(col(\"Address.*\")).show() `\n",
    "- Array a Fallatan in rows/records by using ` df.select(explode(col(\"company_name\")).alias(\"new_col_name\")).show() `\n",
    "                                 \n",
    "`js_df = spark.read.option(\"header\", True)\\`                     \n",
    "`    .option(\"multiline\", True)\\`                           \n",
    "`    .option(\"inferschema\", True)\\`                      \n",
    "`    .format(\"json\")\\`                        \n",
    "`    .load(\"/FileStore/tables/data/resturant_json_data.json\")`                  \n",
    "`js_df.show()`                                                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfaf9f-32c0-4146-968c-bd793d1b88b4",
   "metadata": {},
   "source": [
    "**Q. What if I have 3 keys in all lines and 1 key in one line in the JSON file**\n",
    "****\n",
    "**A.** Create 4 columns in dataframe and assign 4<sup>th</sup> column null if value is not persent \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb39868d-85c8-4fbd-8ea4-90e2fda124e8",
   "metadata": {},
   "source": [
    "**Q. What is multi-line and line-delimited JSON**\n",
    "****\n",
    "**A.**                        \n",
    "**1. Multi-line** : Where JSON single record in more than one line                \n",
    "`          {`                     \n",
    "`            \"name\":\"Nazer\",`                  \n",
    "`            \"email\":\"naziri1920@gmail.com\"`              \n",
    "`            \"mobile\": 5847896542`                    \n",
    "`          }`                   \n",
    "\n",
    "**2. line-delimited**: Single line JSON                  \n",
    "`          {\"name\":\"Nazer\",\"email\":\"naziri1920@gmail.com\",\"mobile\":123456790}`\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb7da15-88f2-433d-af82-4a6d08241dfb",
   "metadata": {},
   "source": [
    "**Q. Which one works faster multi-Line or Line-delimited in JSON in file format**\n",
    "****\n",
    "**A.** line-delimited work fister bucause by derault spark consider JSON line-delimited\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb448d78-ef39-4580-bf9a-bea5e103c4ca",
   "metadata": {},
   "source": [
    "**Q. How to read nested JSON into PySpark DataFrame**\n",
    "****\n",
    "**A.** Use ` option(\"multiline\", True) ` and ` format(\"json\") `\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be005bb-210b-499f-ad59-227e1192b799",
   "metadata": {},
   "source": [
    "**Q. What will happen if I have a corrupted JSON record and corrupted file**\n",
    "****\n",
    "**A.**  In case of corrupted record, this record is saved in _curropt_record column. In case of corrupted file return error. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927f250-4577-44a4-8bf4-0022d6e58634",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20669c01-6287-437a-bd49-6f6554c7aeef",
   "metadata": {},
   "source": [
    "**Q. How to flatten Array and Struct in data frame JSON file format**\n",
    "***\n",
    "**A.** by using ` explode( ) ` and **.** function\n",
    "\n",
    "`js_df.select(explode(\"restaurants\").alias(\"new_restaurant\"))\\`                         \n",
    "`    .select(col(\"new_restaurant.*\"))\\`                                    \n",
    "`    .select(col(\"restaurant.*\"))\\`                         \n",
    "`    .select(`                         \n",
    "`            col(\"R.*\"),` # flatten all Struct                    \n",
    "`            col(\"location.*\"),`                       \n",
    "`            explode(\"offers\").alias(\"new_offer\"),`  # Flatten Array                          \n",
    "`            col(\"user_rating.*\")`                      \n",
    "`            ).printSchema()`                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa38f1d-fefb-4b5b-8734-425e124b34c3",
   "metadata": {},
   "source": [
    "**Q. How to flatten a struct in dataframe JSON file format**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "`flattened_df = df.select(`                               \n",
    "`    col(\"id\"),`                                  \n",
    "`    col(\"details.name\").alias(\"details_name\"),`                   \n",
    "`    col(\"details.age\").alias(\"details_age\"),`                    \n",
    "`    col(\"details.address.city\").alias(\"details_address_city\"),`                   \n",
    "`    col(\"details.address.zip\").alias(\"details_address_zip\"),`                             \n",
    "`    explode(\"details.hobbies\").alias(\"hobby\"),`                                                            \n",
    "`    col(\"salary\")`                                    \n",
    "`)`                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07602c-4669-4ee4-8f80-4a8ce4867d0f",
   "metadata": {},
   "source": [
    "**Q. What is Parquet as a file format**\n",
    "****\n",
    "**A.** Parquet is a default file format in Pyspark and this columner file format. There is not required any format to define during file read \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d2e28-7696-4d7a-85a0-da7023cb038d",
   "metadata": {},
   "source": [
    "**Q. Why do we need Parquet**\n",
    "****\n",
    "**A.** Parquet is a columnar file format, and columnar file format is easy to read and process in case of big-data, low storage required, saved in hybrid form(data divided into column and rows), \n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604be9c1-5f5f-4a8d-8140-66f0c1bcab4b",
   "metadata": {},
   "source": [
    "**Q. Where should we use columnar file format or row file format**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "| Concept                  | OLAP (Online Analytical Processing)                       | OLTP (Online Transaction Processing)                           |\r\n",
    "|--------------------------|---------------------------------------------------------|-------------------------------------------------------------|\r\n",
    "| **Use Case**             | Designed for complex analytical and ad-hoc querying.     | Designed for fast and reliable handling of individual transactions. |\r\n",
    "| **Characteristics**      | Supports complex queries, aggregations, reporting.     | Optimized for fast, real-time transactional operations.       |\r\n",
    "| **Example in PySpark**   | Performing complex aggregations using DataFrame API. | Basic CRUD operations on a DataFrame, dealing with individual records. |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d86450-f073-4040-9471-c721a6e29626",
   "metadata": {},
   "source": [
    "**Q. How to read a Parquet file**\n",
    "****\n",
    "**A.** \n",
    "`spark.read.option(\"header\", True).load(\"file path\")` There is not necessary to provide format\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52035b74-c5a4-4665-af21-6d7c9c68d710",
   "metadata": {},
   "source": [
    "**Q. How to read parquet file in windows CMD**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "Install these libraries                                        \n",
    "> pip install pyarrow                        \n",
    "> pip install parquet-tools\n",
    "\n",
    "Open python terminal and run this code\n",
    "> parquet_file = pq.ParquetFile(r'D:\\Big-Data-2023\\git_repo\\Data-Engineer-Interview-Notes\\git_ignore\\data\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
    "> parquet_file.metadata                                   \n",
    "> parquet_file.metadata.row_group(0)                        \n",
    "> parquet_file.metadata.row_group(0).column(0)                          \n",
    "> parquet_file.metadata.row_group(0).column(0).statistics\n",
    "\n",
    "Run the below command in cmd/terminal\n",
    ">parquet-tools show  D:\\Big-Data-2023\\git_repo\\Data-Engineer-Interview-Notes\\git_ignore\\data\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet                          \n",
    ">parquet-tools inspect  D:\\Big-Data-2023\\git_repo\\Data-Engineer-Interview-Notes\\git_ignore\\data\\part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8efa1-0670-46a3-9293-6eb4a0557429",
   "metadata": {},
   "source": [
    "**Q. How to data organize in parquet**\n",
    "***\n",
    "**A.**\n",
    "Date organizaton in parquet\n",
    "- File \n",
    "  - Row Group (we have metadate(min, max, count, etc) at group level also)\n",
    "    - Column\n",
    "      - Pages\n",
    "        - Metadata\n",
    "          - Min\n",
    "          - Max\n",
    "          - Count\n",
    "         \n",
    "<img src=\"images/036.png\" alt=\"Date organizaton in parquet\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d1124-24df-4f9c-8d02-33845ad3a9ff",
   "metadata": {},
   "source": [
    "**Q. What makes Parquet the default choice**\n",
    "***\n",
    "**A.** Parquet follows RLE (Run Length Encoding) Technique.\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea596f2-0849-4ece-92b9-472c784f2f46",
   "metadata": {},
   "source": [
    "**Q. What encoding is done on Parquet data**\r\n",
    "****\r\n",
    "**A.** \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8f6b2-5748-4205-9048-786407f59d1f",
   "metadata": {},
   "source": [
    "**Q. What comparison technique is used in the Parquet file format**\n",
    "****\n",
    "**A.** ` gzip ` comparison technique\n",
    "` df.write.parquet(\"/path/to/your/output\", compression=\"gzip\") ` \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfab5a4-8dc9-4098-adc5-4d502f913266",
   "metadata": {},
   "source": [
    "**Q. How to optimize the Parquet file**\n",
    "****\n",
    "**A.** By using three technique\n",
    "1. comparison technique ` gzip ` or ` Snappy `\n",
    "2. Parquet has matadate on row group lelvel\n",
    "3. Parquet follows RLE (Run Length Encoding) Technique (store repeated consecutive values efficiently by representing them as a base value and the number of consecutive occurrences)\n",
    "   <img src=\"images/039.png\" alt=\"RLE\" width=\"200\" height=\"300\">\n",
    "4. Bit Packing (reducing storage space by optimizing available bits for representing integers.)\n",
    "5. Predicate Pushdown Technique\n",
    "<img src=\"images/037.png\" alt=\"optimization in parquet\" width=\"900\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d570b2-64aa-46cc-bf7c-bb1ddc018e3e",
   "metadata": {},
   "source": [
    "**Q.How to write data frame to disk in spark**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df.write.format('csv').option('header', True)\\  `                 \n",
    "`     .option('path', '/FileStore/tables/csv_write/')\\  `                   \n",
    "`         .save()  `                \n",
    "\n",
    "- File name create by pySpark in databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175e2f6-fe53-40cf-96ed-d86ec5632952",
   "metadata": {},
   "source": [
    "**Q. How to write data in partition**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df.write.format('csv').option('header', True)\\`                             \n",
    "`    .option('mode','overwrite')\\`                                              \n",
    "`    .option('path', '/FileStore/tables/csv_write_repartition__/')\\`                      \n",
    "`    .partitionBy('ORIGIN_COUNTRY_NAME')\\`                        \n",
    "`    .save()`           \n",
    "\n",
    "> Partition on multiple columns\n",
    "` .partitionBy('col1', 'col2', 'col3')\\` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12682716-a824-4a20-8ef3-391517596a51",
   "metadata": {},
   "source": [
    "**Q. How to write data in bucket**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "\n",
    "` df.write.format('csv').option('header', True)\\ `                                 \n",
    "`        .option('mode','overwrite')\\`                                            \n",
    "`        .option('path', '/FileStore/tables/csv_write_repartition_bucket/')\\`                           \n",
    "`        .bucketBy(3,'ORIGIN_COUNTRY_NAME')\\` #3 is number of buncket                           \n",
    "`        .saveAsTable('bucket_name')`               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ad535-1fed-4bbe-adef-7876e42faa4b",
   "metadata": {},
   "source": [
    "**Q. What is the best way to write data in bucket**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "Most of the time when we go to bucket data, then 200 partitions interrupt in this bucketing, so the best way to write in bucket use repartitioning and then bucket data\n",
    "\n",
    "\n",
    "` df.repartition(3)\\`                       \n",
    "`        .write.format('csv')\\`            \n",
    "`        .option('header', True)\\ `                                 \n",
    "`        .option('mode','overwrite')\\`                                            \n",
    "`        .option('path', '/FileStore/tables/csv_write_repartition_bucket/')\\`                           \n",
    "`        .bucketBy(3,'ORIGIN_COUNTRY_NAME')\\` #3 is number of buncket                           \n",
    "`        .saveAsTable('bucket_name')`     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ee920-ca5a-4901-b27b-f829adc3248d",
   "metadata": {},
   "source": [
    "**Q. What is the write default mode**\n",
    "***\n",
    "**A.** ` mode(\"error\") ` is default write mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea56e8-b83f-4af4-ad5c-ea5490c4f239",
   "metadata": {},
   "source": [
    "**Q. What are the modes available in DataFrame write**\n",
    "****\n",
    "**A.**                                                                    \n",
    "` mode(\"append\") `: Appends the data to the existing data if it exists.                          \n",
    "` mode(\"overwrite\") `: Overwrites the existing data if it exists.                                  \n",
    "` mode(\"ignore\") `: Ignores the operation if the data already exists.                                  \n",
    "` mode(\"error\") `: Raises an error if the data already exists.                                  \n",
    "` mode(\"errorifexist\") `: Raises an error if the data already exists.                                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3fc73-62da-4133-84a0-664701aa7ff2",
   "metadata": {},
   "source": [
    "**Q. How to write data into multiple partitions**\n",
    "****\n",
    "**A.**                             \n",
    "\n",
    "` df.repartition(3).write.format('csv').option('header', True)\\  `                 \n",
    "`     .option('path', '/FileStore/tables/csv_write/')\\  `                   \n",
    "`         .save()  `                 \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06891d-0d4a-46d3-9b39-6e24004750e9",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is schema**\n",
    "****\n",
    "**A.** A schema is a combination of **column-name** and **column-data-type**                  \n",
    "\n",
    "- Print schema in data frame ` df.printSchema() `\n",
    "- Print only columns name ` df.columns  `\n",
    "- print data-frame structType and structField ` df.schema  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf1e99-402d-41e0-906d-f810cf0cd6c6",
   "metadata": {},
   "source": [
    "**Q. What is DataFrame**\n",
    "****\n",
    "**A.** A DataFrame in PySpark is a                    \n",
    "- distributed,                      \n",
    "- immutable, and                    \n",
    "- lazily evaluated data structure                              \n",
    "that represents structured data and enables scalable data processing across a cluster of machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaeecca-1248-4ef8-b1c0-c62bd7d36480",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Q. What is the column**\n",
    "***\n",
    "**A.**  A column represents a named and typed collection of data.                          \n",
    "Columns are expressions, and an expression is a set of transformations on one or more than one value in a record                  \n",
    "` df.select(col('age')+5)  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cc906-e5ee-4d41-8313-08d8f7aa0128",
   "metadata": {},
   "source": [
    "**Q. What is the row**\n",
    "***\n",
    "**A.** Row is an object, define by ` from pyspark.sql import Row  `\n",
    "` row = Row(1, 'Khan', 2563) `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e77ce-c2e1-4564-985f-b3aa9469fc6f",
   "metadata": {},
   "source": [
    "**Q. How many ways to select columns**\n",
    "***\n",
    "\n",
    "**A.** \n",
    "1.  ` df.select('*').show()  ` # select all columns\n",
    "2.  ` df.select(col('age'),col('salary')).show()  `                     \n",
    "3.  ` df.select(\"age\",\"salary\").show()  `                       \n",
    "4.  ` df.select(df[\"age\"]).show() `  # handy option in case of join                       \n",
    "5.  ` df.select(df.ORIGIN_COUNTRY_NAME).show()  `  # handy option in case of join              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d2e88-7fb2-4f78-84cb-a293be7e7689",
   "metadata": {},
   "source": [
    "**Q. What is the expression**\n",
    "***\n",
    "**A.** expr() used for assigning MySQL queries                         \n",
    "` df.select(expr(\"ORIGIN_COUNTRY_NAME AS new_name\")).show()  `               \n",
    "` df.select(expr(\"CONCAT (fname,' ',lname) AS full_name\")).show()  `               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf3211-d9cc-4ea9-b85c-23d24ee30b5a",
   "metadata": {},
   "source": [
    "**Q. What is aliasing**\n",
    "****\n",
    "**A.** alias( ) Function use to change column name\n",
    "` df.select(col(\"ORIGIN_COUNTRY_NAME\").alias(\"new_name\")).show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07a9b1-beec-47eb-8c21-e2d56a8e11e0",
   "metadata": {},
   "source": [
    "**Q. What is the difference between filter and where in Apache PySpark**\n",
    "****\n",
    "**A.** There is no difference, both are used for filtering result                         \n",
    "\n",
    "` df.select(\"*\").filter(col(\"ORIGIN_COUNTRY_NAME\")=='Romania').show()  `\n",
    "` df.select(\"*\").where(col(\"ORIGIN_COUNTRY_NAME\")=='Romania').show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d09489-e080-49d2-bf22-d21176723d61",
   "metadata": {},
   "source": [
    "**Q. What is the literal function**\n",
    "****\n",
    "**A.** Assign a static value in data frame column\n",
    "` df.withColumn(\"lit_col\", lit(\"123\")).show() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f40c31-7f6a-4ce5-be0f-96f927779e79",
   "metadata": {},
   "source": [
    "**Q. How to add a new column in DataFrame**\r\n",
    "****\r\n",
    "**A.*withColumn() used to add a new column or modify an existing column in data-frame*\n",
    "` df.withColumn(\"TOTAL_DEST_COUNTRY\", col(\"count\")+20).show()  `\n",
    "` df.withColumn(\"TOTAL_DEST_COUNTRY\", col(\"count\")+20).show()  `\n",
    " \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c785ef-b294-4581-8393-b063f1e992e4",
   "metadata": {},
   "source": [
    "**Q. How to rename a column in DataFrame**\n",
    "****\n",
    "**A.**  ` df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME_S\").show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cdcc1-b913-4851-ab7c-382df66a6b87",
   "metadata": {},
   "source": [
    "\n",
    "**Q. How to cast data types**\n",
    "****\n",
    "**A.** \n",
    "` df.withColumnRenamed(\"count\", \"count_traivel\").select(col(\"count_traivel\").cast(IntegerType())).printSchema()  ` #New col whth cast()        \n",
    "` df.withColumn('count', col('count').cast(IntegerType())).printSchema() ` # change schema in defined col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b71718-0bca-4b04-975c-3e5aeff54f93",
   "metadata": {},
   "source": [
    "**Q. How to remove a column in DataFrame**\n",
    "****\n",
    "**A.** drop() used to remove an existing column from data-frame                  \n",
    "` df.drop(col(\"DEST_COUNTRY_NAME\")).select(\"*\").show() `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfebf9d-ccee-4bb3-8cd8-d19756df40b0",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is the difference between Union and union all**\n",
    "****\n",
    "**A.** It is used to combine two dataframe vertically, including all rows from both dataframe, even if there are duplicates records.\n",
    "\n",
    "- union() and unionAll() don't remove any duplicate records from dataframe.\n",
    "- union() will remove duplicate records in MySQL table\n",
    "- unionAll() doesn't remove any duplicate records from MySQL table.\n",
    "- Number of columns in both tables must be same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bc1c5-c802-4703-a554-a0ab3e8a11d3",
   "metadata": {},
   "source": [
    "**Q. What will happen if I change the number of columns while Union in the data**\n",
    "****\n",
    "**A.** Return an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2b2e0-7540-4394-9ace-cffa579d5e75",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What if the column name is different**\n",
    "****\n",
    "**A.** Dataframe will be union but fetch header from first table, And column data type may be mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9b2bd-539f-4deb-9686-2cead8aa6d5f",
   "metadata": {},
   "source": [
    "**Q. What is UnionByName**\n",
    "****\n",
    "**A.** UnioByName() tries to find out same column name in both dataframe.\n",
    "\n",
    "- Column order can be mismatched\n",
    "- Column name must be same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13791bd0-964b-4e6e-b348-35b0016f082d",
   "metadata": {},
   "source": [
    "**Q. What is the case when in Spark SQL**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "` df.createOrReplaceTempView(\"sql_table\") `                           \n",
    "` spark.sql( `                                     \n",
    "`    \"\"\" `                    \n",
    "`        select *, `                      \n",
    "`        CASE `                        \n",
    "`            WHEN count>200 and DEST_COUNTRY_NAME = 'United States' THEN 'most_busy' `             \n",
    "`            WHEN count>100 THEN 'busy' `                                            \n",
    "`            ELSE 'okay' `                     \n",
    "`        END AS check_pointing `               \n",
    "`        FROM sql_table `                    \n",
    "`    \"\"\" `                 \n",
    "`).show() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d9af6-44a3-4831-86c6-4e467ffa1587",
   "metadata": {},
   "source": [
    "**Q. What is the when otherwise in Spark**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "`df.withColumn(\"check_point\", when(col(\"count\") > 200, \"most_busy\")\\`                      \n",
    "`    .when(col(\"count\")>100, \"busy\")\\`                    \n",
    "`    .otherwise(\"clear\"))\\`              \n",
    "`    .show()`                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185be32-7530-4dcf-ab40-794fdefd8cb6",
   "metadata": {},
   "source": [
    "**Q. How to deal with Null value in DataFrame**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "`df.withColumn(\"check_point\", when(col(\"count\").isNull(), lit(0))\\` #handel null values                         \n",
    "`    .when(col(\"count\") > 200, \"most_busy\")\\`                       \n",
    "`    .when(col(\"count\")>100, \"busy\")\\`                \n",
    "`    .otherwise(\"clear\"))\\`                  \n",
    "`    .show()`                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26123896-2fa3-49be-92e2-6f36ed932fbf",
   "metadata": {},
   "source": [
    "**Q. How to use case when or when otherwise with multiple AND, OR conditions**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "` df.withColumn('check_pointing', `                             \n",
    "`            when((col(\"count\")>200) & (col(\"DEST_COUNTRY_NAME\")=='United States'),\"Profitable\")\\`                     \n",
    "`            .otherwise(\"okay\")).show()`                                                          \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27c6d2-3420-4b52-b0eb-d33418fcd309",
   "metadata": {},
   "source": [
    "**Q. How to find unique rows**\n",
    "****\n",
    "**A.** distinct( ) Function used to get unique record from data frame                 \n",
    "\n",
    "` df.select(\"*\").distinct().count() ` # get unique rows                              \n",
    "` df.select(col(\"DEST_COUNTRY_NAME\")).distinct().count() ` #get unique country name                          \n",
    "` df.distinct().count() ` #all record based distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a404635-97d0-483d-8e03-fbad77ada045",
   "metadata": {},
   "source": [
    "**Q. How to drop duplicate rows**\n",
    "****\n",
    "**A.** *dropDuplicates( )* and *drop_duplicates( )*\n",
    "\n",
    "\n",
    "` df.dropDuplicates([\"DEST_COUNTRY_NAME\"]).count() ` #drop records on *DEST_COUNTRY_NAME* based output(125)                  \n",
    "`df.dropDuplicates([\"DEST_COUNTRY_NAME\", \"count\"]).count()` #drop records on *DEST_COUNTRY_NAME* and *COUNT*  based output(213)   \n",
    "` df.drop_duplicates([\"DEST_COUNTRY_NAME\", \"count\"]).count() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd2eb84-ed97-4dc1-8197-a46b3615118d",
   "metadata": {},
   "source": [
    "**Q. How to sort the data in ascending and descending order**\r\n",
    "****\r\n",
    "**A.**\n",
    "` df.sort(col(\"count\")).show()` # Dataframe sorting on single column                    \n",
    "` df.sort(col(\"count\"), col(\"ORIGIN_COUNTRY_NAME\")).show() ` # Dataframe sorting on multiple columns             \n",
    "` df.sort(col(\"count\").desc()).show() ` # Dataframe sorting on single column in descending order      \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae64b6d-44ad-4f94-9515-36fc3a0a82b6",
   "metadata": {},
   "source": [
    "**Q. What is the aggregation in PySpark**\n",
    "****\n",
    "**A.** Collecting something together \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509adf9-eff5-4771-a841-81b36b57b6b5",
   "metadata": {},
   "source": [
    "**Q. How to use aggregation in PySpark**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "` df.groupBy(col(\"ORIGIN_COUNTRY_NAME\")).agg(round(avg(col(\"count\"))).alias(\"agg_col\")).show() ` # AVG                 \n",
    "` df.agg(count(col(\"count\")).alias(\"Count_col\")).show() ` # number of records                         \n",
    "` df.agg(min(col(\"count\")).alias(\"min_fun\")).show() ` # min                          \n",
    "` df.agg(max(col(\"count\")).alias(\"max_fun\")).show() ` # max                       \n",
    "` df.agg(sum(col(\"count\")).alias(\"sum_fun\")).show() ` #add all values                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd7cd8-33a7-4354-83ca-f852afac0bd7",
   "metadata": {},
   "source": [
    "**Q, Explain count( ) function**\n",
    "***\n",
    "**A.** It is sometime action(when return a result or calls a job) and sometime transformation(when does not call a job)\n",
    "\n",
    "- count( ) skips null value, If I use single column\n",
    "- count( ) skips null value, If all column has null values on same position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b88a2-454d-4c51-bfd3-b6e32ad47e55",
   "metadata": {},
   "source": [
    "**Q. How groupBy works**\r\n",
    "****\r\n",
    "**A.** \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccc09b-03b7-4239-9a9d-7589b49130e8",
   "metadata": {},
   "source": [
    "**Q. How to implement groupBy in PySpark**\r\n",
    "****\r\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf7c0e-00aa-4295-a5b6-ea5fa43663f3",
   "metadata": {},
   "source": [
    "**Q.Create this table and answer flowing questions**\n",
    "***\n",
    "**A.**\n",
    "<code>\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"dept\", StringType(), True)\n",
    "])\n",
    "data = [\n",
    "    (\"manish\", 50000, \"IT\"),\n",
    "    (\"vikash\", 60000, \"sales\"),\n",
    "    (\"raushan\", 70000, \"marketing\"),\n",
    "    (\"mukesh\", 80000, \"IT\"),\n",
    "    (\"pritam\", 90000, \"sales\"),\n",
    "    (\"nikita\", 45000, \"marketing\"),\n",
    "    (\"ragini\", 55000, \"marketing\"),\n",
    "    (\"rakesh\", 100000, \"IT\"),\n",
    "    (\"aditya\", 65000, \"IT\"),\n",
    "    (\"rahul\", 50000, \"marketing\")\n",
    "]\n",
    "df_test = spark.createDataFrame(data, schema=schema)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fae244-8e80-4126-bd10-5b698786cbab",
   "metadata": {},
   "source": [
    "**Q. What is the total salary given to its employee**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df_test.select(sum(col(\"salary\")).alias(\"total_salary\")).show()  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afec579-34d3-4b81-85a4-41c31fbcf99a",
   "metadata": {},
   "source": [
    "**Q. What is the total salary per department wise**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` df_test.select(col(\"dept\"), col(\"salary\")).groupBy(col(\"dept\")).agg(sum(col(\"salary\")).alias(\"dept_salary\")).show() `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a0482-7160-4b62-b3ff-31f8f138bc99",
   "metadata": {},
   "source": [
    "**Q. I want all column name with one extra column where total salary of each department is maintained**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "` from pyspark.sql.window impoer Window `  # import Window function                               \n",
    "` window = Window.partitionBy(col(\"department\")).orderBy(col(\"salary\")) ` # create new window              \n",
    "` df.withColumn(\"total_dep_salary\", sum(col(\"salary\")).over(window)).show() `                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cd431-8dfe-4473-9e1a-65d0f46fd91a",
   "metadata": {},
   "source": [
    "**Q. How join works**\n",
    "****\n",
    "**A.** First table column value tried to match all column values in a single table \n",
    "<code>result = df1.join(df2, on=\"id\", how=\"inner\")</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b216d-c81c-4b0a-87e3-48c9eb2f3482",
   "metadata": {},
   "source": [
    "**Q. Why do we need JOIN**\n",
    "****\n",
    "**A.** If we don't have enough information in single table then we need another table that fulfills our requirement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364aaed2-fb9b-4be7-9e19-edfc18dbcc6b",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What to do after joining two tables**\n",
    "****\n",
    "**A.** It depends on you after joining you can use distinct and you can skip this part(using distinct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8f550-72b7-4e00-a69c-ddac45a869ba",
   "metadata": {},
   "source": [
    "**Q. What if tables have the same column name**\n",
    "****\n",
    "**A.** If you don't define column name with data frame it will return an **ambiguous error**               \n",
    "` df = df.join(df2, df[\"id\"] == df2[\"id\"], \"inner\") `                     \n",
    "` df.show() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1404a-43c4-4a83-81c0-0e573af82624",
   "metadata": {},
   "source": [
    "**Q. How to join on two more columns**\n",
    "****\n",
    "**A.**                         \n",
    "<code>df.join(df1, \n",
    "        (df[\"id\"]==df2[\"emp_id\"]) & (df2[\"emp_id\"]==df3[\"employee_id\"]),\n",
    "        \"inner\"\n",
    "        ).select(\"*\")show()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609621b-0f94-4c66-bef0-8776178fd3e0",
   "metadata": {},
   "source": [
    "**Q. How to join multiple tables**\n",
    "***\n",
    "**A.**                        \n",
    "<code>df.join(df1, \n",
    "        (df[\"id\"]==df2[\"emp_id\"]) & (df2[\"emp_id\"]==df3[\"employee_id\"]),\n",
    "        \"inner\"\n",
    "        ).select(\"*\")show()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead999b5-dd85-41d2-a11b-2810cb14bc3b",
   "metadata": {},
   "source": [
    "**Q. How many types of join**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "<code>result = df1.join(df2, on=\"id\", how=\"inner\")</code> **Inner Join**                          \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"outer\")</code> **Outer (Full) Join**                   \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"left\")</code> **Left Join**                   \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"right\")</code> **Right Join**               \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"left_semi\")</code> **Left Semi Join**                \n",
    "<code>result = df1.join(df2, df1[\"id\"]df==2[\"emp_id\"]\", how=\"left_anti\")</code> **Left Anti Join**             \n",
    "<code>result = df1.crossJoin(df2)</code> **Cross Join**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadfb1e-7fff-44a6-8b4f-98cef2e001d6",
   "metadata": {},
   "source": [
    "**Q. Define left_semi and left_anti join**\n",
    "***\n",
    "**A.**                           \n",
    "**left_semi**: All records form left table(A), that common in both table(A,B), join(inner-left)                 \n",
    "**left_anti**: All records form left table(A), that not common in table(B), join(full-right-inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5633eb2-3963-4b54-b6b4-dbb19a38389c",
   "metadata": {},
   "source": [
    "**Q. What is the window function**\n",
    "****\n",
    "**A.** Window function used to create rows of the same category into groups, allowing for calculation or aggregation to be applied within these groups \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/imrannazer-/\">\n",
    "<img src=\"images/035.png\" alt=\"035\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e21708-89d0-4c77-82a9-7013ab308c63",
   "metadata": {},
   "source": [
    "**Q. What is the row number rank and dense_rank in PySpark**\n",
    "***\n",
    "**A.**            \n",
    " \n",
    "**row_number():** Returns a sequential number starting from 1 within a window partition         \n",
    "**rank()** Returns the rank of rows within a window partition, with gaps.         \n",
    "**dense_rank():** Returns the rank of rows within a window partition without any gaps. Where as Rank() returns rank with gaps.\n",
    "\n",
    "```python\n",
    "emp_df.withColumn(\"row_num\", row_number().over(window))\\\n",
    "    .withColumn(\"rank_num\", rank().over(window))\\\n",
    "    .withColumn(\"dense_rank_num\", dense_rank().over(window))\\\n",
    "    .show()\n",
    "\n",
    "\n",
    "| emp_id | emp_name | salary | department | gender | row_num | rank_num | dense_rank_num |\n",
    "|--------|----------|--------|------------|--------|---------|----------|----------------|\n",
    "|   1    | imran    | 50000  | IT         | m      |   1     |   1      |        1       |\n",
    "|  11    | Khan     | 50000  | IT         | f      |   2     |   1      |        1       |\n",
    "|   9    | Glaxy    | 65000  | IT         | m      |   3     |   3      |        2       |\n",
    "|   4    | Nazer    | 80000  | IT         | m      |   4     |   4      |        3       |\n",
    "|   8    | roshan   |100000  | IT         | f      |   5     |   5      |        4       |\n",
    "|   6    | Hasib    | 45000  | marketing  | f      |   1     |   1      |        1       |\n",
    "|  10    | abraham  | 50000  | marketing  | m      |   2     |   2      |        2       |\n",
    "|   7    | Baidan   | 55000  | marketing  | f      |   3     |   3      |        3       |\n",
    "|   3    | Nitin    | 70000  | marketing  | m      |   4     |   4      |        4       |\n",
    "|   2    | Mufti    | 60000  | sales      | m      |   1     |   1      |        1       |\n",
    "|   5    | Hasina   | 90000  | sales      | f      |   2     |   2      |        2       |\n",
    "|  12    | Rajab    | 90000  | sales      | m      |   3     |   2      |        2       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9b238-de90-42bf-a056-ef55861883e6",
   "metadata": {},
   "source": [
    "**Q. How to use windows function**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<code>from pyspark.sql.window import Window\n",
    "window = Window.partitionBy(col(\"department\")).orderBy(col(\"salary\"))\n",
    "emp_df.withColumn(\"row_num\", row_number().over(window)).show()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d2e3b-cedc-4641-938d-3979bc090566",
   "metadata": {},
   "source": [
    "**Q. How to calculate the top two salary holders from each department**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "<code>window = Window.partitionBy(col(\"department\"),col(\"gender\")).orderBy(desc(col(\"salary\")))\n",
    "emp_df.withColumn(\"row_num\", row_number().over(window)).filter(col(\"row_num\")<3)\\\n",
    "    .show()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2e26a-6a58-4850-b574-64c1ad9ff476",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is LEAD and LAG in PySpark**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "**lag(columnName: String, offset: Int, defaultValue: Any)** returns the value that is `offset` rows before the current row, and `null` if there is less than `offset` rows before the current row.\n",
    "\n",
    "**lead(columnName: String, offset: Int, defaultValue: Any)**:returns the value that is `offset` rows after the current row, and `null` if there is less than `offset` rows after the current row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e34af-ef7b-4478-8061-bd66134a6595",
   "metadata": {},
   "source": [
    "**Q. what is the percentage of loss or gain based on previous month sales**\n",
    "***\n",
    "**A.**\n",
    "```python\n",
    "prev_month_sales = prd_df.withColumn(\"prev_month_sales\", lead(col(\"p_sales\"),1).over(window))\\\n",
    "    .withColumn(\"profit\", col(\"p_sales\")-col(\"prev_month_sales\"))\\\n",
    "    .withColumn(\"p_or_l\", round((col(\"profit\")/col(\"p_sales\"))*100, 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817dc65-2c04-4bc2-84d6-57d5d277fdf9",
   "metadata": {},
   "source": [
    "**Q. What is the percentage of sales each month based on last 6 month saless**\n",
    "***\n",
    "**A.**\n",
    "```python\n",
    "window = Window.partitionBy(\"p_id\")\r\n",
    "last_six_month_df = prd_df.withColumn(\"previous_six_month_total_sales\", sum(\"p_sales\").over(window))\\\r\n",
    "    .withColumn(\"perc_sales_each_month\", round((col(\"p_sales\")/col(\"previous_six_month_total_sales\")) * 100, 2) )\r\n",
    "last_six_month_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9653b1-c45e-4078-888d-cc38fb0a0493",
   "metadata": {},
   "source": [
    "**Q. Find out the difference in sales of each product from their first month sells to latest sales**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "\n",
    "<code>window = Window.partitionBy(col(\"p_id\")).orderBy(col(\"p_date\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "prd_df.withColumn(\"first_sale\",first(col(\"p_sales\")).over(window))\\\n",
    "    .withColumn(\"last_sale\", last(col(\"p_sales\")).over(window))\\\n",
    "    .withColumn(\"difference\", col(\"first_sale\")-col(\"last_sale\")).show() </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9fba6c-a23a-4076-ba66-76c6a5414e23",
   "metadata": {},
   "source": [
    "**Q. Send a mail who has not completed duty time(8 hours)**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<code>new_df= emp_df.withColumn(\"login\",first(\"timestamp\") .over (window) ) \n",
    "            .withColumn(\"logout\", last(\"timestamp\") .over (window) ) \n",
    "            .withColumn(\"login\", to_timastamp(\"login\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"logout\", to_timestamp(\"logout\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"total_time\",col(\"logout\")-col(\"login\") ) . show() </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e78c26-6854-462b-a57e-2a2f2352cdbf",
   "metadata": {},
   "source": [
    "**Q. Analyze launched three month sales**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "<code>window = Window.partitionBy(col(\"p_name\")).orderBy(col(\"p_date\"))\n",
    "prd_df.withColumn(\"row_number\", row_number().over(window))\n",
    "    .filter(col(\"row_number\")>=3)\n",
    "    .withColumn(\"total_sum\", sum(col(\"p_sales\")).over(window)).show()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ccf57e-1a8d-4c37-88be-91dee433c565",
   "metadata": {},
   "source": [
    "**Q. What is SCD2**\r\n",
    "****\r\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37beb4cf-5918-4568-8778-c66bf0e7a108",
   "metadata": {},
   "source": [
    "**Q. What is default date format**\n",
    "****\n",
    "**A.** *` YYYY-MM-DD `* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d9507-975e-4516-aec9-f1821a676a8a",
   "metadata": {},
   "source": [
    "**Q.**\r\n",
    "****\r\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16956d36-797b-4529-b43b-2f1f6a2edd68",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
