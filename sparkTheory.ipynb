{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ddadd6-e4c2-4988-84f9-7ece1083415b",
   "metadata": {},
   "source": [
    "**Q. What is the Apache Spark**\n",
    "****\n",
    "**A.** Apache spark is a **unified** computing engine and set of **libraries** for parallel data processing on a computer cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed6abb-ccb0-4292-b66d-5f6a79c665d3",
   "metadata": {},
   "source": [
    "**Q. What is unified**\n",
    "****\n",
    "**A.** Spark is designed to support wide range of tasks over the same computing engine,        \n",
    "e.g data scientists, data analytics, and data engineers can all use the same platform(Spark) for their analysis, modeling, and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700125f-4a83-4a79-818a-bf40d22b8b88",
   "metadata": {},
   "source": [
    "**Q. What is computing engine**\n",
    "****\n",
    "**A.** Apache Spark is limited to a computing engine(RAM), it does not store the data on hard disk, Spark can connect with different data sources like HDFS, JDBC, ODBC, Azure, AWS, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af7966-de05-4af9-9af2-bbdd7d700e7b",
   "metadata": {},
   "source": [
    "**Q. What are libraries**\n",
    "****\n",
    "**A.** It is a set of code that can be used in project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266f249-18bb-4cf5-b765-555b564d979e",
   "metadata": {},
   "source": [
    "**Q. What is the computer cluster**\n",
    "****\n",
    "**A.** computer cluster is a group of nodes/machine/CPU, where Master slave architecture                     \n",
    "(CLUSTER is a group of computers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bd357-edd0-4314-ad47-a1a8da9c4ded",
   "metadata": {},
   "source": [
    "**Q. What is parallel data processing?**\n",
    "****\n",
    "**A.** Parallel data processing is a technique that divides a complex problem into smaller parts, each handled by an individual processor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20568b4c-5b43-4038-8d44-9e6e7955f66c",
   "metadata": {},
   "source": [
    "**Q. Why Apache Spark, what problem does it solve**\n",
    "****\n",
    "**A.** \n",
    "**Distributed Computing**: Spark distributes data across a cluster.              \n",
    "**Parallel-Processing** :This minimizes processing time significantly.                                        \n",
    "**In-Memory Processing**: Unlike traditional systems, Spark keeps intermediate data in memory, avoiding constant reads/writes to disk.     \n",
    "**More APIs**: Spark provides APIs in Python, Java, Scala, and SQL, making it accessible for various use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20337dc4-41e5-414c-96a3-74bebd8d752e",
   "metadata": {},
   "source": [
    "**Q. Why we need Apache Spark**\n",
    "****\n",
    "**A.** Apache spark processes data in-memory(RAM), Apache Spark schedules jobs in stages and tasks, and it uses catalyst optimizer to enhance execution plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96567487-f5d3-4a9f-823f-3a1fd77c6a88",
   "metadata": {},
   "source": [
    "**Q. Which language is Spark written**\n",
    "***\n",
    "**A.** Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885048a-4708-4ceb-8ba7-b71389fef073",
   "metadata": {},
   "source": [
    "**Q. Hadoop vs Apache Spark**\n",
    "****\n",
    "**A.**\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Feature</th>\n",
    "      <th>Spark</th>\n",
    "      <th>MapReduce</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Processing</td>\n",
    "      <td>In-memory data processing</td>\n",
    "      <td>Batch processing using disk storage(intermediate result)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Data Source and Formats</td>\n",
    "      <td>Support various data sources and structured, semi-structured <br>and un-structured file format</td>\n",
    "      <td>Handles structured data using HDFS</td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "      <td>Batch | Streaming</td>\n",
    "      <td> Build for batch as well as streaming data processing </td>\n",
    "      <td> Build for batch data processing </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Language Support</td>\n",
    "      <td>JAVA, Python, R, Scala</td>\n",
    "      <td>Primarily JAVA</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Machine Learning</td>\n",
    "      <td>Provides MLLib library for machine learning</td>\n",
    "      <td>Not natively designed for machine learning</td>\n",
    "    </tr>\n",
    "<tr>\n",
    "      <td>Data Source and Formats</td>\n",
    "      <td>Support various data sources and structured, semi-structured <br>and un-structured file format</td>\n",
    "      <td>Handles structured data using HDFS</td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "      <td> Code complexity </td>\n",
    "      <td>Easy to write and debug code we have interactive shell to develop and test</td>\n",
    "      <td> Difficult to write code in Hadoop I've was built to make easy code </td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "      <td> Fault tolerance </td>\n",
    "      <td> use DAG to provide fault tolerance </td>\n",
    "      <td> It is having block of data and replication factor to handle the fault tolerance </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Ease of Use</td>\n",
    "      <td>Provides low-level (RDD) APIs, and High-level (Dataframe) APIs</td>\n",
    "      <td>Requires low-level programming in JAVA</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dacc70-be2c-4fa6-82be-3f480554c916",
   "metadata": {},
   "source": [
    "**Q. What is the Apache Spark ecosystem?**\n",
    "****\n",
    "**A.**              \n",
    "<div>\n",
    "    <div>\n",
    "        <img src=\"https://i.ibb.co/RpGZ4BC/009.png\" alt=\"009\" border=\"0\" style=\"width:450px;height:250px\" align=\"right\">\n",
    "    \n",
    "   </div>\n",
    "        <div>\n",
    "<strong>Spark Core:</strong> Nothing but Low level API(RDD).<br>\n",
    "<strong>Cluster Manager:</strong> Cluster manager is used to acquire resources for executing jobs.<br>\n",
    "<strong>RDD:</strong> Resilient Distributed Dataset (RDD) is a low-level API.<br>\n",
    "<strong>DataFrame: </strong>It is a library on top up spark core(RDD).<br>\n",
    "<strong>Spark Engine:</strong> It is compute engine. Get resource from cluster manager(YEAR) and data form HDFS<br>\n",
    "<p>By default, Spark’s scheduler runs jobs in <strong>FIFO</strong>(first-in-first-out) fashion. Each job is divided into “stages” (e.g. map and reduce phases), and the first job gets priority on all available resources while its stages have tasks to launch, then the second job gets priority, etc.</p>\n",
    "   </div>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5287d899-624d-4556-ba4e-4e9b38feeec8",
   "metadata": {},
   "source": [
    "**Q. what is Apache Spark architecture?**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "\n",
    "<div>\n",
    "    <div>\n",
    "        <img src=https://i.ibb.co/q7gkv4S/040.pngg\" alt=\"009\" border=\"0\" style=\"width:500px;height:300px\" align=\"right\">\n",
    "        <img src=https://i.ibb.co/pjWLPGw/041.pngg\" alt=\"009\" border=\"0\" style=\"width:500px;height:300px\" align=\"right\">   \n",
    "   </div>\n",
    "        <div>\n",
    "<span>Apache Spark uses <strong>master-slave</strong></span> architecture, where there will be one master process & multiple slave processes. This master slave architecture is applied at the<br>\n",
    "1) <strong>Cluster Management Level</strong>: Application master is the master and the Node managers are the slaves. Application master is responsible for coordinating the node mangers to allocate resources like memory & CPU cores.<br>\n",
    "2) <strong>Application Level</strong>: Spark Driver is the master & Spark executors are the slaves/workers. Driver is responsible for sending the portion of the logic to each executor.<br>\n",
    "The key components of Apache Spark are:<br>\n",
    "1) Spark Driver (master)<br>\n",
    "2) Executors (slave)<br>\n",
    "3) Spark Session (or Spark Context prior to Spark 2.0).<br>\n",
    "4) Cluster Resource Manager (E.g. YARN, Kubernetes, Mesos, Nomad & Spark’s own standalone cluster manager).<br>\n",
    "5) Application Master i.e. Resource Manager (master).<br>\n",
    "6) Node managers i.e. slave node mgr (slave).<br>\n",
    "   </div>\n",
    "</div>\n",
    "visit : https://www.java-success.com/apache-spark-anatomy-interview-qas/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7bb8c-3b8f-4ad3-badf-c85d693cd340",
   "metadata": {},
   "source": [
    "**Q. What is the difference between low-level API and high-level API**\n",
    "***\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d7034-d04d-4d19-874c-2513aad8e69a",
   "metadata": {},
   "source": [
    "**Q. What is the transformation and how many types of transformation do we have?**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Transformation</th>\n",
    "      <th>Type of Transformation</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Transformation is a function applied to an RDD/DataFrame that creates a new RDD/DataFrame, defining a sequence of data processing operators without executing them until an action is called, there are two types of transformation <strong>narrow dependency</strong> and <strong>wide dependency</strong>. Narrow transformation is cheap and wide transformation is expensive. </br><strong> * Narrow Transformation </strong>one to one and </br><strong> * Wide Transformation </strong>one to N</td>\n",
    "      <td>\n",
    "          <img src=\"https://i.ibb.co/ynQ57mh/010.png\" style=\"width:400px;height:100px;\" alt=\"010\" border=\"0\">\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eee420-d6d0-4267-8a00-e9e40d8eb572",
   "metadata": {},
   "source": [
    "**Q. What is the narrow dependency transformation**\n",
    "****\n",
    "**A.** Narrow dependence transformation that does not require data movement/shuffling between partitions, e.g. ` filter() `, ` select() `, ` union() `, ` map() `, etc.  Narrow transformation is cheap.\n",
    "\n",
    "<center> <b>----------------------------------List of Narrow Transformation----------------------------------</b> </center>    \n",
    "\n",
    "| Narrow Transformation | Description                                           |\n",
    "|------------------------|-------------------------------------------------------|\n",
    "| `map`                  | Applies a function to each element of the RDD or DataFrame. |\n",
    "| `filter`               | Selects elements based on a given condition.          |\n",
    "| `flatMap`              | Similar to `map`, but each input item can produce zero or more output items. |\n",
    "| `union`                | Combines two RDDs or DataFrames without shuffling.    |\n",
    "| `distinct`             | Returns unique elements in the RDD or DataFrame.      |\n",
    "| `intersection`         | Returns common elements between two RDDs or DataFrames.|\n",
    "| `subtract`             | Returns elements in one RDD or DataFrame not present in another. |\n",
    "| `cartesian`            | Computes the Cartesian product of two RDDs.           |\n",
    "| `sortBy`               | Sorts elements based on a specified key.              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fef15f-ec64-4e4e-abda-3dd047fabde0",
   "metadata": {},
   "source": [
    "**Q. What is the wide dependency transformation**\n",
    "****\n",
    "**A.** Wide dependence transformation that does require data movement/shuffling between partitions, e.g. ` groupBy() `, ` join() `, ` distinct() `, etc. Wide transformation is expensive.\n",
    "\n",
    "<center> <b>--------------------------------------------List of Wide Transformation--------------------------------------------</b> </center>\n",
    "\n",
    "| Wide Transformation    | Description                                           |\n",
    "|------------------------|-------------------------------------------------------|\n",
    "| `groupByKey`           | Groups the data by key, requiring a shuffle.          |\n",
    "| `reduceByKey`          | Aggregates values based on key, requiring a shuffle.  |\n",
    "| `aggregateByKey`       | Aggregates values with more control than `reduceByKey`, requiring a shuffle. |\n",
    "| `sortByKey`            | Sorts the RDD or DataFrame based on key, requiring a shuffle. |\n",
    "| `join`                 | Joins two RDDs or DataFrames based on a common key, requiring a shuffle. |\n",
    "| `cogroup`              | Groups the data from multiple RDDs or DataFrames based on a common key, requiring a shuffle. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0353c0d-bd62-4bc0-92e4-627cb64c75d3",
   "metadata": {},
   "source": [
    "**Q. What happens when we use groupBy or join transformation**\n",
    "****\n",
    "**A.** Both are groupBy and join are wide transformations for data will be shuffled between partitions, there for it is wide transformation and they are expensive transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81842a95-5de5-4eb0-b2a2-6edb926b1a8f",
   "metadata": {},
   "source": [
    "**Q. How jobs are created in Apache Spark**\n",
    "****\n",
    "**A.** When we trigger an action like ` collect() `, ` count() `, ` show() `, ` take() `, ` printSchema() `, etc. Every action has won DAG.                                \n",
    "Driver returns action result, therefore we need aware of driver out of memory (OOM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b47add-2803-4660-b23b-adc4ee216750",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/xS9xCsb/042.png\" alt=\"009\" border=\"0\" style=\"width:100%;height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5e69d-2edb-4350-bc0b-5d374ebdc2dc",
   "metadata": {},
   "source": [
    "**Q. What is the DAG (directed acyclic graph) in Apache Spark**\n",
    "****\n",
    "**A.**<div>\n",
    "    <img src=\"https://i.ibb.co/YtPyg6r/011.jpg\" style=\"width:450px;height:150px;\" align=\"right\" alt=\"011\" border=\"0\">\n",
    "    <p>\n",
    "        DAG (directed acyclic graph) is a scheduling layer of Apache Spark that implements stages-oriented scheduling and it is a transformation of a logical execution plan to a physical execution plan using stages.\n",
    "    <p>Action creates job and every job has a own DAG</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774d3bd-76fd-49d2-a711-f0a40de22795",
   "metadata": {},
   "source": [
    "**Q. What is the lazy evalution?**\n",
    "****\n",
    "**A.** Lazy evolution is **planning everything but doing nothing**. In Apache Spark when we submit any type of transformation, Spark creates only plan(optimization) no more (till action calling), therefore it is called <u>_lazy evolution_</u>. Spark optimizes code/transformation during lazy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32bab2f-6f50-4f17-8714-d8d0c31b83db",
   "metadata": {},
   "source": [
    "**Q. What are the task, job, and stages in Apache Spark?**\n",
    "****\n",
    "**A.** \n",
    "\n",
    "<div>\n",
    "   <img src=\"https://i.ibb.co/C2fcVwJ/043.png\" style=\"width:450px;height:200px;\" align=\"right\" alt=\"011\" border=\"0\" align=\"right\"> \n",
    "</div>\n",
    "<div>\n",
    "    <p><strong>Task</strong>: A task is a unit of work that is sent to the executor, One task per partition. The same task is done over different partitions of the RDD parallelly.</p>\n",
    "<p><strong>Stages</strong>: Stages in Apache Spark represent groups of tasks, that can be executed together(task and stage) to compute the same transformation on multiple machines, stages Run sequentially of parallel data processing to the executor.</p>\n",
    "    <p><strong>Jobs</strong>: Job is a sequence of stages triggered by an action such as <code>count()</code>, <code>collect()</code>, <code>show()</code>, <code>read()</code>, etc</p>\n",
    "<ul>\n",
    "  <li>Every action inside Spark application trigger spark-job action <strong>(action = SparkJob)</strong></li>\n",
    "    <li>Every job has minimum one stage and every stage has minimum one task</li>\n",
    "    <li>Task process parallelly based on partitons</li>\n",
    "    <li>Stage change on wide dependence transformation </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12bb51-fe34-46a9-8a08-c51ebb66d91b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a70df6-a04f-4992-986a-c374ebf456dc",
   "metadata": {},
   "source": [
    "**Important points**\n",
    "- Driver and executor are exclusive no other can use to Spark app\n",
    "- Multiple executors of the same application can run inside our worker machine\n",
    "- Multiple executors of different applications can run inside one worker machine\n",
    "- One executor handles multiple parallel tasks on the same stages and the same application\n",
    "- Tasks of the same stages can run in parallel stages are executed sequentially\n",
    "- A Spark application consists of multiple jobs and each job can be delivered into multiple stages\n",
    "- The result of each job is written to the driver by the executor\n",
    "- The new stage starts after a wild transformation\n",
    "- Stages run sequentially\n",
    "- Each stage has some tasks\n",
    "- The Task the smallest unit in the execution hierarchy\n",
    "- One task can not executed more than one executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd2940-b7e5-496b-b1ee-f3213a6cdbf9",
   "metadata": {},
   "source": [
    "**Q. What is the catalyst optimizer or Spark SQL engine?**\n",
    "****\n",
    "**A.**\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/znSZ5L9/013.jpg\" style=\"width:100%;height:350px;\"><br />\n",
    "</div>\n",
    "<div>\n",
    "    <p><strong>Catalyst</strong> optimizer is a query Optimisation and execution framework, which decides how the code should be executed and lays out a plan\n",
    "    </p>\n",
    "    <p>There are four paces</p>\n",
    "    <ol style=\"padding-left:50px;\">\n",
    "      <li>Code analysis (Un-resolve to resolve code)</li>\n",
    "      <li>Logical plan optimization</li>\n",
    "      <li>Physical plan optimization</li>\n",
    "      <li>Code generation on</li>\n",
    "    </ol>\n",
    "</div>\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/KF9ZqFZ/012.jpg\" style=\"width:100%;height:350px;\"><br />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce5f65-b999-465b-9ea0-eb0220c6db26",
   "metadata": {},
   "source": [
    "**Q. What is the difference between logical-plan and physical-plan**\n",
    "***\n",
    "**A.** Logical plan means **only thinking**, and physical plan means **doing something**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1190eaf-2389-4ec3-9730-c8f0962405f7",
   "metadata": {},
   "source": [
    "**Q. Why do we get AnalysisException error**\n",
    "****\n",
    "**A.** An AnalysisException in Apache Spark typically occurs during the analysis phase of query execution from **catalog**. This error is often related to issues such as:\n",
    "\n",
    "- Undefined or Unresolved Columns\n",
    "  - df.select(\"nonexistent_column\")\n",
    "- Ambiguous(same column name in two table in case of join) Column Reference\n",
    "  - df.select(\"ambiguous_column\")\n",
    "- Unsupported Operations\n",
    "  - df.write.saveAsTable(\"unsupported_operation\")\n",
    "- Issues with Table or View Resolution\n",
    "  - spark.sql(\"SELECT * FROM non_existent_table\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b5071-e88b-4e2d-9567-31b49a20f087",
   "metadata": {},
   "source": [
    "**Q. What is the catalog?**\n",
    "****\n",
    "**A.** The catalog is a part of the catalyst optimizer and metadata repository that organizes and manages information about tables, databases, and functions, facilitating structured data management and access in Spark SQL.                     \n",
    "- It is used to verify user code during the code conversion from un-resolve to resolve plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93be9cb-05de-4a82-a7e5-f968100a2b34",
   "metadata": {},
   "source": [
    "**Q. What is the physical planning or Spark plan**\n",
    "****\n",
    "**A.** It is a detailed strategy outlining how to execute a given DataFrame operation, specifying the series of stages and tasks that will be carried out on the distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c448e5-431d-4ebe-9a30-cb2a4659a275",
   "metadata": {},
   "source": [
    "**Q. Is Spark SQL engine a compiler or not**\n",
    "****\n",
    "**A.** **Yes**. It is a compiler, converts code to JAVA bite code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffddd99-65a5-46e2-9d32-9918e8a38212",
   "metadata": {},
   "source": [
    "**Q. How many phases are involved in Spark SQL engine to convert a code into java bite code**\n",
    "****\n",
    "**A.** **Four Phases**\n",
    "<ol >\n",
    "  <li>Code analysis (Un-resolve to resolve code)</li>\n",
    "  <li>Logical plan optimization</li>\n",
    "  <li>Physical plan optimization</li>\n",
    "  <li>Code generation</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62732a1-6d85-45f9-aeab-9a46e6c546d8",
   "metadata": {},
   "source": [
    "**Q. What is the RDD**\n",
    "***\n",
    "**A.** RDD is standard for **R**esilient **D**istributed **D**ata-set. \n",
    "- Resilient: Fault tolerance/ healing power\n",
    "- Distributed: Over the cluster\n",
    "- Dataset: Actual data\n",
    "  \n",
    "It is one of the fundamental schema-less data structures. That can handle both structured and semi-structured data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7b130-a40b-4846-b9af-8be657f7d9fb",
   "metadata": {},
   "source": [
    "**Q. When do you we need an RDD**\n",
    "****\n",
    "**A.** When user wants to process data on low-level then RDD can be used. RDD does not provide any optimization during the code execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df2990-4ffb-422b-a88b-cfb1deb97598",
   "metadata": {},
   "source": [
    "**Q. What is the Features of an RDD**\n",
    "****\n",
    "**A.**\n",
    "- In-memory competition\n",
    "- Lazy evolution\n",
    "- Fault-tolerant\n",
    "- Immutability\n",
    "- Partition\n",
    "- Low-level API\n",
    "- Code Optimisation by user\n",
    "- RDD uses user memory in memory management(Dataframe uses spark memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0fd6a-29e2-49ae-8b84-85fba55b6a67",
   "metadata": {},
   "source": [
    "**Q. What is the difference between dataFrame and data set**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a119c-bb8c-424e-90a5-7ed3f3cc04b0",
   "metadata": {},
   "source": [
    "**Q. Why we should not use an RDD**\n",
    "****\n",
    "**A.** RDD does not provide any Optimisation technique and code complexity is high, therefore we don't use RDD\n",
    "- RDD works on ` How to do `\n",
    "- Dataframe works on ` What to do `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da1958-85ba-4dd6-b5ef-4e99e85a2eff",
   "metadata": {},
   "source": [
    "**Q. What is the difference between SparkSession and SparkContext**\n",
    "****\n",
    "**A.**\n",
    "<center><b>---------------------------------------SparkSession vs SparkContext---------------------------------------</b></center>\n",
    "\n",
    "| Aspect                | SparkSession                                           | SparkContext                                       |\n",
    "|-----------------------|--------------------------------------------------------|----------------------------------------------------|\n",
    "| **Purpose**           | Unified entry point for high-level Spark operations, including DataFrame and SQL functionality | Low-level interface for managing Spark jobs and RDD operations |\n",
    "| **Creation**          | Implicitly created in Spark applications               | Explicitly created using SparkConf in applications |\n",
    "| **Functionality**      | Higher-level API for Spark operations                 | Focused on low-level RDD operations and cluster configuration |\n",
    "| **Concurrency**        | Supports concurrent execution of multiple Spark jobs    | Manages the execution of a single Spark job at a time |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f384e01-ad44-4c13-9459-af31c698e58e",
   "metadata": {},
   "source": [
    "**Q. How to create a SparkSession**\n",
    "***\n",
    "**A.** \n",
    "\n",
    "` spark = SparkSession.builder.master(\"local[5]\")\\ `                    \n",
    "`         .appName(\"testing\")\\ `                        \n",
    "`         .config(\"driver-memory\", \"12G\")\\ ` configure your application                         \n",
    "`         .getOrCreate() ` get session if exist otherwise create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690450a-88fe-4b57-9837-29d8e3a4d93a",
   "metadata": {},
   "source": [
    "**Q. How to create a SparkContext**\n",
    "***\n",
    "**A.** \n",
    "` sc = spark.SparkContext `                    \n",
    "` rdd = sc.parallelize(data) `                     \n",
    "` rdd.collect() `"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ab78ce1-d123-4856-9175-ad3bfadee331",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/tJ03ySk/012.png\" alt=\"012\" width=\"100%\" height=\"300\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5ae31-85a9-48cd-a4b4-e976ea750b74",
   "metadata": {},
   "source": [
    "**Q. How many jobs will be created in the given question**\n",
    "****\n",
    "**A.** Two jobs                   \n",
    "1. read()\n",
    "2. collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ed2bb-d2db-4a28-b8e6-eb9d6c9ec584",
   "metadata": {},
   "source": [
    "**Q. How many stages will be created in the given question**\n",
    "***\n",
    "**A.** \n",
    "1. read()\n",
    "2. repartition()\n",
    "3. groupBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109bb04-0c8b-4d85-9fee-56baae46edad",
   "metadata": {},
   "source": [
    "**Q. How many tasks will be created in the given question**\n",
    "****\n",
    "**A.**\n",
    "1. read() : 1 task\n",
    "2. repartition(): 2 task\n",
    "3. groupBy(): 200 tasks (data shuffling creates 200 partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8cf53-1abb-49f5-815e-fe47acdfecb4",
   "metadata": {},
   "source": [
    "**Q. How to convert Python variable into RDD**\n",
    "****\n",
    "**A.** By using parallelize() funciton                      \n",
    "`python_variable = [2,3,6,58,9,8,7,5,4]`                      \n",
    "`RDD = sc.parallelize(python_variable)`                             \n",
    "`type(RDD)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c63c6-4e2d-45ce-928d-582ac85826ea",
   "metadata": {},
   "source": [
    "**Q. How to create an RDD by using a file**\n",
    "****\n",
    "**A.** By using textFile() function                \n",
    "`fileRDD = sc.textFile(r\"file_path\")`              \n",
    "`print(fileRDD.collect())`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce23c3-cec0-4adb-9c52-d1cc9b12c7ce",
   "metadata": {},
   "source": [
    "**Q. How to convert an RDD to Dataframe**\n",
    "****\n",
    "**A.** By using toDF() funciton\n",
    "\n",
    "`df = RDD.toDF(schema = [column1, column2, ...])`                       \n",
    "`type(df)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9116a7-040a-4adc-a885-1881d1719d4d",
   "metadata": {},
   "source": [
    "**Q. So how to convert data frame into RDD**\n",
    "***\n",
    "**A.** By rdd keyword\n",
    "\n",
    "`new_rdd = df.rdd `                       \n",
    "`type(new_rdd)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cc2f6-10dc-466e-9d05-b06859d69959",
   "metadata": {},
   "source": [
    "**Q. How to get number of partitions in RDD**\n",
    "****\n",
    "**A.** By using getNumPartitions() function                 \n",
    "`RDD.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a9e87-5762-45a4-a982-240b3938eede",
   "metadata": {},
   "source": [
    "**Q. map( ) vs flatMap**\n",
    "****\n",
    "**A.**  \n",
    "**map()** is used to operate on the record level. It returns a new RDD by applying a function to each element of the RDD function, in map( ) must return only one item.                       \n",
    "**flatMap()** is similar to map(), it returns a new RDD by applying a function to each element of the RDD but output is flattend\n",
    "\n",
    "\n",
    "\n",
    "| **Aspect**           | `map` Transformation                              | `flatMap` Transformation                            |\n",
    "|-----------------------|----------------------------------------------------|-----------------------------------------------------|\n",
    "| **Functionality**     | Applies a function to each element of the RDD.    | Applies a function to each element and flattens the result. |\n",
    "| **Output**            | One output element for each input element.        | Can produce zero, one, or multiple output elements for each input element. |\n",
    "| **Output Structure**  | Maintains the structure of one-to-one mapping.     | Flattens the results into a single sequence.         |\n",
    "| **Example**           | `rdd.map(lambda x: (x, x * 2))`                   | `rdd.flatMap(lambda x: (x, x * 2))`                  |\n",
    "| **Input RDD**         | `[1, 2, 3, 4]`                                   | `[1, 2, 3, 4]`                                      |\n",
    "| **Output RDD**        | `[(1, 2), (2, 4), (3, 6), (4, 8)]`               | `[1, 2, 2, 4, 3, 6, 4, 8]`                           |\n",
    "|**Code**| ` sc.parallalize([3,4,5]).map(lambda x: [x**x]).collect() ` | ` sc.parallalize([3,4,5]).flatMap(lambda x: [x**x]).collect() ` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08309bbd-a74d-4dc3-8882-7b9be4d20995",
   "metadata": {},
   "source": [
    "**Q. reduceByKey vs groupByKey**\n",
    "****\n",
    "**A.** <p><code>reduceByKey()</code> and <code>groupByKey()</code> both are wide dependency transformations, both perform transformation on key-value pair RDD</p>\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/92pkK1G/014.jpg\" style=\"width:400px;height:250px;\" align=\"right\">\n",
    "    \n",
    "\n",
    "<ul>\n",
    "  <li>reduceByKey is something like grouping + aggregation</br>\n",
    "      <code>rdd = sc.parallelize([(1, 2), (2, 4), (1, 6), (2, 8)])\n",
    "result = rdd.reduceByKey(lambda x, y: x + y)\n",
    "# Output: [(1, 8), (2, 12)]</code>\n",
    "  </li>\n",
    "    <li>groupByKey is just to group your dataset based on a key</br>\n",
    "    <code>rdd = sc.parallelize([(1, 2), (2, 4), (1, 6), (2, 8)])\n",
    "result = rdd.groupByKey()\n",
    "# Output: [(1, [2, 6]), (2, [4, 8])]\n",
    "</code>\n",
    "    </li>\n",
    "    <li>Too many unique keys go to <code>groupByKey()</code></li>\n",
    "    <li>Too many values (not unique) go to <code>reduceByKey()</code></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c870b-3cca-4810-b60c-904d4d97365d",
   "metadata": {},
   "source": [
    "**Q. Write word count program and explain it**\n",
    "****\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da5bd77-8bfa-46ea-9d09-4c5207ba1ff6",
   "metadata": {},
   "source": [
    "**Q. What is the repartitioning in Apache Spark**\n",
    "****\n",
    "**A.** Repartition is an operation that redistributes the data across the specified number of partitions to balance partition size and remove data skewness. Repartition performance data shuffling (hash base shuffling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9a2e3-73d0-45cc-b536-71bfb4c5e52b",
   "metadata": {},
   "source": [
    "**Q. What is the coalesce in Spark**\n",
    "****\n",
    "**A.** Coalesce is merging partitions and reducing number of partitions it is a more efficient operation when you want to decrease the number of partitions without suffering data across the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e67dfe-c2c6-466f-a249-23ad6e150dcf",
   "metadata": {},
   "source": [
    "**Q. What is the difference between repartitioning and coalesce in Apache Spark**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "\n",
    "| **Aspect**          | `repartition( )` Operation                                        | `coalesce( )` Operation                                           |\n",
    "|----------------------|---------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Functionality**    | Increases or decreases the number of partitions in a DataFrame by reshuffling the data across the Spark cluster. | Decreases the number of partitions without a full shuffle, trying to minimize data movement. |\n",
    "| **Performance**      | More resource-intensive as it involves a full shuffle.         | More efficient when decreasing the number of partitions as it minimizes data movement. |\n",
    "| **Use Cases**        | Useful when you want to either increase or decrease the level of parallelism and distribute the data **more evenly**. | Useful when you want to decrease the number of partitions to reduce overhead or optimize performance. |\n",
    "|**Partition size**| More evenly| not confirm|\n",
    "|**Transformation**| Wide transformation|Narrow transformation|\n",
    "| **Example**          | `df.repartition(5, \"city\")`                                             | `df.coalesce(3)`                                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055edd8e-2342-4648-8020-168ea3c489ae",
   "metadata": {},
   "source": [
    "* Data shuffling does exchange write and exchange read and by default creates 200 partitions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf46e2-0a28-4223-bb3a-c81d0e1f739e",
   "metadata": {},
   "source": [
    "**Q. Which one will you choice and why (repartition or coalesce)**\n",
    "***\n",
    "**A.** \n",
    "|**`df.repartition(int, col)`**|**`df.coalesce(int)`**|\n",
    "|---------------------|--------------------|\n",
    "|When **more** data skewness|When **less** data skewness|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f0b22-6c55-481e-9223-2fd400f1f88a",
   "metadata": {},
   "source": [
    "**Q. How to get the record in partitions**\n",
    "***\n",
    "**A.** ` df.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\").count().show() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca3a7a-a03f-45dc-adac-a902360974bc",
   "metadata": {},
   "source": [
    "**Q. What are the join strategies in Apache Spark**\n",
    "****\n",
    "**A.** Join strategies in Apache Spark\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/mcXw72y/013.png\" width=\"700\" height=\"600\" align=\"right\">\n",
    "    \n",
    "<ol>\n",
    "  <li>Broadcast hash join</li>\n",
    "  <li>Shuffle hash join</li>\n",
    "  <li>Shuffle sort-merge join <i>(nlogn)</i></li>\n",
    "  \n",
    "  \n",
    "  <li>Cartesian join</li>\n",
    "  <li>Broadcast nested loop join(on < or > id. </or>)</li>\n",
    "</ol>\n",
    "</div>\n",
    "visit : <a>https://www.linkedin.com/pulse/spark-join-strategies-mastering-joins-apache-venkatesh-nandikolla-mk4qc/</a>\n",
    "\n",
    "- The **Broadcast Hash Join** is the speedster of Spark joins. As the name suggests, it occurs when one of the data frames or tables is broadcast to all the executor nodes. To make this happen, one side of the data must be small enough to fit entirely in memory. Initially, this table is stored on the Driver node and then broadcast to all the executor nodes.\n",
    "- **Shuffle Hash Joins**\n",
    "When both tables are large, getting the broadcast join may lead to out-of-memory issues. In that case, Shuffle Hash Join can be a better option. These are quite expensive compared to broadcast joins as they involve both <u>shuffling of data and hashing</u>(hashing consumes in-memory utilization).\n",
    "It happens in two phases:\n",
    "  1. Repartitioning on both tables will happen based on the joining keys using hash partitioning, providing 200 partitions (as per the default shuffle partitions config; default value is 200).\n",
    "  2. A hash table (small size table) is created for the lookup activity, and the join happens based on that.\n",
    "- **Shuffle Sort Merge Join** This is more preferable over Shuffle Hash Join, considering its consistency over different cases. These joins happen in three stages shuffle, sort (sort consumes CPU utilization) and marging:\n",
    "  1. Repartitioning on both tables will happen based on the joining keys using hash partitioning, providing 200 partitions (as per the default shuffle partitions config; default value is 200).\n",
    "  2. Sorting of data happens on each partition individually.\n",
    "  3. As a last step, merging happens based on the joining key.\n",
    "- A **Cartesian Join** results in each record from one table being joined with every row in the other table. This is a costly join and can lead to out-of-memory (OOM) issues when there is insufficient space to execute the join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0027e595-869b-41d4-9237-b2b84512b971",
   "metadata": {},
   "source": [
    "**Q. Why join is expensive or wide dependency transformation**\n",
    "****\n",
    "**A.** because every join perform data shuffling between partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c8673-ed2c-439e-b70c-48405c0a32b1",
   "metadata": {},
   "source": [
    "* Spark intelligently filter-out null records, therefore we are not able to join tables on null record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd85b0-28f7-40c8-8e93-ce91e7941e35",
   "metadata": {},
   "source": [
    "**Q. Difference between shuffle-hash join and shuffle-short-marge join**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/YhDSJMz/017.png\" width=\"400\" height=\"45000\" align=\"right\">\n",
    "    <table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Aspect</th>\n",
    "      <th>Shuffle Sort-Merge Join</th>\n",
    "      <th>Shuffle Hash Join</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><strong>Process</strong></td>\n",
    "      <td>Sorting data before going to join</td>\n",
    "      <td>Create hash table by using smaller table, then generate hash number in the large table, and then join tables</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Resource Utilization</strong></td>\n",
    "      <td>Shuffling consume CPU</td>\n",
    "      <td>Sorting consume in-memory</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Time Complexity</strong></td>\n",
    "      <td>Sorting time complexity is O(nlogn)</td>\n",
    "      <td>Hashing time complexity is O(1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Error</strong></td>\n",
    "      <td><center>-------------------------</center></td>\n",
    "      <td>If you don't have enough memory, then you will get a memory out-of-exception error</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Spark Prefer</strong></td>\n",
    "      <td>By default, Spark prefers Shuffle Sort-Merge Join</td>\n",
    "      <td><center>-------------------------</center></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945bef9-bd49-44e0-843b-01b21a0e47ae",
   "metadata": {},
   "source": [
    "**Q. When do we need broadcast hash join**\n",
    "****\n",
    "**A.** Remove data shuffling (table available on own executor), this happens in three steps\n",
    "1. table broadcast to all executors\n",
    "2. hashing broadcast table\n",
    "3. join will large table which persent on executor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51d927-d053-49ab-a42e-1f00aa52aafc",
   "metadata": {},
   "source": [
    "**Q. How to hint broadcast join**\n",
    "***\n",
    "**A.** ` largeDF.join(smallDF.hint(\"broadcast\"),largeDF.joinkey == smallDF.joinkey,\"inner\") `                 \n",
    "or             \n",
    "` largeDF.join(broadcast(\"smallDF\"),largeDF[\"joinkey\"] == smallDF[\"joinkey\"],\"inner\")  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f095fb-407f-4c9e-ac40-3968c124ba1d",
   "metadata": {},
   "source": [
    "**Q. What is the accumulator in Apache Spark**\n",
    "***\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85348268-db2d-43d0-99ba-1c9bcfe55153",
   "metadata": {},
   "source": [
    "**Q. How does broadcast join works**\n",
    "****\n",
    "**A.** Driver sends data to executors, therefore it will save data shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238dea0-88a1-4f56-9b9a-468766a680b2",
   "metadata": {},
   "source": [
    "**Q. Difference between broadcast hash join and shuffle-hash join**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "\n",
    "|**broadCast Join**|**shuffle-hash join**|\n",
    "|-------------------|--------------------|\n",
    "|Join without data shuffling|Join with data shuffling|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd412ea0-49f9-413a-b798-294cfeb5e642",
   "metadata": {},
   "source": [
    "**Q. How can we change broadcast size of table**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "`spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")` Get by default small table size (10MB)                              \n",
    "`spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)` Set smaller table size in bite\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16d74f-5014-4830-a215-8d743e6b4d02",
   "metadata": {},
   "source": [
    "**Q. When broadcast table is not good or it will fail**\n",
    "***\n",
    "**A.** When broadcast table size is large according to driver size memory, the Safe side size is 10MB, but you should configure according to your driver memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75983ce3-66b7-4b15-bdf8-544ee1917e7c",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/7b0NsbY/001.webp\" style=\"width:100%;height:350px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2ce31-e9e3-47d3-89ba-0eeb4222b0c6",
   "metadata": {},
   "source": [
    "**Q. What is the OOM(out of memory) in Apache Spark**\n",
    "****\n",
    "**A.** When the program tries to use more memory than is available. There are two types of OOM `driver out-of-memory` and `executor out-of-memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b6492-a7e7-47bb-a7d9-600f462994d4",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d4a80-70da-43a8-ae5d-55b1aeaad06d",
   "metadata": {},
   "source": [
    "**Q. Types of Driver Memory**\n",
    "***\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/Xyw1GcQ/044.png\" style=\"width:500px;height:150px;\" align=\"right\"/>\n",
    "</div>\n",
    "<div>\n",
    "<p><strong>A.</strong> <code>spark.driver.memory</code> and <code>spark.driver.memoryOverhead</code></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241dbee-ff84-4a8c-9cee-fdeac1115607",
   "metadata": {},
   "source": [
    "**Q. Why do we get driver OOM**\n",
    "****\n",
    "**A.** When the program tries to use more memory than is available.           \n",
    "- ` collect( ) ` : It will show OOM error, try to read whold data form all executor\n",
    "- ` show( ) ` : only show data single partition from executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad40d84-e05b-417a-a9c1-e01372af802f",
   "metadata": {},
   "source": [
    "**Q. What is the driver overhead memory**\n",
    "****\n",
    "**A.** Driver Overhead Memory is a non-JVM process and container hold this space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a42958-9c84-46a3-8d9c-d10260212aa3",
   "metadata": {},
   "source": [
    "**Q. Coomon reason to get a driver OOM**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "- collect() method is used\n",
    "- Performe broadCast join on large table\n",
    "- More objects are used in the process\n",
    "- configuration problem (set less size driver-memory, broadcast table size, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde26ba-d89b-4103-ada6-450a4d4f6bb0",
   "metadata": {},
   "source": [
    "**Q. How to handle OOM**\n",
    "****\n",
    "**A.**\n",
    "- Don't use collect() if not required\n",
    "- Check broadCast join table size\n",
    "- Don't use unnecessary objects.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e0246-b458-4639-a579-cc7746db24fd",
   "metadata": {},
   "source": [
    "**Q. Why do we get OOM when data can be spill to the disc**\n",
    "****\n",
    "**A.** A spark has two types of memory 1 is execution memory and second is storage memory. Execution memory is used for transformation and storage memory is used for storing cache, dataframe, broadcast variable, etc. If storage memory is full by using cache or other things and execution memory needs more space(RAM), then try to get space data from storage memory(storage memory is already full by storing chche, dataframe, rdd, ect) and data is skewed data so in this case we will get out of memory exception and data cannot be spill to the disc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d295e-883d-4391-8544-1e60fb92c4d8",
   "metadata": {},
   "source": [
    "**Q. How Spark manage storage inside executor intelligently**\n",
    "****\n",
    "**A.**  Spark has two types of memory, execution memory, and storage memory, and both have 50%. This distribution is unified (dynamically) after 1.6.0 version, so when cache, RDD, and dataframe need to store data of more than 50% space then get space from execution memory, and in another case when execution memory needs space of more than 50% it goes to storage in memory and gets space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561b382-1356-44e0-aed4-88606ffd7682",
   "metadata": {},
   "source": [
    "**Q. How task is spill in executor**\n",
    "****\n",
    "**A.** 1.6.0 + task is spill in executor dynamically(from execution memory to storage memory and storage to execution memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2524636-ea0f-4db9-b74d-14cf3a290131",
   "metadata": {},
   "source": [
    "**Q. Why do we need overhead memory**\n",
    "****\n",
    "**A.** Overhead Memory hold non-JVM process and container required spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd9abe-d3bc-4732-b405-d7e30bb20dfc",
   "metadata": {},
   "source": [
    "**Q. When do we get executor OOM**\n",
    "****\n",
    "**A.** When a worker runs out of space to perform tasks. This can be due to tasks needing too much memory, not enough memory allocated to the worker, or **unevenly distributed data**. To solve it, consider giving more memory to workers, improving code efficiency, and making sure data is spread evenly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d2f66-7979-445b-a5c4-08f1579cded7",
   "metadata": {},
   "source": [
    "**Q. How many type of Spark Memory Manager**\n",
    "****\n",
    "<div>\n",
    "    <img src=\"https://i.ibb.co/bgRcFwx/018.png\" width=\"600\" height=\"100\" align=\"right\">\n",
    "</div>\n",
    "<div>\n",
    "    <p><strong>A.</strong> Two types of Spark Memory Manager</p>\n",
    "<ol>\n",
    "  <li>Static Memory Manager</li>\n",
    "  <li>Dynamic Memory Manager</li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ccf38-ed45-4546-9744-fa738f580f77",
   "metadata": {},
   "source": [
    "<center><strong><h3>Calculate the Memory for 5GB executor memory</h3></strong></center>\n",
    "<img src=\"https://i.ibb.co/pXxPqNv/022.jpg\" style=\"width:100%;height:400px\"/>\n",
    "<pre>Java Heap Memory       = 5 GB\n",
    "                       = 5 * 1024 MB\n",
    "                       = 5120 MB\n",
    "\n",
    "Reserved Memory        = 300 MB\n",
    "\n",
    "Usable Memory          = (Java Heap Memory — Reserved Memory)\n",
    "                       = 5120 MB - 300 MB\n",
    "                       = 4820 MB\n",
    "\n",
    "User Memory            = Usable Memory * (1.0 — spark.memory.fraction) \n",
    "                       = 4820 MB * (1.0 - 0.6) \n",
    "                       = 4820 MB * 0.4 \n",
    "                       = 1928 MB\n",
    "\n",
    "Spark Memory           = Usable Memory * spark.memory.fraction\n",
    "                       = 4820 MB * 0.6 \n",
    "                       = 2892 MB\n",
    "\n",
    "Spark Storage Memory   = Spark Memory * spark.memory.storageFraction\n",
    "                       = 2892 MB * 0.5 \n",
    "                       = 1446 MB\n",
    "Spark Execution Memory = Spark Memory * (1.0 - spark.memory.storageFraction)\n",
    "                       = 2892 MB * ( 1 - 0.5) \n",
    "                       = 2892 MB * 0.5 \n",
    "                       = 1446 MB</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dece61-2847-4c0d-ae09-defc21b667d7",
   "metadata": {},
   "source": [
    "<center><bold>Spark Executor Memory Management</bold></center>\n",
    "<img src=\"https://i.ibb.co/ZdgZnVk/023.jpg\" style=\"width:100%;height:300px\"/>\n",
    "\n",
    "\n",
    "- If Storage fraction is having some free RAM, execution can utilize it\n",
    "- If Execution memory is having some free RAM storage can utilize it\n",
    "- If execution needs RAM, and storage has occupied some RAM in execution area, execution can free it up / evict.\n",
    "- If storage needs RAM, and execution has consumed some RAM in storage area, storage can’t free it up.\n",
    "- visit : https://community.cloudera.com/t5/Community-Articles/Spark-Memory-Management/ta-p/317794"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524cb53-30f8-4215-ac99-08f52a4a4945",
   "metadata": {},
   "source": [
    "**Q. What is Master in Spark- submit**\n",
    "****\n",
    "**A.** ` spark-submit ` is a command-line tool in Apache Spark used to submit and launch Spark applications on a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ad0e1-0714-4b76-8820-a85339374bce",
   "metadata": {},
   "source": [
    "**Q. How do you run your job on Spark cluster**\n",
    "****\n",
    "**A.**           \n",
    "\n",
    "\n",
    "`/bin/ Spark-submit\\` If you are facing any issues then you should provide full-directory path                         \n",
    "`     -- master local [5] \\ ` YARN, spark://10.160.78.10:7077                                \n",
    "`    -- deploy-mode Cluster \\` client/cluster where should run application driver                            \n",
    "`    -- class main-class.scala \\` only for JAVA code nor python code                             \n",
    "`    -- jars C:\\my-sql-jar\\my-sql-connector.jar \\` jar file path                 \n",
    "`    -- conf Spark.dynamicAllocation.enabled = true \\`                 \n",
    "`    -- conf spark.dynamicAllocation.minExecutors = 1 \\`                \n",
    "`    -- conf spark.dynamicAllocation.maxExecutors = 10\\`                     \n",
    "`    -- Conf spark.sql.broadcastTimeout = 3600\\`                          \n",
    "`    -- Conf spark.sql.autoBroadcastJoinThreshold = 100000 \\`                       \n",
    "`    -- driver-memory 1g\\`                            \n",
    "`    -- executor-memory 2G\\`                 \n",
    "`    -- executor-cores 2\\`                                  \n",
    "`    -- py-files spark-submission.py, spark-log.py, ... \\` use absolute path for all dependency files                                \n",
    "`    -- files config.py, readme.ini` configuration and extensions files path                          \n",
    "`    -- C:\\\\spark-project\\DE-project\\main.py testing-project` it will pass in list                                     \n",
    "\n",
    "visit : https://spark.apache.org/docs/latest/configuration.html               \n",
    "visit : https://spark.apache.org/docs/latest/submitting-applications.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0efc7-9041-4249-a993-569ac492ec8d",
   "metadata": {},
   "source": [
    "\n",
    "**Q. Where is your Spark cluster**\n",
    "****\n",
    "**A.** On YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2970b-fee7-45b2-8813-c620a8fdac9e",
   "metadata": {},
   "source": [
    "**Q. How do we provide memory configuration and why do you use this much memory**\n",
    "****\n",
    "**A.** Try to run application with required memory, it will not get out-of-memory issues. \n",
    "\n",
    "`    -- driver-memory 1g\\`                            \n",
    "`    -- executor-memory 2G\\`                 \n",
    "`    -- executor-cores 2\\` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9587fd-14de-45ac-98cd-f858ffa7f8df",
   "metadata": {},
   "source": [
    "**Q. How to update configuration like broadcast threshold timeout dynamic memory allocation**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "`    -- Conf spark.sql.broadcastTimeout = 3600\\`                          \n",
    "`    -- Conf spark.sql.autoBroadcastJoinThreshold = 100000 \\`       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff5747-1e3a-4579-8f0f-52d47507c0de",
   "metadata": {},
   "source": [
    "**Q. What is the deploy mode in Apache Spark submit**\n",
    "****\n",
    "**A.** There is two types of deployment mode in spark                   \n",
    "**1. Client Mode**: In client mode, the Spark driver program runs on the machine where the application is submitted, coordinating and overseeing the Spark job. `/bin/` directory must be on edge node                           \n",
    "\n",
    "**2. Cluster Mode**: In cluster mode, the Spark driver program is submitted to run on one of the worker nodes within the Spark cluster(YARN), operating independently after submission.\n",
    "<div>\n",
    "<img src=\"https://i.ibb.co/qmX1jKZ/022.png\" width=\"100%\" height=\"300px\" align=\"right\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae56c2-784b-40dd-aa19-8585f6f7ea70",
   "metadata": {},
   "source": [
    "\n",
    "**Q. What is the edge node**\n",
    "****\n",
    "**A.** An edge node is a computer that acts as an end-user portal for communication with other nodes in cluster computing. Edge nodes are also sometimes called gateway nodes or edge communication nodes.\n",
    "In cluster, there are three types of nodes\n",
    "1. Master Node\n",
    "2. Worker/Data Node\n",
    "3. Edge Node\n",
    "\n",
    "- Edge nodes facilitate communications from end users to master and worker nodes.\n",
    "- Edge node isn’t used to store data or perform computation.\n",
    "- Edge node allows end users to contact worker nodes when necessary\n",
    "- Edge node authenticates user by using karbores\n",
    "- Edge node authorized user, have read-write access or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2a13d-cd93-4832-b43e-522cef6c86cd",
   "metadata": {},
   "source": [
    "**Q. Why do we need client and cluster modes**\n",
    "****\n",
    "**A.** Watch all execution processes in case of client deployment mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf56f7-c489-4eca-af35-aa00a814c6eb",
   "metadata": {},
   "source": [
    "**Q. What will happen if I close my adge node**\n",
    "****\n",
    "**A.** When we shut down edge node in client mode all processes will be stopped(executors kill in the absence of driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d739393-1b99-4430-9047-d87295f3927e",
   "metadata": {},
   "source": [
    "**Q. what is the Client mode vs Cluster mode deployment**\n",
    "***\n",
    "\n",
    "**A.** \n",
    "\n",
    "\n",
    "| Aspect                              | Client Mode Deployment                                         | Cluster Mode Deployment                                       |\r\n",
    "|-------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|\r\n",
    "| **Logs Location**                   | Logs are generated on the client machine, making it easy to debug. | Logs are generated in STDOUT or STDERR files, suitable for production workloads. |\r\n",
    "| **Network Latency**                 | Network latency is higher due to communication with the client.  | Network latency is less, as the driver communicates directly with the cluster. |\r\n",
    "| **Driver Memory Issues**            | Driver out of memory can occur.                                   | Driver can go into out of memory (OOM), but chances are less. Even if the edge server is closed, the process still runs on the cluster. |\r\n",
    "| **Driver Persistence**              | The driver goes away once the edge node server is disconnected or closed. | The process continues to run on the cluster even if the edge server is closed. |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b9bcc-6ab5-4c32-abdb-47c26ed34cc3",
   "metadata": {},
   "source": [
    "**Q. What is the adaptive query execution (AQE) in Apache Spark**\n",
    "***                  \n",
    "**A.** Adaptive Query Execution (AQE) provides facilities to control application processes on **runtime**, update and enhance performance dynamically it is enrolled 3.0 and above\n",
    "</br>**<center>---------------------------------------or---------------------------------------</center>**</br>\n",
    "Adaptive Query Execution (AQE) in Apache Spark dynamically adjusts and optimizes the execution plan of Spark SQL queries based on runtime statistics, enhancing performance by adapting to data characteristics and cluster conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7162c-9268-4b1c-b34f-a542bc0d248a",
   "metadata": {},
   "source": [
    "**Q. What is the features of adaptive query execution?**\n",
    "***\n",
    "**A.** Features of AQE (AQE start when data goes to shuffle)\n",
    "  <div>\n",
    "      <div>\n",
    "          <img src=\"https://i.ibb.co/yPcNwRD/023.png\" width=\"100%\" height=\"600\" />\n",
    "      </div>\n",
    "      <div>\n",
    "          <ul>\n",
    "              <li>Dynamically coalescing (margin) the number of shuffle partitions</li>\n",
    "              <li>Dynamically switching join strategies</li>\n",
    "              <li>Dynamically optimizing skewed data join</li>\n",
    "          </ul>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729ad3e-7e09-4149-8c16-00ea05593b5c",
   "metadata": {},
   "source": [
    "**Q. What is the Over Parallelism and Under Parallelism**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "1. **Over Parallelism** :\n",
    "Small amount of data divided into large number of partitions\n",
    "2. **Under Parallelism** :\n",
    "Large amount of data divided into small number of partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf29e5-8ff8-4b12-be5c-03a645ddbac4",
   "metadata": {},
   "source": [
    "**Q. Why do we need adaptive query execution**\n",
    "***\n",
    "**A.**\n",
    "|   Optimization                |    Description                           |\n",
    "|-------------------------|-------------------------------------------------|\n",
    "| Dynamically Coalescing         |   Adjusting the number of shuffle partitions during execution based on the data size to optimize esource utilization.|\n",
    "| Dynamically Switching Join Strategies     |   Adapting join strategies during execution based on the size of input data to improve performance.     |\n",
    "| Dynamically Optimizing Skewed Data Join  |   Identifying and optimizing skewed data in join operations to prevent performance degradation due to uneven data distribution. Split skewed data from one table And generate duplicate data on another table then join both table              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b61d9d-2500-453b-82db-5673ac84ea4e",
   "metadata": {},
   "source": [
    "**Visit for AQE** : https://spark.apache.org/docs/latest/sql-performance-tuning.html                                        \n",
    "**Visit for AQE** : https://kyuubi.readthedocs.io/en/master/deployment/spark/aqe.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34306765-aaf6-4ed3-92de-3975cb852dc5",
   "metadata": {},
   "source": [
    "**Q. What is the caching and persist**\n",
    "****\n",
    "**A.** Caching and persist both in Spark is an optimization technique to store the intermediate result of an RDD, DataFrame, and Dataset so they can be reused in subsequent actions(reusing the RDD, Dataframe, and Dataset computation results). \n",
    "- cache() method default saves it to memory (MEMORY_ONLY) or spark storage pole\n",
    "- persist() method is used to store it to the user-defined storage level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87cd63-c2c0-4589-90a8-654547ea234e",
   "metadata": {},
   "source": [
    "**Q. Best choice to select storage label in prestige**\n",
    "***\n",
    "**A.** ` df.persist(StorageLevel(useDisk, useMemory, UseOffHeap, deserialize, Replication = 1)) `              \n",
    "` df.persist(StorageLevel(False, True, False, False, Replication = 1)) `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b3623-7688-43c9-a869-034345028f11",
   "metadata": {},
   "source": [
    "**Q. Why we use caching in a spark**\n",
    "***\n",
    "**A.**  \n",
    "\n",
    "When we create a dataframe, this dataframe is store in short-live memory, short-live memory is alive in a short time when we do not use this dataframe, this data frame will be deleted form short-live memory, If I again try to use this dataframe, again create is by using **DAG**, and this is a time-consuming process, In this case, we are creating a cache for this data frame to store in spark-memory it will save some processing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a3e18-5028-4963-a09a-eebe76554d5b",
   "metadata": {},
   "source": [
    "\n",
    "**Q. Why do we need caching or persistence**\n",
    "****\n",
    "**A.** Do'nt need to create dataframe again and again, Reduce processing time(to create dataframe form DAG), both store data in spark-storeage-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78247586-72d6-4063-bac4-43c581c8d881",
   "metadata": {},
   "source": [
    "**Q. RAM and Hard-Disk in which format store data**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "1. You can have serialized and deserialized format while caching to **MEMORY**, default is deserialized(speed processing)                \n",
    "2. You can have only serialized format while caching to **DISK** and **OFF_HEAP**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aea0c-83ea-4eb7-9435-f1421d7a8c14",
   "metadata": {},
   "source": [
    "**Q. How many storage level in Spark perseste()**\n",
    "***\n",
    "**A.** \n",
    "- RAM: Store data in De-Serialized / Serialized\n",
    "- Disk: Store data in Serialized\n",
    "\n",
    "\n",
    "| Storage Level        | Description                                                        | Example                                           | Prone | Causes |\n",
    "|----------------------|--------------------------------------------------------------------|---------------------------------------------------|-------|--------|\n",
    "| MEMORY_ONLY          | Data stored as deserialized Java objects in JVM heap.              | `df.persist(storageLevel=\"MEMORY_ONLY\")`           | High  | Memory pressure |\n",
    "| MEMORY_ONLY_SER      | Data stored as serialized Java objects in JVM heap.                | `df.persist(storageLevel=\"MEMORY_ONLY_SER\")`       | Moderate  | Serialization overhead |\n",
    "| MEMORY_ONLY_2        | MEMORY_ONLY with 2 replicas for fault tolerance.                   | `df.persist(storageLevel=\"MEMORY_ONLY_2\")`         | High  | Memory pressure, Fault tolerance |\n",
    "| DISK_ONLY            | Data stored on disk in serialized format, read into memory on demand. | `df.persist(storageLevel=\"DISK_ONLY\")`           | Low  | Disk I/O |\n",
    "| MEMORY_AND_DISK      | Data stored as deserialized Java objects in JVM heap, with overflow to disk. | `df.persist(storageLevel=\"MEMORY_AND_DISK\")`  | High  | Memory pressure, Disk I/O |\n",
    "| MEMORY_AND_DISK_SER  | Data stored as serialized Java objects in JVM heap, with overflow to disk. | `df.persist(storageLevel=\"MEMORY_AND_DISK_SER\")`  | Moderate  | Serialization overhead, Disk I/O |\n",
    "| MEMORY_AND_DISK_2    | MEMORY_AND_DISK with 2 replicas for fault tolerance.               | `df.persist(storageLevel=\"MEMORY_AND_DISK_2\")`   | High  | Memory pressure, Disk I/O, Fault tolerance |\n",
    "| OFF_HEAP             | Data stored off-heap using Spark's external memory management.     | `df.persist(storageLevel=\"OFF_HEAP\")`            | High  | Memory pressure, External memory management overhead |\n",
    "\n",
    "- ` SER `: only works in Java and Scala like MEMORY_AND_DISK_SER, MEMORY_ONLY_SER, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427572cc-f455-4765-98ae-d84a7d8e90e4",
   "metadata": {},
   "source": [
    "**Q. caching Vs persistence**\n",
    "***\n",
    "**A.**\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Criteria</th>\n",
    "      <th>Caching using <code>persist()</code></th>\n",
    "      <th>Caching using <code>cache()</code></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Lazy Operation</td>\n",
    "      <td>Caching is a lazy operation; it occurs on executing an action.</td>\n",
    "      <td>Similar lazy operation as <code>persist()</code>.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Storage Fraction</td>\n",
    "      <td>Does not store a fraction of a partition; either stores the complete partition or none.</td>\n",
    "      <td>Similar behavior, complete partition or none.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Caching Complete DataFrame</td>\n",
    "      <td>Use an action like <code>count()</code> to cache the complete DataFrame.</td>\n",
    "      <td>Similar approach using actions like <code>count()</code>.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Default Storage Level</td>\n",
    "      <td>MEMORY_AND_DISK_SER with 1x replication.</td>\n",
    "      <td>MEMORY_AND_DISK_SER with 1x replication.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Removing from Cache</td>\n",
    "      <td>Use <code>unpersist()</code> method.</td>\n",
    "      <td>Use <code>unpersist()</code> method.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Different Storage Levels</td>\n",
    "      <td>Offers flexibility to use different storage levels.</td>\n",
    "      <td>Similar flexibility in using various storage levels.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005a928-af1f-4f47-b8b7-db79c1943be8",
   "metadata": {},
   "source": [
    "**Q. \n",
    "What happens in case of caching partition not filling on memory**\n",
    "***\n",
    "**A.**  Data will spill on the Disk or recalculate process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0681559-dd63-4ef7-b881-302bc5897924",
   "metadata": {},
   "source": [
    "**Q. When should we avoid cashing**\n",
    "****\n",
    "**A.** When we have a small size dataframe or no need for enough time to recalculate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98524b-2f90-4630-ab0e-885eb43ce5c6",
   "metadata": {},
   "source": [
    "**Q. How do we recalculate, When we lose partition during the read or write process?**\n",
    "***\n",
    "**A.** Partition again recalculated by using DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac86af6-7988-4b80-adca-cf201581cd64",
   "metadata": {},
   "source": [
    "**Q. How to uncache the data**\n",
    "****\n",
    "**A.** By using unpersist() function `df.unpersist()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f8188-fadf-46e1-8373-ed6c4a332162",
   "metadata": {},
   "source": [
    "**Q. Difference between cache and persist**\n",
    "****\n",
    "**A.** cache() nothing but just like persist() only has a fix argument on storage level `MEMORY_AND_DISK`\n",
    "\n",
    "\n",
    "\n",
    "| Operation | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| `cache`   | Quick method equivalent to `persist` with default storage level (`MEMORY_ONLY`). | `df.cache()` |\n",
    "| `persist` | More flexible operation allowing customization of storage levels and options. | `df.persist(storageLevel=\"MEMORY_AND_DISK\")` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a8293-4dda-4f4a-b0a2-60898c67b1dd",
   "metadata": {},
   "source": [
    "**Q. Which storage level to choice**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "<img src=\"https://i.ibb.co/m43pzGg/024.png\" width=\"100%\" height=\"600\" alt=\"024\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329ac9b-0614-4c94-ba88-62d08218d37e",
   "metadata": {},
   "source": [
    "**Q. What is the dynamic resource allocation in Spark**\n",
    "****\n",
    "**A.** Dynamic Resource Allocation in Apache Spark is a feature that allows a Spark application to dynamically acquire and release executor resources based on the workload. It aims to optimize resource utilization and improve the overall performance of Spark applications by adjusting the number of executors based on the workload's demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36863de3-f020-4368-b6c3-755b59b64404",
   "metadata": {},
   "source": [
    "`spark-submit \\`                           \n",
    "`  --conf spark.dynamicAllocation.enabled=true \\`                                \n",
    "`  --conf spark.dynamicAllocation.minExecutors=5 \\`                       \n",
    "`  --conf spark.dynamicAllocation.maxExecutors=20 \\`                       \n",
    "`  --class YourSparkApp \\`                             \n",
    "`  your-spark-app.jar`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9876c-1d5c-418f-856f-8c82a7017b5b",
   "metadata": {},
   "source": [
    "**Q. How resource manager provides the resources if dynamic resource allocation**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "`  --conf spark.dynamicAllocation.enabled=true \\`                                \n",
    "`  --conf spark.dynamicAllocation.minExecutors=5 \\`                       \n",
    "`  --conf spark.dynamicAllocation.maxExecutors=20 \\`                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348db01-c723-48c5-92d8-1e12a4d3a479",
   "metadata": {},
   "source": [
    "**Q. What are the resource allocation techniques we have in Apache Spark**\n",
    "****\n",
    "**A.** Two types of technique\n",
    "- Static Resource Allocation\n",
    "- Dynamic Resource Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127ac2e-6402-43dd-9d1b-7889c293edeb",
   "metadata": {},
   "source": [
    "**Q. What are the challenges involved with dynamic resource allocation**\n",
    "****\n",
    "**A.** Be careful about release time and demand time\n",
    "\n",
    "- relase but have minimum executor\n",
    "- set a perfect time to demand executor\n",
    "- Care on the ideal time to de-allocate resources from spark jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9bc024-dcfa-44b4-8a5f-1e12913cfd05",
   "metadata": {},
   "source": [
    "**Q. When to avoid dynamic resource allocation?**\n",
    "***\n",
    "**A.** When spark-submit runs on production-level code and critical spark job, I do not want to delay any jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ae408-805e-40fd-bedc-bd235dcc1198",
   "metadata": {},
   "source": [
    "**Q. What is the ideal time to release resources and ask resources**\n",
    "***\n",
    "**A.** 60s by default and configurable. and ask within 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b2d6a-8d3f-42f9-a7fa-60353456c65c",
   "metadata": {},
   "source": [
    "\n",
    "**Q. How spark will remove or add resources**\n",
    "***\n",
    "\n",
    "<b>A.</b> If resource not use 60s spark will remove resource, and add by uisng <b>two fold</b> technique and ideal time is 1s\n",
    "<img src=\"https://i.ibb.co/DfKTn6L/025.png\" alt=\"025\" style=\"width:100%;height:50px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb651e8-3c0a-46ee-bb27-e24ebc789cbe",
   "metadata": {},
   "source": [
    "**What configuration needed for dynamic resource allocation**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "`spark-submit \\`                             \n",
    "`    --name Your AppName\\ --master yarn \\ `                          \n",
    "`    --deploy-mode cluster \\`                                 \n",
    "`    --conf spark.dynamicAllocation.enabled=true \\ `                    \n",
    "`    --conf spark.dynamicAllocation.minExecutors=5 \\ `                          \n",
    "`    --conf spark.dynamicAllocation.maxExecutors=49 \\`                  \n",
    "`    --conf spark.shuffleTracking.enabled=true\\`                 \n",
    "`    --conf spark.dynamicAllocation.executorIdleTimeout=45s \\`                     \n",
    "`    --conf spark.scheduler.backlogTimeout=2s \\` this is two-fold (not configurable)                   \n",
    "`    --conf spark.executor.memory=20g \\`                          \n",
    "`    --conf spark.executor.cores=4 \\`                           \n",
    "`    --conf spark.driver.memory=20g \\`                    \n",
    "`    --py-files python_dependencies.zip \\`                       \n",
    "`    main.py`                                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea8478-3052-4095-9022-285fef591e9c",
   "metadata": {},
   "source": [
    "**Q. What is spark.shuffleTracking.enabled=true configration**\n",
    "***\n",
    "**A.** When data shuffle then write it to output exchange, and use it when executor de-allocates resource, this configuration protects cache in executor (spark storage memory). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0bfec3-5119-479e-aa9c-cf7c4ee73d73",
   "metadata": {},
   "source": [
    "**Q. What is the dynamic partition pruning**\n",
    "****\n",
    "**A.**  It allows Spark to dynamically skip unnecessary partitions when we use filter transformation and does this on run time. This feature is handy when dealing with large datasets and partitioned tables. Key points for dynamic partition pruning\n",
    "- Data must be partitioned\n",
    "- Second table must be broadcasted\n",
    "- this feature available 3.0 and above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8660eb-dc8a-4f1a-963c-94d1726540cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45879871-a435-417e-81d6-d24564ea599b",
   "metadata": {},
   "source": [
    "**Q. Why do we need dynamic partition pruning (DPP)**\n",
    "****\n",
    "**A.** Dynamic Partition Pruning optimizes query performance by intelligently skipping unnecessary partitions based on filter conditions, reducing data scanning and improving resource efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa06c0-57f6-4e2f-805b-3bd3c2fed228",
   "metadata": {},
   "source": [
    "**Q. When dynamic partition pruning will not work**\n",
    "****\n",
    "**A.**\n",
    "- When data not partitioned\n",
    "- 2nd table cant be broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9805e83-c04f-483c-8446-f7eef0dace6a",
   "metadata": {},
   "source": [
    "**Q. What is Data Skew?**\n",
    "***\n",
    "**A.** When the data are not balanced between workers, we call the data “skewed.”\n",
    "<img src=\"https://i.ibb.co/4FSwVHV/002.webp\" style=\"width:100%;height:200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337977c4-fe4e-43a0-96e9-17f8e30a7d7d",
   "metadata": {},
   "source": [
    "**Q. Can we can split a skewed data**\n",
    "***\n",
    "**A.** Yes, If your skweed data is more than 5 times of median and size of the skweed data is greater than 256MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc219f7c-c134-4e6a-9b82-14d789dc2ce2",
   "metadata": {},
   "source": [
    "**Q. What is the data skewness problem**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17c2e2-90c7-4abc-a582-03280e226da4",
   "metadata": {},
   "source": [
    "**Q. What are the ways to remove skewness**\n",
    "***\n",
    "**A.**\n",
    "1. repartition( )\n",
    "2. AQE\n",
    "3. Salting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054858f-de29-463d-bcdd-fc5566b02982",
   "metadata": {},
   "source": [
    "<h3><center>Skewed data salting in groupBy</center></h3>\n",
    "<img src=\"https://i.ibb.co/2j8gy7T/047.png\" alt=\"047\" style=\"width:100%;height:400px;\">\n",
    "visit: https://stackoverflow.com/questions/58110207/spark-how-does-salting-work-in-dealing-with-skewed-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8412acf-91bb-463e-aad6-09b7bb7a6571",
   "metadata": {},
   "source": [
    "<h3><center>Skewed data salting in join</center></h3>\n",
    "<img src=\"https://i.ibb.co/cX12xgM/003.webp\" style=\"width:100%;\">\n",
    "visit : https://selectfrom.dev/data-skew-in-apache-spark-f5eb194a7e2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18f5ae-6b88-4d42-9855-28cab888acfc",
   "metadata": {},
   "source": [
    "**Q. What is salting**\n",
    "****\n",
    "**A.** Salting is a technique used in Apache Spark to evenly distribute data across partitions. It involves adding a random or unique identifier (called a \"salt\") to each record before performing operations like grouping or joining. This helps avoid data skew and improves parallelism in data processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a1a7f-a32d-44d6-ac84-bfa56c4b5ca5",
   "metadata": {},
   "source": [
    "**Q. How can we implement salting**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6bd52-15a8-433a-a045-11444c5a8c8d",
   "metadata": {},
   "source": [
    "**Q. what is the difference between salting and repartitioning in spark**\n",
    "***\n",
    "**A.**\n",
    "\n",
    "| Feature              | Repartitioning                                      | Salting                                           |\n",
    "| -------------------- | --------------------------------------------------- | ------------------------------------------------- |\n",
    "| **Purpose**          | Improve parallelism by redistributing data          | Alleviate (reduce) data skewness by distributing keys more evenly |\n",
    "| **How it Works**     | Reshuffles data across partitions                  | Adds random or predetermined values (salt) to keys |\n",
    "| **Transformation**   | `repartition()`, `coalesce()`                       | Custom logic to add salt to keys                  |\n",
    "| **Use Cases**        | Optimize performance by adjusting partition count | Address data skewness issues in join operations   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd48972-0c45-4a24-b265-802ad8527ad5",
   "metadata": {},
   "source": [
    "**Q. Which cluster manager have you used in project**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63834c-715b-4bf4-a88e-b272336d0f13",
   "metadata": {},
   "source": [
    "\r\n",
    "**Q. What is the size of the cluster?**\r\n",
    "****\r\n",
    "**A.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c4e24-e26b-4e32-aa53-f2deae2219e1",
   "metadata": {},
   "source": [
    "**Q. Data warehouse Vs Data Lake**\n",
    "****\n",
    "**A.**\n",
    "\n",
    "| DATAWAREHOUSE | Vs | DATALAKE|\n",
    "|----------|---------------|-----------------|\n",
    "|THINK FIRST, LOAD LATER |**Philosophy** |LOAD FIRST, THINK LATER|\n",
    "|STRUCTURED DATA| **Processing** | STRUCTURED, SEMI-STRUCTURED, UNSTRUCTURED DATA\n",
    "|EXPENSIVE FOR LARGE DATA STORAGE|**Storage**|BUILT FOR LOW COST STORAGE|\n",
    "|LESS AGILE (RIGID)|**Agility**|HIGHLY AGILE (FLEXIBLE)|\n",
    "|OPERATIONAL REPORTING(SUITS BUSINESS USERS)|**Usage**|ADVANCED ANALYTICS (SUITS DATA SCIENTISTS)|\n",
    "|MATURED|**Security**|STILL MATURING|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641b6cb-b851-4b87-951f-3b7dc463aefa",
   "metadata": {},
   "source": [
    "## Spark Execcutor Memory Management System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e0199-c68d-49b1-9ec5-60909cded176",
   "metadata": {},
   "source": [
    "<div style=\"float: left; width: 70%;\">\n",
    "    <p>Lets Assume ...</p>\n",
    "    <blockquote>\n",
    "      <ul>\n",
    "        <li>1 worker node</li>\n",
    "        <li>1 executor within worker</li>\n",
    "        <li>Worker memory -&gt; 16 GB</li>\n",
    "        <li>Executor memory -&gt; 10 GB</li>\n",
    "      </ul>\n",
    "    </blockquote>\n",
    "    <p>So remaining 6 GB controlled by OS which is called off-heap memory</p>\n",
    "    <blockquote>\n",
    "      <ul>\n",
    "        <li>Executor memory controlled by JVM process</li>\n",
    "      </ul>\n",
    "    </blockquote>\n",
    "</div>\n",
    "\n",
    "<div style=\"float: right; width: 30%;\">\n",
    "    <img src=\"images/030.png\" width=\"300\" height=\"600\" alt=\"024\" border=\"0\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557d02b-9d30-468f-9861-87238eb831da",
   "metadata": {},
   "source": [
    "> - Executor memory controlled by JVM process\n",
    "> - off-heap memory controlled by OS(operating system) \n",
    "> - on-heap memory controlled by Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c927e5a-6588-495e-84e9-43de8f3dec29",
   "metadata": {},
   "source": [
    "1. **Reserved Memory**: Reserved by Spark for internal purposes               \n",
    "2. **User Memory**: For storing the data- structures created and managed by the user's code            \n",
    "3. **Execution Memory**: JVM heap space used by data-structures during shuffle operations (join and aggregations)         \n",
    "4. **Storage Memory**: JVM heap space reserved for cached data                 \n",
    "UMM = Execution memory (50% UMM) + Storage memory (50% of UMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdf8fe-d758-4b95-b7db-bf81a9a51d8f",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <div style=\"width: 65%;\">\n",
    "<blockquote>\n",
    "  <ul>\n",
    "    <li>Each executor within the worker node has access to Off-heap memory</li>\n",
    "    <li>Off-Heap memory can be used by Spark explicitly for storing its data</li>\n",
    "    <li>The amount of off-heap memory used by Spark to store actual data frames is governed by <code>spark.memory.offHeap.size</code></li>\n",
    "    <li>To enable off-heap memory, <code>set spark.memory.offHeap.use=true</code></li>\n",
    "    <li>Accessing off-heap is slightly slower than accessing on-heap storage but still faster than reading/writing from a disk. GC (Garbage Collector) Scan can be avoided by using off-heap memory</li>\n",
    "  </ul>\n",
    "</blockquote>\n",
    "    </div>\n",
    "\n",
    "  <div style=\"width: 35%;\">\n",
    "    <img src=\"https://i.ibb.co/ZhBjq2c/031.png\" width=\"200\" height=\"600\" alt=\"024\" border=\"0\">\n",
    "  </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d35b8-9be0-4bdd-b26b-19fe8a66f3fe",
   "metadata": {},
   "source": [
    "| **On-Heap**      | **Off-Heap**              |\n",
    "|----------|-------|\n",
    "| Better performance than Off-heap because object allocation and deallocation happens automatically | Slower than On-heap but still better than disc performance. Manual memory management                                                                  |\n",
    "| Managed and controlled by Garbage collector within JVM process so adding overhead of GC scans     | Directly managed by Operating system so avoiding the overhead of GC                                                                                   |\n",
    "| Data stored in the format of Java bytes (deserialized) which Java can process efficiently         | Data stored in the format of array of bytes(serialized). So adding overhead of serializing/ deserializing when java program needs to process the data |\n",
    "| While processing smaller sets of data that can fit into heap memory, this option is suitable      | When need to store bigger dataset that can not fit into heap memory, can make advantage of off-heap memory to store the data outside JVM process      |\n",
    "|                                                                                                   |                                                                                                                                                       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
